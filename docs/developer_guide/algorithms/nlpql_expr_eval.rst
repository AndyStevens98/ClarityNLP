NLPQL Expression Evaluation
***************************

Overview
========

In this section we describe the mechanisms that ClarityNLP uses to evaluate
NLPQL expressions. NLPQL expressions are found in ``define`` statments such as:
::
   define hasFever:
       where Temperature.value >= 100.4;

   define hasSymptoms:
       where hasFever AND (hasDyspnea OR hasTachycardia);
       
The expressions in each statement consist of everything between the ``where``
keyword and the semicolon:
::
   Temperature.value >= 100.4

   hasFever AND (hasDyspnea OR hasTachycardia)

NLPQL expressions can either be mathematical or logical in nature, as these
examples illustrate.

Recall that the processing stages for a ClarityNLP job proceed roughly as
follows:

1. Parse the NLPQL file and determine which NLP tasks to run.
2. Formulate a Solr query to find relevant source documents, partition the
   source documents into batches, and assign batches to computational tasks.
3. Run the tasks in parallel and write individual task results to MongoDB.
   Each individual result from an NLP task comprises a *task result document*
   in the Mongo database. The term *document* is used here in the MongoDB
   sense, meaning an object containing key-value pairs. The MongoDB 'documents'
   should not be confused with the Solr source documents, which are electronic
   health records.
4. Evaluate NLPQL expressions using the **task result documents** as the source
   data. Write expression evaluation results to MongoDB as separate result
   documents.

Thus ClarityNLP evaluates expressions **after** all tasks have finished running
and have written their individual results to MongoDB. The expression evaluator
consumes the task results and uses it to generate new results from the
expression statements.

We now turn our attention to a description of how the expression evaluator
works. We should state at the outset that the descriptions below apply to the
expression evaluator built on
`MongoDB aggregation <https://docs.mongodb.com/manual/aggregation/>`_. This
evaluator is currently in a testing phase and must be explicitly enabled by
adding the following line to the ``project.cfg`` file in the ``[local]``
section:
::
   [local]
   evaluator=mongo

A Pandas-based evaluator will be used if this line is absent or is commented
out with a `#` character. The Pandas evaluator uses different techniques from
those described below.

Why use MongoDB aggregation to evaluate NLPQL expressions? The basic reason
is that the data resides in a Mongo collection, and it is more efficient to
evaluate expressions using MongoDB capabilities than to use something else.
Use of a non-Mongo evaluator requires ClarityNLP to:

- Run a set of queries to extract the data from MongoDB
- Transmit the query results across a network if the Mongo instance is hosted
  remotely
- Ingest the query results into another evaluation engine
- Evaluate the NLPQL expressions and generate results
- If MongoDB is hosted remotely, transmit the results back to the Mongo host
- Insert the results into MongoDB.
   
Evaluation via the MongoDB aggregation framework should prove to be much more
efficient than this process, since all data remains inside MongoDB.


NLPQL Expression Types
======================

In the descriptions below we refer to NLPQL **variables**, which have the
form ``nlpql_feature.field_name``. The NLPQL feature is a label introduced in a
``define`` statement. The ``field_name`` is the name of an output field
generated by the task associated with the NLPQL feature.

The output field names from ClarityNLP tasks can be found in the
`API documentation <https://claritynlp.readthedocs.io/en/latest/api_reference/index.html>`_.

1. Simple Mathematical Expressions
-------------------------------------

A simple mathematical expression is a string containing NLPQL variables,
operators, parentheses, or numeric literals. Some examples:
::
   Temperature.value >= 100.4
   (Meas.dimension_X > 5) AND (Meas.dimension_X < 20)
   (0 == Temperature.value % 20) OR (1 == Temperature.value % 20)

The variables in a simple mathematical expression all refer to a **single**
NLPQL feature.

Simple mathematical expressions produce a result from data contained in a
**single** task result document. The result of the expression evaluation is
written to a new MongoDB result document.

2. Simple Logic Expressions
-----------------------------

A simple logic expression is a string containing NLPQL features,
parentheses, and the logic operators ``AND`` and/or ``OR``. For instance:
::
   hasRigors OR hasDyspnea
   hasFever AND (hasDyspnea OR hasTachycardia)
   (hasShock OR hasDyspnea) AND (hasTachycardia OR hasNausea)

Logic expressions operate on high-level NLPQL features, **not** on numeric
literals or NLPQL variables. The presence of a numeric literal or NLPQL
variable indicates that the expression is either a mathematical expression
or possibly invalid.

Simple logic expressions produce a result from data contained in one or more
task result documents. The result from the expression evaluation is written to
one or more new MongoDB result documents (the details will be explained below).
   
3. Mixed Expressions
--------------------

A *mixed* expression is a string containing either:

- A mathematical expression **and** a logic expression
- A mathematical expression using variables involving two or more NLPQL features

For instance:
::
   // both math and logic
   (Temperature.value >= 100.4) AND (hasDyspnea OR hasTachycardia)

   // two NLPQL features: LesionMeasurement and Temperature
   (LesionMeasurement.dimension_X >= 10) OR (Temperature.value >= 100.4)

   // math, logic, and multiple NLPQL features
   Temperature.value >= 100.4 AND (hasRigors OR hasNausea) AND (LesionMeasurement.dimension_X >= 15)

The evaluation mechanisms used for mathematical, logic, and mixed expressions
are quite different. To fully understand the issues involved, it is helpful to
first understand the meaning of the 'intermediate' and 'final' phenotype
results.

Phenotype Result CSV Files
--------------------------

Upon submission of a new job, ClarityNLP prints information to stdout that
looks similar to this:
::
    HTTP/1.0 200 OK
    Content-Type: text/html; charset=utf-8
    Content-Length: 1024
    Access-Control-Allow-Origin: *
    Server: Werkzeug/0.14.1 Python/3.6.4
    Date: Fri, 23 Nov 2018 18:40:38 GMT
    {
       "job_id": "11108",
       "phenotype_id": "11020",
       "phenotype_config": "http://localhost:5000/phenotype_id/11020",
       "pipeline_ids": [
            12529,
            12530,
            12531,
            12532,
            12533,
            12534,
            12535
        ],
        "pipeline_configs": [
            "http://localhost:5000/pipeline_id/12529",
            "http://localhost:5000/pipeline_id/12530",
            "http://localhost:5000/pipeline_id/12531",
            "http://localhost:5000/pipeline_id/12532",
            "http://localhost:5000/pipeline_id/12533",
            "http://localhost:5000/pipeline_id/12534",
            "http://localhost:5000/pipeline_id/12535"
        ],
        "status_endpoint": "http://localhost:5000/status/11108",
        "results_viewer": "?job=11108",
        "luigi_task_monitoring": "http://localhost:8082/static/visualiser/index.html#search__search=job=11108",
        "intermediate_results_csv": "http://localhost:5000/job_results/11108/phenotype_intermediate",
        "main_results_csv": "http://localhost:5000/job_results/11108/phenotype"
    }

Here we see various items relevant to the job submission. Each submission
receives a *job_id*, which is a unique numerical identifier for the run.
ClarityNLP writes all task results from all jobs to the ``phenotype_results``
collection in a Mongo database named ``nlp``. The job_id is
needed to distinguish the data belonging to each run. Results can be extracted
directly from the database by issuing `MongoDB queries <https://docs.mongodb.com/manual/tutorial/query-documents/>`_.

We also see URLs for 'intermediate' and 'main' phenotype results. These are
convenience APIs that export the results to CSV files. The data in the
intermediate result CSV file contains the output from each NLPQL
task not marked as ``final``. The main result CSV contains the results
from any final tasks or final expression evaluations. The CSV file can be
viewed in Excel or in another spreadsheet application.

Each NLP task generates a result document distinguished by a particular value
of the ``nlpql_feature`` field. The *define* statement
::
   define hasFever:
        where Temperature.value >= 100.4;

generates a set of rows in the intermediate CSV file with the
``nlpql_feature`` field set to ``hasFever``.  The NLP tasks
::
    // nlpql_feature 'hasRigors'
    define hasRigors:
        Clarity.ProviderAssertion({
            termset: [RigorsTerms],
            documentset: [ProviderNotes]
        });

    // nlpql_feature 'hasDyspnea
    define hasDyspnea:
        Clarity.ProviderAssertion({
            termset: [DyspneaTerms],
            documentset: [ProviderNotes]
        });

generate two blocks of rows in the CSV file, the first block having the
``nlpql_feature`` field set to ``hasRigors`` and the next block having it
set to ``hasDyspnea``.  The different nlpql_feature blocks appear in order
as listed in the source NLPQL file. The presence of these nlpql_feature
blocks makes locating the results of each NLP task a relatively simple
matter.

Expression Evaluation Algorithms
================================

ClarityNLP evaluates expressions via a multi-step procedure. In this section
we describe the different processing stages.

Expression Tokenization and Parsing
-----------------------------------

The NLPQL front end parses the NLPQL file and sends the raw expression text
to the evaluator (``nlp/data_access/expr_eval.py``). The evaluator module
parses the expression text and converts it to a fully-parenthesized token
string. The tokens are separated by whitespace and all operators are replaced
by string mnemonics (such as ``GE`` for the operator ``>=``, ``LT`` for the
operator ``<``, etc.).

If the expression includes any subexpressions involving numeric literals, they
are evaluated at this stage and the literal subexpression replaced with the
result.

Validity Checks
---------------

The evaluator then runs validity checks on each token. If it finds a token that
it does not recognize, it tries to resolve it into a series of known NLPQL
features separated by logic operators. For instance, if the evaluator were
to encounter the token ``hasRigorsANDhasDyspnea`` under circumstances in which
only ``hasRigors`` and ``hasDyspnea`` were valid NLPQL features, it would
replace this single token with the string ``hasRigors AND hasDyspnea``.  If it
cannot perform the separation (such as with the token
``hasRigorsA3NDhasDyspnea``) it reports an error and writes error information
into the log file.

If the validity checks pass, the evaluator next determines the expression type.
The valid types are ``EXPR_TYPE_MATH``, ``EXPR_TYPE_LOGIC``, and
``EXPR_TYPE_MIXED``. If the expression type cannot be determined, the evaluator
reports an error and writes error information into the log file.

Subexpression Substitution
--------------------------

If the expression is of mixed type, the evaluator locates all simple math
subexpressions contained within and replaces them with temporary NLPQL feature
names, thereby converting math subexpressions to logic subexpressions. The
substitution process continues until all mathematical
subexpressions have been replaced with substitute NLPQL features, at which
point the expression type becomes ``EXPR_TYPE_LOGIC``.

To illustrate the substitution process, consider one of the examples from
above:
::
   Temperature.value >= 100.4 AND (hasRigors OR hasNausea) AND (LesionMeasurement.dimension_X >= 15)

This expression is of mixed type, since it contains the mathematical
subexpression ``Temperature.value >= 100.4``, the logic subexpression
``(hasRigors OR hasNausea)``, and the mathematical subexpression
``(LesionMeasurement.dimension_X >= 15)``. The NLPQL features in each math
subexpression, ``Temperature`` and ``LesionMeasurement``, also differ.

The evaluator identifies the Temperature subexpression and replaces it with a
substitute NLPQL feature, ``m0`` (for instance). This transforms the original
expression into:
::
   (m0) AND (hasRigors OR hasNausea) AND (LesionMeasurement.dimension_X >= 15)

Now only one mathematical subexpression remains.

The evaluator again makes a substitution ``m1`` for the remaining mathematical
subexpression, which converts the original into
::
   (m0) AND (hasRigors OR hasNausea) AND (m1)

This is now a pure logic expression.

Thus the substitution process transforms the original mixed-type
expression into three subexpressions, each of which is of simple math
or simple logic type:
::
   subexpression 1 (m0): 'Temperature.value >= 100.4'
   subexpression 2 (m1): 'LesionMeasurement.dimension_X >= 15'
   subexpression 3:      '(m0) AND (hasRigors OR hasNausea) AND (m1)'

By evaluating each subexpression in order, the result of evaluating the
original mixed-type expression can be obtained.

Evaluation of Mathematical Expressions
======================================

Removal of Unnecessary Parentheses
----------------------------------

The evaluator next removes all unnecessary pairs of parentheses from the
mathematical expression. A pair of parentheses is unnecessary if it can be
removed without affecting the result. The evaluator detects changes in the
result by converting the expression with a pair of parentheses removed to
postfix, then comparing the postfix form with that of the original. If the
postfix expressions match, that pair of parentheses was non-essential and
could be removed. The postfix form of the expression has no parentheses, as
described below.

Conversion to Explicit Form
---------------------------

After removal of nonessential parentheses, the evaluator rewrites the
expression so that the tokens match what's actually stored in the database.
This involves an explicit comparison for the NLPQL feature and the
unadorned use of the field name for variables. To illustrate, consider the
``hasFever`` example above:
::
   define hasFever:
       where Temperature.value >= 100.4;

The expression portion of this define statement is
``Temperature.value >= 100.4``. The evaluator rewrites this as:
::
   (nlpql_feature == Temperature) AND (value >= 100.4)

In this form the tokens match the fields actually stored in the task result
documents in MongoDB.

Conversion to Postfix
---------------------

Direct evaluation of an infix expression is complicated by parenthesization and
operator precedence issues. The evaluation process can be greatly simplified by
first converting the infix expression to postfix form. Postfix expressions
require no parentheses, and a simple stack-based evaluator can be used to
evaluate them directly.

Accordingly, a conversion to postifx form takes place next. This conversion
process requires an operator precedence table. The NLPQL operator precedence
levels match those of Python and are listed here for reference. Lower numbers
imply lower precedence, so ``or`` has a lower precedence than ``and``, which
has a lower precedence than ``+``, etc.

========  ================
Operator  Precedence Value
========  ================
or        1
and       2
<         4
<=        4
>         4
>=        4
!=        4
==        4
\+        9
\-        9
\*        10
/         10
%         10
^         12
========  ================

Conversion from infix to postfix is unambiguous if operator precedence and
associativity are known. Operator precedence is given by the table above.
All NLPQL operators are left-associative except for exponentiation, which is
right-associative. The infix-to-postfix conversion algorithm is the standard
one and can be found in the function ``_infix_to_postfix`` in the file
``nlp/data_access/expr_eval.py``.

After conversion to postfix, the ``hasFever`` expression becomes:
::
   'nlpql_feature', 'Temperature', '==', 'value', '100.4', '>=', 'and'


Generation of the Aggregation Pipeline
--------------------------------------

The next task for the evaluator is to convert the expression into a sequence of
MongoDB aggregation pipeline stages. This process involves the generation of an
initial `$match <https://docs.mongodb.com/manual/reference/operator/aggregation/match/>`_
query to filter out everything but the data for the current job. The match query
also checks for the existence of all entries in the field list and that they
have non-null values. **A simple existence check is not sufficient**, since a
null field actually exists but has a value that cannot be used for computation.
Hence checks for **existence** and a **non-null value** are both necessary.

For the ``hasFever`` example, the initial match query generates a pipeline
filter stage that looks like this, assuming a job_id of 12345:
::
   {
       $match : {
           "job_id" : 12345,
           "nlpql_feature" : {$exists:true, $ne:null},
           "value"         : {$exists:true, $ne:null}
       }
   }

This match pipeline stage runs first and performs coarse filtering on the
data in the result database. It finds only those task result documents
matching the specified job_id, and it further restricts consideration to
those documents having valid entries for the expression's fields.

Subsequent Pipeline Stages
--------------------------

After generation of the initial match filter stage, the postfix expression
is then 'evaluated' by a stack-based mechanism. The result of the evaluation
process is **not** the actual expression value, but instead a set of MongoDB
aggregation commands that tell MongoDB how to compute the result. The
evaluation process essentially generates Python dictionaries that obey the
aggregation syntax rules. More information about the aggregation pipeline can
be found `here <https://docs.mongodb.com/manual/aggregation/>`_.

The pipeline actually does a
`$project <https://docs.mongodb.com/manual/reference/operator/aggregation/project/>`_
operation and creates a new document with a Boolean field called ``value``.
This field has a value of True or False according to whether the source
document satisfied the mathematical expression. The ``_id`` field of the
projected document matches that of the original, so that a simple query on
these ``_id`` fields can be used to recover the desired documents.

The final aggregation pipeline for our example becomes:
::
    // (nlpql_feature == Temperature) and (value >= 100.4)
    {
       $match : {
           "job_id" : 12345
           "nlpql_feature" : {$exists:true, $ne:null},
           "value"         : {$exists:true, $ne:null}
       }
    },
    {
        "$project" : {
            "value" : {
                "$and" : [
                    {"$eq"  : ["$nlpql_feature", "Temperature"]},
                    {"$gte" : ["$value", 100.4]}
                ]
            }
        }
    }

The completed aggregation pipeline gets sent to MongoDB for evaluation.
Mongo performs the initial filtering operation, applies the subsequent
pipeline stages to all surviving documents, and sets the "value" Boolean
result. A final query extracts the matching documents and writes new result
documents with an ``nlpql_feature`` field equal to the label from the
``define`` statement, which for this example would be ``hasFever``.


Evaluation of Logic Expressions
===============================

The initial stages of the evaluation process for logic expressions proceed
similarly to those for mathematical expressions. Unnecessary parentheses are
removed and the expression is converted to postfix.

Detection of n-ary AND and OR
-----------------------------

After the postfix conversion, a pattern matcher looks for instances of n-ary
``AND`` and/or ``OR`` in the set of postfix tokens. An n-ary ``OR`` would look
like this, for n == 4:
::
   // infix
   hasRigors OR hasDyspnea OR hasTachycardia OR hasNausea

   // postfix
   hasRigors hasDyspnea OR hasTachycardia OR hasNausea OR

The n-value refers to the number of operands.  All such n-ary instances are
replaced with a variant form of the operator that includes the count. The
reason for this is that n-ary ``AND`` and ``OR`` can be handled easily by the
aggregation pipeline, and their use simplifies the pipeline construction
process. For this example, the rewritten postfix form would become:
::
   hasRigors hasDyspnea hasTachycardia hasNausea OR4

Generation of the Aggregation Pipeline
--------------------------------------

As with mathematical expressions, the logic expression aggregation pipeline
begins with an initial stage that filters on the job_id and checks that the
``nlpql_feature`` field exists and is non-null. No explicit field checks are
needed since logic expressions do not use NLPQL variables. For a job_id of
12345, this inital filter stage is:
::
   {
       "$match": {
           "job_id":12345
           "nlpql_feature": {"$exists":True, "$ne":None}
       }
   }

Following this is another filter stage that removes all docs not having the
desired NLPQL features. For the original logic expression example above:
::
   hasFever AND (hasDyspnea OR hasTachycardia)

this second filter stage would look like this:
::
   {
       "$match": {
           "nlpql_feature": {"$in": ['hasFever', 'hasDyspnea', 'hasTachycardia']}
       }
   }

Grouping by Value of the Context Variable
-----------------------------------------

The next stage in the logic pipeline is to group documents by the **value** of
the context field. Recall that NLPQL files specify a context of either
'document' or 'patient', meaning that a document-centric or patient-centric
view of the results is desired. In a document context, ClarityNLP needs to
examine all data pertaining to a given document. In a patient context, it needs
to examine all data pertaining to a given patient.

The grouping operation collects all such data (the ClarityNLP task result
documents) that pertain to a given document or a given patient. Documents are
distinguished by their ``report_id`` field, and patients are distinguished by
their patient IDs, which are stored in the ``subject`` field. **You can**
**think of these groups as being the 'evidence' for a given document or for**
**a given patient.** If the patient has the conditions expressed in the NLPQL
file, the evidence for it will reside in the group for that patient.

As part of the grouping operation ClarityNLP also generates a **set** of NLPQL
features for each group. This set is called the **feature_set** and it will be
used to evaluate the expression logic for the group as a whole.

The grouping pipeline stage looks like this:
::
   {
       "$group": {
           "_id": "${0}".format(context_field),

           # save only these four fields from each doc; more efficient
           # than saving entire doc, uses less memory
           "ntuple": {
               "$push": {
                   "_id": "$_id",
                   "nlpql_feature": "$nlpql_feature",
                   "subject": "$subject",
                   "report_id": "$report_id"
               }
           }, 
           "feature_set": {"$addToSet": "$nlpql_feature"}
       }
   }

Here we see the
`$group <https://docs.mongodb.com/manual/reference/operator/aggregation/group/>`_
operator grouping the documents on the value of the context field. An
**ntuple** array is generated for each different value of the context variable.
This is the 'evidence' as discussed above. Only the essential fields for each
document are used, which reduces memory consumption and improves efficiency.
We also see the generation of the feature set for each group, in which each
NLPQL feature for the group's documents is added to the set.

At the conclusion of this pipeline stage, each group has two fields: an
``ntuple`` array that contains the relevant data for each document in the
group, and a ``feature_set`` field that contains the distinct features for
the group.

Logic Operation Stage
---------------------

After the grouping operation, the logic operations of the expression are
applied to the elements of the feature set. If a particular patient
satisfies the ``hasFever`` condition, then at least one document in that
patient's group will have an NLPQL feature field with the value of
``hasFever``. Since all the distinct values of the NLPQL features for the
group are stored in the feature set, the feature set must also have an element
equal to ``hasFever``.

A check for set membership using aggregation syntax is expressed as:
::
   {"$in": ["hasFever", "$feature_set"]}

This construct means to use the
`$in <https://docs.mongodb.com/manual/reference/operator/aggregation/in/>`_
operator to test whether ``feature_set`` contains the element ``hasFever``.
The ``$in`` operator returns a Boolean result.

A successful test for feature set membership means that the patient has
the stated feature.

The evaluator implements the expression logic by translating it into a series
of set membership tests. For our example above, the logic operation pipeline
stage becomes:
::
   {
       '$match': {
           '$expr': {
               '$and': [
                   {'$in': ['hasFever', '$feature_set']},
                   {
                       '$or': [
                           {'$in': ['hasDyspnea', '$feature_set']},
                           {'$in': ['hasTachycardia', '$feature_set']}
                       ]
                   }
               ]
           }
       }
   }

Once again we have a match operation to filter the documents. Only those
documents satisfying the expression logic will survive the filter. The
`$expr <https://docs.mongodb.com/manual/reference/operator/query/expr/index.html>`_
operator allows the use of aggregation syntax in contexts where the standard
MongoDB query syntax would be required.

Following that we see a series of logic operations for our expression
``hasFever AND (hasDyspnea OR hasTachycardia)``.  The inner ``$or`` operation
tests the feature set for membership of ``hasDyspnea`` and ``hasTachycardia``.
If either or both are present, the ``$or`` operator returns True. The result of
the ``$or`` is then used in an ``$and`` operation which tests the feature set
for the presence of ``hasFever``. If it is also present, the ``$and`` operator
returns True as well, and the document in question survives the filter operation.

To summarize the evaluation process so far: ClarityNLP converts infix logic
expressions to postfix form and groups the documents by value of the context
variable. It uses a stack-based postfix evaluation mechanism to generate the
aggregation statements for the expression logic. Each logic operation is
converted to a test for the presence of an NLPQL feature in the feature set.

Document Sorting and Regrouping
-------------------------------

After the logic pipeline stage the evaluator sorts the documents in each group
on the value of the 'other' context variable. For a document context, the
documents in each surviving group would be sorted on the value of the patient
IDs, which are stored in the ``subject`` field. For a patient context, the
surviving groups would be sorted on the value of the document IDs, which are
stored in the ``report_id`` field. The sort is performed in the aggregation
pipeline to avoid having to do it externally.

To sort the documents the groups must first be temporarily `unwound`, sorted,
then regrouped. The
`$unwind <https://docs.mongodb.com/manual/reference/operator/aggregation/unwind/>`_
operator is used to flatten the ``$ntuple`` array so that the documents can
be sorted with the
`$sort <https://docs.mongodb.com/manual/reference/operator/aggregation/sort/>`_
operator. Following the sort, the documents are regrouped as before, this time
on the ``_id`` values for the previous groups. The ``$ntuple`` array is
reconstructed, but this time its members will be sorted on the value of the
'other' context variable. The ``$feature_set`` is also reconstructed. The
pipeline stages for these operations are:
::
   # sort on 'other' context variable (compared as strings)
   {"$unwind": "$ntuple"},
   {"$sort": {sort_field: 1}},

   # Restore the previous groups and feature_sets
   {
       "$group": {
           "_id": "$_id",
           "ntuple": {"$push": "$ntuple"},
           "feature_set": {"$addToSet": "$ntuple.nlpql_feature"}
       }
   }

With these operations the pipeline is complete. The full pipeline for our
example is:
::
   // pipeline for hasFever AND (hasDyspnea OR hasTachycardia)

   // filter documents on job_id and check validity of the nlpql_feature field
   {
       "$match": {
           "job_id":12345
           "nlpql_feature": {"$exists":True, "$ne":None}
       }
   },

   // filter docs on the desired NLPQL feature values
   {
       "$match": {
           "nlpql_feature": {"$in": ['hasFever', 'hasDyspnea', 'hasTachycardia']}
       }
   },

   // group docs by value of context variable, construct feature set
   {
       "$group": {
           "_id": "${0}".format(context_field),
           "ntuple": {
               "$push": {
                   "_id": "$_id",
                   "nlpql_feature": "$nlpql_feature",
                   "subject": "$subject",
                   "report_id": "$report_id"
               }
           }, 
           "feature_set": {"$addToSet": "$nlpql_feature"}
       }
   },

   // perform expression logic on the feature set
   {
       '$match': {
           '$expr': {
               '$and': [
                   {'$in': ['hasFever', '$feature_set']},
                   {
                       '$or': [
                           {'$in': ['hasDyspnea', '$feature_set']},
                           {'$in': ['hasTachycardia', '$feature_set']}
                       ]
                   }
               ]
           }
       }
   },

   // sort on the 'other' context field value, regroup
   {"$unwind": "$ntuple"},
   {"$sort": {sort_field: 1}},
   {
       "$group": {
           "_id": "$_id",
           "ntuple": {"$push": "$ntuple"},
           "feature_set": {"$addToSet": "$ntuple.nlpql_feature"}
       }
   }

Result Generation
-----------------

