NLPQL Expression Evaluation
***************************

Overview
========

In this section we describe the mechanisms that ClarityNLP uses to evaluate
NLPQL expressions. NLPQL expressions are found in ``define`` statments such as:
::
   define hasFever:
       where Temperature.value >= 100.4;

   define hasSymptoms:
       where hasFever AND (hasDyspnea OR hasTachycardia);
       
The expressions are the strings in each statement between the ``where`` keyword
and the semicolon:
::
   Temperature.value >= 100.4

   hasFever AND (hasDyspnea OR hasTachycardia)

NLPQL expressions can either be mathematical or logical in nature, as these
examples illustrate.

Recall that the processing stages for a ClarityNLP job proceed roughly as
follows:

1. Parse the NLPQL file and determine which NLP tasks to run.
2. Formulate a Solr query to find relevant source documents, partition the
   source documents into batches, and assign batches to computational tasks.
3. Run the tasks in parallel and write individual task results to MongoDB.
   Each individual result from an NLP task comprises a *task result document*
   in the Mongo database. The term *document* is used here in the MongoDB
   sense, meaning an object containing key-value pairs. The MongoDB 'documents'
   should not be confused with the Solr source documents, which are electronic
   health records.
4. Evaluate NLPQL expressions using the task result documents as the source
   data. Write expression evaluation results to MongoDB as separate result
   documents.

Thus ClarityNLP evaluates expressions **after** all tasks have finished running
and have written their individual results to MongoDB. The expression evaluator
consumes this task data and uses it to generate new results from the expression
statements.

We now turn our attention to a description of how the expression evaluator
works. We should state at the outset that the descriptions below apply to the
expression evaluator built on
`MongoDB aggregation <https://docs.mongodb.com/manual/aggregation/>`_. This
evaluator is currently in a testing phase and must be explicitly enabled by
adding the following line to the ``project.cfg`` file in the ``[local]``
section:
::
   [local]
   evaluator=mongo

A Pandas-based evaluator will be used if this line is absent or is commented
out with a `#` character. The Pandas evaluator uses different techniques from
those described below.

Why use MongoDB aggregation to evaluate NLPQL expressions? The basic reason
is that the data resides in a Mongo collection, and it is more efficient to
evaluate expressions using capabilities provided by MongoDB than to use
something else. Use of a non-Mongo evaluator would require us to:

- Run a set of queries to extract the data from MongoDB
- Transmit the query results across a network if the Mongo instance is hosted
  remotely
- Ingest the query results into another evaluation engine
- Evaluate the NLPQL expressions and generate results
- If MongoDB is hosted remotely, transmit the results back to the Mongo host
- Insert the results into MongoDB.
   
Evaluation via the MongoDB aggregation framework should prove to be much more
efficient than this process, since all data remains inside MongoDB.


NLPQL Expression Types
======================

In the descriptions below we refer to NLPQL **variables**, which have the
form ``nlpql_feature.field_name``. The NLPQL feature is a label introduced in a
``define`` statement. The ``field_name`` is the name of an output value
generated by the task associated with the NLPQL feature. Thus the variable
``Temperature.value`` has an NLPQL feature equal to ``Temperature`` and a field
name of ``value``. The `ClarityNLP ValueExtractor <https://claritynlp.readthedocs.io/en/latest/developer_guide/algorithms/value_extraction.html>`_ would be used to extract temperature values.
According to the `ValueExtractor API documentation <https://claritynlp.readthedocs.io/en/latest/api_reference/nlpql/valueextractor.html>`_ , the extracted value is returned in
a field named ``value``, hence the variable name ``Temperature.value``.


1. Simple Mathematical Expressions
-------------------------------------

A simple mathematical expression is a string containing NLPQL variables,
operators, parentheses, or numeric literals. Some examples:
::
   Temperature.value >= 100.4
   (Meas.dimension_X > 5) AND (Meas.dimension_X < 20)
   (0 == Temperature.value % 20) OR (1 == Temperature.value % 20)

The variables in a simple mathematical expression all refer to a **single**
NLPQL feature.

Simple mathematical expressions produce a result from data contained in a
**single** task result document. The result of the expression evaluation is
written to a new MongoDB result document.

2. Simple Logic Expressions
-----------------------------

A simple logic expression is a string containing NLPQL features,
parentheses, and the logic operators ``AND`` and/or ``OR``. For instance:
::
   hasRigors OR hasDyspnea
   hasFever AND (hasDyspnea OR hasTachycardia)
   (hasShock OR hasDyspnea) AND (hasTachycardia OR hasNausea)

Logic expressions operate on high-level NLPQL features, **not** on numeric
literals or NLPQL variables. The presence of a numeric literal or NLPQL
variable indicates that the expression is either a mathematical expression
or possibly invalid.

Simple logic expressions produce a result from data contained in one or more
task result documents. The result from the expression evaluation is written to
one or more new MongoDB result documents (the details will be explained below).
   
3. Mixed Expressions
--------------------

A *mixed* expression is a string containing either:

- A mathematical expression **and** a logic expression
- A mathematical expression using variables involving two or more NLPQL features

For instance:
::
   // both math and logic
   (Temperature.value >= 100.4) AND (hasDyspnea OR hasTachycardia)

   // two NLPQL features: LesionMeasurement and Temperature
   (LesionMeasurement.dimension_X >= 10) OR (Temperature.value >= 100.4)

   // math, logic, and multiple NLPQL features
   Temperature.value >= 100.4 AND (hasRigors OR hasNausea) AND (LesionMeasurement.dimension_X >= 15)

The evaluation mechanisms used for mathematical, logic, and mixed expressions
are quite different. To fully understand the issues involved, it is helpful to
first understand the meaning of the 'intermediate' and 'final' phenotype
results.

Phenotype Result CSV Files
--------------------------

Upon submission of a new job, ClarityNLP prints information to stdout that
looks similar to this:
::
    HTTP/1.0 200 OK
    Content-Type: text/html; charset=utf-8
    Content-Length: 1024
    Access-Control-Allow-Origin: *
    Server: Werkzeug/0.14.1 Python/3.6.4
    Date: Fri, 23 Nov 2018 18:40:38 GMT
    {
       "job_id": "11108",
       "phenotype_id": "11020",
       "phenotype_config": "http://localhost:5000/phenotype_id/11020",
       "pipeline_ids": [
            12529,
            12530,
            12531,
            12532,
            12533,
            12534,
            12535
        ],
        "pipeline_configs": [
            "http://localhost:5000/pipeline_id/12529",
            "http://localhost:5000/pipeline_id/12530",
            "http://localhost:5000/pipeline_id/12531",
            "http://localhost:5000/pipeline_id/12532",
            "http://localhost:5000/pipeline_id/12533",
            "http://localhost:5000/pipeline_id/12534",
            "http://localhost:5000/pipeline_id/12535"
        ],
        "status_endpoint": "http://localhost:5000/status/11108",
        "results_viewer": "?job=11108",
        "luigi_task_monitoring": "http://localhost:8082/static/visualiser/index.html#search__search=job=11108",
        "intermediate_results_csv": "http://localhost:5000/job_results/11108/phenotype_intermediate",
        "main_results_csv": "http://localhost:5000/job_results/11108/phenotype"
    }

Here we see various items relevant to the job submission. Each submission
receives a *job_id*, which is a unique numerical identifier for the run.
ClarityNLP writes all task results from all jobs to the ``phenotype_results``
collection in a Mongo database named ``nlp``. The job_id is
needed to distinguish the data belonging to each run. Results can be extracted
directly from the database by issuing `MongoDB queries <https://docs.mongodb.com/manual/tutorial/query-documents/>`_.

We also see URLs for 'intermediate' and 'main' phenotype results. These are
convenience APIs that export the results to CSV files. The data in the
intermediate result CSV file contains the output from each NLPQL
task not marked as ``final``. The main result CSV contains the results
from any final tasks or final expression evaluations. The CSV file can be
viewed in Excel or in another spreadsheet application.

Each NLP task generates a result document distinguished by a particular value
of the ``nlpql_feature`` field. The *define* statement
::
   define hasFever:
        where Temperature.value >= 100.4;

generates a set of rows in the intermediate CSV file with the
``nlpql_feature`` field set to ``hasFever``.  The NLP tasks
::
    // nlpql_feature 'hasRigors'
    define hasRigors:
        Clarity.ProviderAssertion({
            termset: [RigorsTerms],
            documentset: [ProviderNotes]
        });

    // nlpql_feature 'hasDyspnea
    define hasDyspnea:
        Clarity.ProviderAssertion({
            termset: [DyspneaTerms],
            documentset: [ProviderNotes]
        });

generate two blocks of rows in the CSV file, the first block having the
``nlpql_feature`` field set to ``hasRigors`` and the next block having it
set to ``hasDyspnea``.  The different nlpql_feature blocks appear in order
as listed in the source NLPQL file. The presence of these nlpql_feature
blocks makes locating the results of each NLP task a relatively simple
matter.

Expression Evaluation Algorithms
================================

Expression Tokenization and Parsing
-----------------------------------

The NLPQL front end parses the NLPQL file and sends the raw expression text
to the evaluator (``nlp/data_access/expr_eval.py``). The evaluator module
parses the expression text and converts it to a fully-parenthesized token
string. The tokens are separated by whitespace and all operators are replaced
by string mnemonics (such as ``GE`` for the operator ``>=``, ``LT`` for the
operator ``<``, etc.).

If the expression includes any subexpressions involving numeric literals, they
are evaluated at this stage and the literal subexpression replaced with the
result.

Validity Checks
---------------

The evaluator then runs validity checks on each token. If it finds a token that
it does not recognize, it tries to resolve it into a series of known NLPQL
features separated by logic operators. For instance, if the evaluator were
to encounter the token ``hasRigorsANDhasDyspnea`` under circumstances in which
only ``hasRigors`` and ``hasDyspnea`` were valid NLPQL features, it would
replace this single token with the string ``hasRigors AND hasDyspnea``.  If it
cannot perform the separation (such as with the token
``hasRigorsA3NDhasDyspnea``) it reports an error and writes error information
into the log file.

If the validity checks pass, the evaluator next determines the expression type.
The valid types are ``EXPR_TYPE_MATH``, ``EXPR_TYPE_LOGIC``, and
``EXPR_TYPE_MIXED``. If the expression type cannot be determined, the evaluator
reports an error and writes error information into the log file.

Subexpression Substitution
--------------------------

If the expression is of mixed type, the evaluator locates all simple math
subexpressions contained within and replaces them with temporary NLPQL feature
names, thereby converting math subexpressions to logic subexpressions. The
substitution process continues until all mathematical
subexpressions have been replaced with substitute NLPQL features, at which
point the expression type becomes ``EXPR_TYPE_LOGIC``.

To illustrate the substitution process, consider one of the examples from
above:
::
   Temperature.value >= 100.4 AND (hasRigors OR hasNausea) AND (LesionMeasurement.dimension_X >= 15)

This expression is of mixed type, since it contains the mathematical
subexpression ``Temperature.value >= 100.4``, the logic subexpression
``(hasRigors OR hasNausea)``, and the mathematical subexpression
``(LesionMeasurement.dimension_X >= 15)``. The NLPQL features in each math
subexpression also differ.

The evaluator identifies the Temperature subexpression and replaces it with a
substitute NLPQL feature, ``m0`` (for instance). This transforms the original
expression into:
::
   (m0) AND (hasRigors OR hasNausea) AND (LesionMeasurement.dimension_X >= 15)

Now only one mathematical subexpression remains.

The evaluator again makes a substitution ``m1`` for the remaining mathematical
subexpression, which converts the original into
::
   (m0) AND (hasRigors OR hasNausea) AND (m1)

This is now a pure logic expression.

Thus the substitution process transforms the original mixed-type
expression into three subexpressions, each of which is of simple math
or simple logic type:
::
   subexpression 1 (m0): 'Temperature.value >= 100.4'
   subexpression 2 (m1): 'LesionMeasurement.dimension_X >= 15'
   subexpression 3:      '(m0) AND (hasRigors OR hasNausea) AND (m1)'

By evaluating each subexpression in order, the result of evaluating the
original mixed-type expression can be achieved.

Evaluation of Mathematical Expressions
======================================


Initial Pipeline Stage
----------------------

The next task for the evaluator is to convert the expression into a sequence of
MongoDB aggregation pipeline stages. This process involves the generation of an
initial ``$match`` query to filter out everything but the data for the current
job. The match query also checks for the existence of all entries in the field
list and that they have non-null values. **A simple existence check is not**
**sufficient**, since a null field actually exists but has a value that cannot
be used for computation. Hence checks for existence and a non-null value are
both necessary.

For the two examples above, the initial ``$match`` query generates a pipeline
filter stage that looks like this, assuming a job_id of 11116:
::
   // first example
   {
       $match : {
           "job_id" : 11116,
           "nlpql_feature" : {$exists:true, $ne:null},
           "value"         : {$exists:true, $ne:null}
       }
   }

   // second example
   {
       $match : {
           "job_id" : 11116,
           "nlpql_feature" : {$exists:true, $ne:null},
           "dimension_X"   : {$exists:true, $ne:null},
           "dimension_Y"   : {$exists:true, $ne:null}
       }
   }

This ``$match`` pipeline stage runs first and performs coarse filtering on the
data in the MongoDB result database. It finds only those task result documents
matching the specified job_id, and it further restricts consideration to
those documents having valid entries for the expression's fields.

Note that the validity checks imply that any fields used in NLPQL expressions
will only generate results if valid entries for those fields exist. For the
LesionMeasurement statement above, if a task result measurement is missing the
Y dimension, the NLPQL statement will not generate a result for that
particular measurment. The NLQPL example below will help make this clear.

Subsequent Pipeline Stages
--------------------------

After generation of the initial ``$match`` filter stage, the expression is
further transformed so that additional MongoDB aggregation pipeline stages
can be generated to evaluate it. The ``nlpql_feature`` is extracted and
inserted as an additional matching operation. For the examples above, the
evaluator rewrites the expressions as:
::
   (nlpql_feature == Temperature) and (value >= 100.4)
   (nlpql_feature == LesionMeasurement) and (dimension_X < 5 and dimension_Y < 5)

In this form the variables used in each statement match the fields
actually stored in the task result documents in MongoDB.

Note that both of these expressions are in infix form. Direct evaluation of an
infix expression is complicated by parenthesization and operator precedence
issues. Evaluation of a mathematical expression is greatly simplified by first
converting to postfix form. Postfix expressions require no parentheses, and a
simple stack-based evaluator can be used to evaluate them directly.

Accordingly, a conversion to postifx form takes place next. This conversion
process requires an operator precedence table. The NLPQL operator precedence
levels match those of Python and are listed here for reference. Lower numbers
imply lower precedence, so ``or`` has a lower precedence than ``and``, which
has a lower precedence than ``+``, etc.

========  ================
Operator  Precedence Value
========  ================
or        1
and       2
not       3
<         4
<=        4
>         4
>=        4
!=        4
==        4
\+        9
\-        9
\*        10
/         10
%         10
^         12
========  ================

Conversion from infix to postfix is unambiguous if operator precedence and
associativity are known. Operator precedence is given by the table above.
All NLPQL operators are left-associative except for exponentiation, which is
right-associative. The infix-to-postfix conversion algorithm is the standard
one and can be found in the function ``_infix_to_postfix`` in the file
``nlp/data_access/mongo_eval.py``.

After conversion to postfix, the two expressions above become lists of tokens:
::
   'nlpql_feature', 'Temperature', '==', 'value', '100.4', '>=', 'and'
   'nlpql_feature', 'LesionMeasurement', '==', 'dimension_X', '5', '<', 'dimension_Y', '5', '<', 'and', 'and'


The postfix expressions are then 'evaluated' by a stack-based mechanism, which
can be found in the function ``_to_mongo_pipeline`` in the file
``nlp/data_access/mongo_eval.py``. The result of the evaluation process is
**not** the actual expression value, but a set of MongoDB aggregation commands
that tell MongoDB how to compute the result. The evaluation process is
essentially string formatting that follows the aggregation syntax rules. More
information about the aggregation pipeline can be found here:
https://docs.mongodb.com/manual/aggregation/.

The pipeline actually does a ``$project`` operation and creates a new document
with a Boolean field called ``value``.  This field has a value of True or False
according to whether the source document satisfied the mathematical expression.
The ``_id`` field of the projected document matches that of the original, so
that a simple query on these ``_id`` fields can be used to recover the desired
documents.

After generation of the MongoDB commands, the aggregation pipelines for the two
examples above become:
::
    // (nlpql_feature == Temperature) and (value >= 100.4)
    {
       $match : {
           "job_id" : 11116,
           "nlpql_feature" : {$exists:true, $ne:null},
           "value"         : {$exists:true, $ne:null}
       }
    },
    {
        "$project" : {
            "value" : {
                "$and" : [
                    {"$eq"  : ["$nlpql_feature", "Temperature"]},
                    {"$gte" : ["$value", 100.4]}
                ]
            }
        }
    }
    
    // (nlpql_feature == LesionMeasurement) and (dimension_X < 5 and dimension_Y < 5)
    {
        "$match" : {
            "job_id" : 11116,
            "nlpql_feature" : {$exists:true, $ne:null},
            "dimension_X"   : {$exists:true, $ne:null},
            "dimension_Y"   : {$exists:true, $ne:null}
        }
    },
    {
        "$project" : {
            "value" : {
                "$and" : [
                    {
                        "$eq" : ["$nlpql_feature", "LesionMeasurement"]
                    },
                    {
                        "$and" : [
                            {"$lt" : ["$dimension_X", 5]},
                            {"$lt" : ["$dimension_Y", 5]}
                        ]
                    }
                ]
            }
        }
    }

The completed aggregation pipeline stages are sent to MongoDB for evaluation.
Mongo performs the initial filtering operation, applies the subsequent
pipeline stages to all surviving documents, and sets the "value" Boolean
result. A final query extracts the matching documents and writes new result
documents with an ``nlpql_feature`` field equal to that of the single-row
operation.

NLPQL Example 1 - Mathematical Expressions
------------------------------------------

Now let's look at an example. Suppose we would like to search radiology
reports for lesions of various sizes. To do this we must first create a termset
for finding 'lesion' and related words in the narrative text of the reports.
The NLPQL should invoke the MeasurementFinder to extract any
measurements associated with those terms. Some mathematical
expressions for setting conditions on the sizes of the lesions will also
be necessary.

Here is an NLPQL file satisfying these criteria:
::
    limit 50;
    phenotype "Lesion Example" version "1";
    description "Find lesions of various sizes.";
    include ClarityCore version "1.0" called Clarity;

    // use radiology reports for the source documents
    documentset Docs:
        Clarity.createDocumentSet({
            "report_types":["Radiology"]
        });

    // search the text for these lesion-related terms
    termset LesionTerms: [
        "lesion", "growth", "mass", "malignancy", "tumor", "neoplasm",
        "nodule", "cyst", "focus of enhancement", "echodensity",
        "hyperechogenic focus"
    ];

    // extract lesion measurements in 1D, 2D, and 3D
    define LesionMeasurement:
        Clarity.MeasurementFinder({
            documentset: [Docs],
            termset: [LesionTerms]
        });

    // want to find patients, so use patient context
    context Patient;

    ///////// 1D ///////////

    define has1DLesionLt5mm:
        where LesionMeasurement.dimension_X < 5;

    define has1DLesionGe10andLe25mm:
        where LesionMeasurement.dimension_X >= 10 AND LesionMeasurement.dimension_X <= 25;

    define has1DLesionGt30mm:
        where LesionMeasurement.dimension_X > 30;

    ///////// 2D ///////////

    define has2DLesionLt5mm:
        where LesionMeasurement.dimension_X < 5 AND
              LesionMeasurement.dimension_Y < 5;

    define has2DLesionGe10andLe25mm:
        where LesionMeasurement.dimension_X >= 10 AND LesionMeasurement.dimension_X <= 25 AND
              LesionMeasurement.dimension_Y >= 10 AND LesionMeasurement.dimension_Y <= 25;

    define has2DLesionGt30mm:
        where LesionMeasurement.dimension_X > 30 AND
              LesionMeasurement.dimension_Y > 30;

    ///////// 3D ///////////

    define has3DLesionLt5mm:
        where LesionMeasurement.dimension_X < 5 AND
              LesionMeasurement.dimension_Y < 5 AND
              LesionMeasurement.dimension_Z < 5;

    define has3DLesionGe10andLe25mm:
        where LesionMeasurement.dimension_X >= 10 AND LesionMeasurement.dimension_X <= 25 AND
              LesionMeasurement.dimension_Y >= 10 AND LesionMeasurement.dimension_Y <= 25 AND
              LesionMeasurement.dimension_Z >= 10 AND LesionMeasurement.dimension_Z <= 25;

    define has3DLesionGt30mm:
        where LesionMeasurement.dimension_X > 30 AND
              LesionMeasurement.dimension_Y > 30 AND
              LesionMeasurement.dimension_Z > 30;

This NLPQL file begins with four lines of boilerplate followed by the creation
of a document set for radiology reports. A termset containing common
descriptive forms for lesions appears next. This termset is not exhaustive by
any means, but it suffices for demonstration purposes.

The ``LesionMeasurement`` statement activates the ClarityNLP measurement
finder, which searches the text looking for 1D, 2D, and 3D measurements. Any
measurements that it finds are written into task result documents and stored
to MongoDB. More information on the fields generated by the measurement finder
can be found here: :ref:`measurementfinder`.

The context statement follows the measurement finder task specification.
A ``Patient`` context means that we are interested in finding patient ids
that satisfy the stated NLPQL conditions.

The mathematical expressions follow the context statement. These expressions
are arranged in 1D, 2D, and 3D groups.  In each group there is an expression
that finds lesions < 5mm, lesions between 10 and 25 mm, and lesions greater
than 30mm. The 2D and 3D groups impose these conditions on each component
of the measurement. As described above, these components must exist and must
have non-null values for a result to be generated.

A sample run of this NLPQL file on the MIMIC-III data set found a set of 147
lesion measurements. Of these, focusing on the 1D results, there were 23
measurements satisfying the criterion for ``has1DLesionLt5mm``, 49 for
the ``has1DLesionGe10andLe25mm``, and 27 for ``has1DLesionGt30mm``.

The result set for ``has1DLesionGe10andLe25mm`` contained a 2D measurement
of dimensions 24 x 38 mm. This is acceptable, since the expression for
membership in this set checks only that the X component exists, is non-null,
and satisfies the expression. This particular measurement does not appear
in the set ``has2DLesionGe10andLe25mm``, since the expression for that result
validates both the X and Y dimensions.

Similarly, the result set for ``has1DLesionGt30mm`` includes a 3D
measurement of dimensions 39 x 12 x 35 mm. This is again a valid result
since only the X-component of the measurement is validated and checked by the
expression. This result neither appears in the result set for
``has2DLesionGt30mm`` or ``has3DLesionGt30mm``, since both of those expressions
validate the Y-component.


Evaluation of Logic Expressions
===============================

Multi-row expressions apply the logical operations ``AND``, ``OR``, and ``NOT``
to **sets** of MongoDB result documents. Typically the sets are determined by
the different values of the ``nlpql_feature`` field. In the lesion example above,
a multi-row operation that looks for small or large 3D lesions would be written
::
   define has3DSmallOrLargeLesion:
       where has3DLesionLt5mm OR has3DLesionGt30mm;

This logical ``OR`` operates on two sets of results. The first set
contains all result documents in ``has3DLesionLt5mm``, and the second set
contains all result documents in ``has3DLesionGt30mm``. The result of this
logical OR is a new set of documents, each of which satisfies the logical
OR condition individually.

Document Filtering and Grouping
-------------------------------

Evaluation of an n-ary logical OR proceeds by filtering result documents by
the job_id, similar to the process described above for single-row expressions.
Next, an additional filter stage is applied that discards all documents whose
``nlpql_feature`` value differs from those of the sets being OR'd together.
Any documents that remain are grouped by value of the context variable, which
is the document_id for a ``Document`` context, or the subject field for a
``Patient`` context.

Evaluation of an n-ary logical AND proceeds similarly, except the number of
documents in each group is counted. Any groups not having n members
for an n-ary logical AND are discarded. Additionally, any groups containing
duplicate nlpql_features are discarded as well. Only those document groups
with n members and n different nlpql_features are kept.

The logical NOT operation is used to compute set differences, such as in
``A NOT B``.  This expression generates a result set that contains all
documents in set A but not also in set B. Evaluation of a set difference
proceeds by first filtering by the nlpql_feature fields, as described above
for logical AND and OR. The records are grouped by the context variable
(either the ``document_id`` or ``subject`` field), and then any documents
having an nlpql_feature in set B are discarded.

After these filtering operations the aggregation pipeline emits a set of
documents grouped by **value** of the context variable. For a patient context,
the documents are grouped by value of the ``subject`` field. For a document
context, the documents are grouped by value of the ``report_id`` field. This
grouping operation is similar to the grouping performed by a database join
operation.

Next, the documents in each group are sorted on the value of the 'other'
context variable. Thus for a patient context the documents in each group are
sorted on the ``report_id`` field. This sort operation generates subgroups
of documents sharing the same value of the 'other' field.

To summarize the state of the result documents at this point: all surviving
documents have been filtered and separated into groups. The members of each
group all share identical values of the context variable. Within each group,
the documents are further separated into subgroups. The documents in each
subgroup have identical values of the 'other' context variable.

NTuple Formation
----------------




