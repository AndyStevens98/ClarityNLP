#!/usr/bin/env python3
"""


OVERVIEW:


The code in this module searches a sentence for size measurements and attempts
to determine the subject(s) of those measurements. A 'size measurement' is a
1D, 2D, or 3D expression involving lengths, such as:

        3 mm                                    (1D measurement)
        1.2 cm x 3.6 cm                         (2D measurement)
        2 by 3 by 4 mm                          (3D measurement)
        1.1, 2.3. 8.5, and 12.6 cm              (list of lengths)
        1.5 cm2, 4.3 mm3                        (area and volume)
        2.3 - 4.5 cm                            (range)
        1.5 cm craniocaudal x 2.2 cm transverse (measurement with views)

A measurement 'subject' is the object with those dimensions.

All numeric values are converted to mm, mm2, or mm3 in the JSON results.

The sentence and one or more query terms are specified on the command 
line. A Boolean result is provided in the JSON output indicating 
whether any of the query terms match the measurement subjects.

All term matching is case-insensitive.


OUTPUT:


The set of JSON fields in the output includes:

        sentence           the sentence from which measurements were extracted
        terms              comma-separated list of query terms
        querySuccess       whether any query terms matched a measurement subject
        measurementCount   number of size measurements found
        measurementList    array of individual size measurements

            text           text of the complete size measurement
            start          offset of first char in the matching text
            end            offset of final char in the matching text plus 1

            temporality    indication of when measurement occurred
                           values: 'CURRENT', 'PREVIOUS'

            units          units of the x, y, and z fields
                           values: 'MILLIMETERS', 'SQUARE_MILLIMETERS', 
                                   'CUBIC_MILLIMETERS'

            condition      numeric ranges will have this field set to 'RANGE'
                           all other measurements will set this field to 'EQUAL'

            matchingTerm   an array of all matching query terms
            subject        an array of strings, the possible measurement subjects
            location       string; location of the object being measured

            x              numeric value of first measurement dimension
            y              numeric value of second measurement dimension
            z              numeric value of third measurement dimension
            values         JSON array of all numeric values in a size list

            x_view         view specification for x value
            y_view         view specification for y value
            z_view         view specification for z value

All JSON results will contain the same number of fields.

Fields that are not applicable to a given sentence will have a value of 
EMPTY_FIELD.

JSON results are written to stdout.


USAGE:


This code requires the SpaCy natural language processing module. SpaCy can be
downloaded and installed by following the instructions at the SpaCy website:

        https://spacy.io/

You will also need to download and install SpaCy's English model files.
Open a terminal and run this command after installing SpaCy:

        python3 -m spacy download en


To use this code as an imported module, add the following lines to the
import list in the importing module:

        import json
        import spacy
        import subject_finder as sf

Before processing any sentences, call init with the path to the Clarity 
ngram file:

        sf.init('path/to/clarity_ngrams.txt')

The file 'clarity_ngrams.txt' is generated by ngram_gen.py. If this file
cannot be found, this module generates a FileNotFound exception.


To process a sentence and capture the JSON result:

        json_string = sf.run(term_string, sentence)

        The term string is a comma-separated list of search terms, i.e.
        "cyst, node, kidney".

To unpack the JSON result:

        json_data = json.loads(json_string)
        result = sf.SubjectFinderResult(**json_data)

        The following fields are now available:

            result.sentence
            result.terms
            result.querySuccess
            result.measurementCount
            result.measurementList

To unpack the measurement list:

        meas_list = result.measurementList
        measurements = [sf.Measurement(**m) for m in meas_list]

The fields of each measurement are now accessible via:

        for m in measurements:
            print(m.text)
            print(m.start)
            print(m.end)
            etc.

        If any field has the value EMPTY_FIELD it should be ignored.



COMMAND-LINE USAGE EXAMPLES:



To run the self tests:

        python3 ./subject_finder.py --selftest

        All self-tests passed if no output is generated.

To search for measurements of the spleen using this module's test sentences:

        python3 ./subject_finder.py -t "spleen" --test

To search for a term in a sentence provided on the command line:

        python3 ./subject_finder.py -t "cyst" -s "The cyst in the lower pole of the left kidney measures 1.3 x 1.1 cm."

To visualize a dependency parse of the previous sentence:

        python3 ./subject_finder.py -t "cyst" -s "The cyst in the lower pole of the left kidney measures 1.3 x 1.1 cm." --displacy

        View the dependency parse by opening a browser to the URL 'localhost:5000'. 
        Press <CTRL>C to continue processing after viewing the diagram.

        The words in the sentence depicted in the visualization may contain
        simple word substitutions for medical ngrams. To disable these
        substitutions, run this command:

        python3 ./subject_finder.py -t "cyst" -s "The cyst in the lower pole of the left kidney measures 1.3 x 1.1 cm." --displacy --nosub

        Disabling the ngram substitions may change the dependency parse, though. 
        In general, the use of ngram substitutions results in simpler sentences
        and helps SpaCy produce better results.

"""

import re
import os
import sys
import json
import spacy
import optparse
from collections import namedtuple
from spacy.symbols import ORTH, LEMMA, POS, TAG

if __name__ is not None and "." in __name__:
    from .size_measurement_finder import run as smf_run, SizeMeasurement, STR_PREVIOUS
else:
    from size_measurement_finder import run as smf_run, SizeMeasurement, STR_PREVIOUS
    
FILE_DIR = os.path.dirname(__file__)

# imports from clarity
# import size_measurement_finder as smf

# debug only
from spacy import displacy

# load Spacy's English model
nlp = spacy.load('en')

VERSION_MAJOR = 0
VERSION_MINOR = 1

# set to True to enable debug output
TRACE = False

# serializable result object
# 'measurementList' is an array of Measurement namedtuples
EMPTY_FIELD = -1
MEASUREMENT_RESULT_FIELDS = ['sentence', 'terms', 'querySuccess',
                             'measurementCount', 'measurementList']
SubjectFinderResult = namedtuple('SubjectFinderResult', MEASUREMENT_RESULT_FIELDS)

MEASUREMENT_FIELDS = ['text', 'start', 'end', 'temporality', 'units',
                      'condition', 'matchingTerm', 'subject', 'location',
                      'x', 'y', 'z', 'values', 'x_view', 'y_view', 'z_view']
Measurement = namedtuple('Measurement', MEASUREMENT_FIELDS)


# match anything between {} and [], including the brackets
str_brackets_1 = r'\[[^\]]+\]'
str_brackets_2 = r'\{[^\}]+\}'
str_brackets = r'(' + str_brackets_1 + r'|' + str_brackets_2 + r')'
regex_brackets = re.compile(str_brackets)

# match parentheses
str_parens = r'[()]'
regex_parens = re.compile(str_parens)

# image annotations (image 302:33), (782b:49), etc.
regex_image = re.compile(r'\(\s*(image\s+)?[a-z\d]+[:;\s]+[a-z\d]+\)')

# one month ago, two weeks ago, etc.
str_ago = r'\b(one|two|three|four|five|six|seven|eight|nine|ten|eleven|' + \
          r'twelve)\s+(day|week|month|year|)s?\s+ago'
regex_ago = re.compile(str_ago)

# common punctuation (no hyphens)
regex_punctuation = re.compile(r'[.,:;()%\[\]_?!\"]')

# regexes for identifying blather that can be removed
blather_regexes = [
    re.compile(r'\b(additional(ly)?|also|another|apparently|approximately|' + \
               r'approx\.?|about|however|immediately|predominantly)\b'),
    re.compile(r'\bas\s+well\s+as\b'),
    re.compile(r'\b(what|there)\s+(may|might|could)\s+(possibly\s+)?be\b'),
    re.compile(r'\bin\s+addition'),
    re.compile(r'\bother\s+than'),
    re.compile(r'\bare\s+[-a-zA-Z\s]+(noted|identified)'),
    re.compile(r'\bfor\s+example\b'),
    re.compile(r'\Aagain\s+seen\s+(is|are)'),
    re.compile(r'\Amost\s+representative\s(is|are)'),
    re.compile(r'\b(no\s+)?evidence\s+of\s+')
]

# verbosity that can be simplified
verbosity_map = {
    re.compile(r'\bmeasuring\s+upwards\s+of\b') : ['measuring'],
    re.compile(r'\bmeasures\s+upwards\s+of\b')  : ['measures'],
    re.compile(r'\bmeasured\s+upwards\s+of\b')  : ['measured'],
    re.compile(r'\bmeasure\s+upwards\s+of\b')   : ['measure'],
    re.compile(r'\bupwards\s+of\b')             : [' '],
    re.compile(r'\bmore\s+so\b')                : ['more'],
    re.compile(r'\bsmall\s+to\s+moderate\b')    : ['small'],
    re.compile(r'\bappears\s+to\s+be\b')        : ['is'],
    re.compile(r'\bcan\s+be\s+seen\b')          : ['is'],
    re.compile(r'\bno\s+evidence\s+of\b')       : ['no'],
    re.compile(r'\binvolving\b')                : ['in'],
    re.compile(r'\bidentified\s+within\b')      : ['inside'],
    re.compile(r'\bright\s+at\s+the\b')         : ['at', 'the'],
    re.compile(r'\bas\s+compared\s+to\b')       : ['compared', 'to'],
    re.compile(r'\bappears\s+unchanged\b')      : ['is', 'unchanged'],
    re.compile(r'\bis\s+seen\s+in\b')           : ['in'],
    re.compile(r'\bexcept\s+to\s+note\b')       : ['except'],
    re.compile(r'\bare\s+again\s+noted\s+to\s+be\b')        : ['are'],
    re.compile(r'\bare\s+again\s+noted\s+arising\s+from\b') : ['in'],
    re.compile(r'\bare\s+again\s+noted\s+at\b')             : ['at'],
    re.compile(r'\bare\s+again\s+noted\s+in\b')             : ['in'],
    re.compile(r'\bare\s+again\s+noted\b')                  : ['exist'],
    re.compile(r'\barising\s+from\b')                       : ['in'],
    re.compile(r'\bagain\s+seen\s+is\b')        : ['there', 'is'],
    re.compile(r'\bis\s+again\s+seen\b')        : ['exists'],
    re.compile(r'\bpresenting\s+with\b')        : ['with'],
    re.compile(r'\branging\s+in\s+size\b')      : ['measuring'],
    re.compile(r'\bconsistent\s+with\s')        : ['like'],
    re.compile(r'\bis\s+seen\s+to\s+contain\b') : ['contains'],
    re.compile(r'\bmeasuring\s+with\b')         : ['with']
}

# roman numerals; must be preceded by whitespace and followed either by
# punctuation or whitespace
str_roman_numeral = r'\s+' + r'(?P<numeral_text>'        +\
                    r'(i|ii|iii|iv|v|vi|vii|viii|ix)'    +\
                    r')' + r'[.,:;?!%]?\s+'
regex_roman_numeral = re.compile(str_roman_numeral)
roman_numeral_map = {
    'i':'1', 'ii':'2', 'iii':'3', 'iv':'4',
    'v':'5', 'vi':'6', 'vii':'7', 'viii':'8', 'ix':'9'
}

# adjectives causing an incorrect dependency parse
str_problem_adjectives = r'\b(diffuse|fusiform|hilar|marked|nondistended)\b'
regex_problem_adjectives = re.compile(str_problem_adjectives)

# colors to be substituted for problematic medical adjectives
COLORS = ['red', 'green', 'blue', 'white', 'yellow', 'orange', 'pink']

# word replacements made during processing
replacements = {}

# liver segments
str_liver_seg_roman_num = r'([i]{1,3}|iv[a,b]?|v[i]{0,3})'
str_liver_seg_decimal = r'(1|2|3|4|4a|4b|5|6|7|8)'
str_liver_seg_num = r'(' + str_liver_seg_roman_num + r'|' + str_liver_seg_decimal + r')'
str_liver_seg = r'\b((liver|hepatic)\s+)?seg(ment)?\s+(?P<segnum>' + str_liver_seg_num + r')\b'
regex_liver_seg = re.compile(str_liver_seg)
liver_seg_map = {
    'i':'1', 'ii':'2', 'iii':'3', 'iv':'4',
    'iva':'4a', 'ivb':'4b', 'v':'5','vi':'6', 'vii':'7', 'viii':'8'
}

# Match various word sequences, including hyphenated words and abbreviations.
# A 'word' can also be a number or a measurement with a '/' character, such as
# a blood pressure value.
str_word = r'\b([-/.a-z\d]+[,;:%?!]?\s+)'
str_words = str_word + r'{0,12}?'          # nongreedy
str_words_g = r'(\b[-/.a-z\d]+\b[,;:%?!]?\s*){0,12}'   # greedy
str_words_0_to_n = r'(\b[-/.a-z\d]+\b[,;:%?!]?\s+)*?'  # 0 or more words, nongreedy
str_words_1_to_n = str_word + r'+?'        # 1 or more words, nongreedy

# measurement verbs
str_meas_verb = r'\b(measure[ds]?|measuring)\s+'

# measurement qualifiers
str_qualifiers = r'(up\s+to\b|all|approximately|approx\.?\b|about|'           +\
                 r'currently|dominant|mainly|maximally|minimally|mostly|'     +\
                 r'of\b|occasionally|only)\s+'

str_meas_qualifiers = r'(' + str_qualifiers + r'){0,3}'

# recognize sentences containing a measurement verb and 1-3 measurements
str_measures_m_1 = r'(' + str_words_1_to_n + str_meas_verb + str_words_0_to_n + r'\bM\s*' + r')'
str_measures_m_2 = str_measures_m_1 + r'{2}'
str_measures_m_3 = str_measures_m_1 + r'{3}'

regex_measures_m_1 = re.compile(str_measures_m_1)
regex_measures_m_2 = re.compile(str_measures_m_2)
regex_measures_m_3 = re.compile(str_measures_m_3)

# now vs. then sentence template
str_now_vs_then_1 = r'\b((now|currently|today)/s+)?' + str_meas_qualifiers    +\
                    r'\b(measures?|measuring)\s+' + str_meas_qualifiers + r'\bM'
str_now_vs_then_21 = r'\b(compared\s+to|with)\s+' + str_meas_qualifiers + r'\bM\s+previously'
str_now_vs_then_22 = r'\b(previously|formerly)\s+(had\s+)?(measured|measuring)\s+' +\
                     str_meas_qualifiers + r'\bM'
str_now_vs_then_23 = r'\b(previously|formerly)\s+' + str_meas_qualifiers + r'\bM'
str_now_vs_then_24 = r'\bprior\s+measurement\s+of\s+' + str_meas_qualifiers + r'\bM'
str_now_vs_then_25 = r'\bM\s+' + str_meas_qualifiers + r'\b(then|on\s+date)'
str_now_vs_then_26 = r'\bM\s+' + str_meas_qualifiers + r'\b(in|on)\s+(the\s+)?prior'

str_now_vs_then_2  = r'(' + str_now_vs_then_21 + r'|' + str_now_vs_then_22    +\
                     r'|' + str_now_vs_then_23 + r'|' + str_now_vs_then_24    +\
                     r'|' + str_now_vs_then_25 + r'|' + str_now_vs_then_26 + r')'

regex_now_vs_then_1 = re.compile(str_now_vs_then_1);
regex_now_vs_then_2 = re.compile(str_now_vs_then_2);

# subjects before and after the measurements
str_before_and_after = r'\b((a|the)\s+)?' + r'(?P<subject1>' + str_words + r')' +\
                       str_meas_verb + str_meas_qualifiers + r'\bM\s+'        +\
                       str_words_0_to_n + r'\b(a|an)\s+M\s+'                  +\
                       r'(?P<subject2>' + str_words_g + r')'
regex_before_and_after = re.compile(str_before_and_after)

# M and M templates

str_m_and_m_1 = r'(?P<text1>' + str_words_0_to_n + str_meas_verb              +\
                str_meas_qualifiers + r'\bM\s+' + str_words_0_to_n + r')'     +\
                r'\band\s+' + r'(?P<text2>' + str_words_0_to_n                +\
                str_meas_qualifiers + r'\bM\s*' + str_words_g + r')'
regex_m_and_m_1 = re.compile(str_m_and_m_1)

# special form: {subject} measures M and {verb} to M
str_m_and_m_2 = r'(?P<text1>' + str_words_1_to_n + str_meas_verb             +\
                str_meas_qualifiers + r'\bM\s+' + str_words_0_to_n + r')'    +\
                r'\band\s+' + str_words_1_to_n + r'\bto\s+'                  +\
                str_meas_qualifiers + r'\bM\s*' + str_words_g
regex_m_and_m_2 = re.compile(str_m_and_m_2)
        
# 'a M {words}' and 'a {words} M' templates

str_a_m_terminations = r'(?=\b(a|an|measure[ds]?|measuring|M)\b)'

# match constructs such as "a M node in the right low paratracheal station",
# either alone or in a list
str_a_m_wds_1 = r'\b(an?|the)\s+' + str_meas_qualifiers + r'\bM\s+'          +\
                r'(?P<wordsNG>' + str_words_1_to_n + r')'                    +\
                str_a_m_terminations
str_a_m_wds_2 = r'\b(an?|the)\s+' + str_meas_qualifiers + r'\bM\s+'          +\
                r'(?P<wordsG>' + str_words_g + r')'
str_a_m_wds = r'(' + str_a_m_wds_1 + r'|' + str_a_m_wds_2 + r')'
regex_a_m_wds = re.compile(str_a_m_wds)

# match constructs such as "an unchanged M hyperechoic focus..."
str_a_wds_m_1 = r'\b(an?|the)\s+(?P<words1>' + str_words_1_to_n + r')'       +\
                str_meas_qualifiers + r'\bM\s+' + r'(?P<words2>'             +\
                str_words_1_to_n + r')' + str_a_m_terminations
str_a_wds_m_2 = r'\b(an?|the)\s+(?P<words3>' + str_words_1_to_n + r')'       +\
                str_meas_qualifiers + r'\bM\s+' + r'(?P<words4>'             +\
                str_words_g + r')'
str_a_wds_m = r'(' + str_a_wds_m_1 + r'|' + str_a_wds_m_2 + r')'
regex_a_wds_m = re.compile(str_a_wds_m)


# in largest dimension, in size, in largest axis, on inspiratory imaging, etc.
str_size_or_image = r'\b(in|on|with)\s+' + str_words_0_to_n                  +\
                    r'(\b(axis|diameter|dimension|imaging|length|width|'     +\
                    'height|size|area|vol(ume)?)\s*){1,2}(of\s*)?'
regex_size_or_image = re.compile(str_size_or_image)

# carina
str_endo_tube = r'\b(?P<endotube>(et|endo(tracheal)?)\s+tube)\s+'
str_carina = str_endo_tube + str_words_0_to_n + str_meas_qualifiers          +\
             r'\bM\s+(just\s+)?(above|from)\s+(the\s+)?carina'
regex_endo_tube = re.compile(str_endo_tube)
regex_carina = re.compile(str_carina)



# match ngram lengths (from the ngram file)
regex_length = re.compile(r'\A#\s+length:\s+(?P<num>\d+)')

# replacement for dates - capitalize? TBD
STR_DATE = r'date'

# prepositional dependencies, conjuncts, direct objects
IGNORE_DEPS = set(['acomp', 'attr', 'conj', 'dobj', 'pcomp', 'pobj', 'prep'])

# dependencies to ignore when resolving a general term
RESOLVE_DEPS = set(['acl', 'pcomp', 'pobj', 'prep'])

# prepositional dependencies
MEAS_VERB_DEPS = set(['dobj', 'pcomp', 'pobj', 'prep'])

# special handling for these deps preceded by 'with'
WITH_DEPS = set(['acl', 'pcomp', 'pobj', 'prep'])

# general terms for tokens that need more specific resolution, if possible
GENERAL_TERMS = set(['area', 'volume', 'size', 'mass'])

# default ngram file
NGRAM_FILE = 'clarity_ngrams.txt'

ngram_dict = {}
ngram_min_chars = 9999999
ngram_word_counts = []

# replacement nouns - ensure none are in the ngram file
ngram_replacements = ['car', 'city', 'year', 'news', 'math', 'hall',
                      'poet', 'fact', 'idea', 'oven', 'poem', 'dirt', 'tale',
                      'world', 'hotel']

# displacy use flag
ENABLE_DISPLACY = False

###############################################################################
class Meas():
    """
    The measurement objects manipulated by this code.
    """

    # initialize with a SizeMeasurement namedtuple
    def __init__(self, sm):

        # from SizeMeasurement
        self.text         = sm.text
        self.start        = sm.start
        self.end          = sm.end
        self.temporality  = sm.temporality
        self.units        = sm.units
        self.condition    = sm.condition
        self.matchingTerm = []
        self.subject      = []
        self.location     = EMPTY_FIELD
        self.x            = sm.x
        self.y            = sm.y
        self.z            = sm.z
        self.values       = sm.values
        self.x_view       = sm.x_view
        self.y_view       = sm.y_view
        self.z_view       = sm.z_view


###############################################################################
def init(ngram_file_path=NGRAM_FILE):
    """
    Initialize this module. Must be called once, prior to  sentence processing.
    """

    global ngram_word_counts

    load_ngram_file(ngram_file_path, ngram_dict)
    ngram_word_counts = sorted(ngram_dict.keys())

    if TRACE:
        print('ngram min chars: {0}'.format(ngram_min_chars))
        for n in range(1, len(ngram_word_counts)+1):
            print('Number of ngrams of length {0:2}: {1:6}'.format(n, len(ngram_dict[n])))
    
    # 'measures' is a 3rd person singular present verb
    special_case = [{ORTH: u'measures', LEMMA: u'measure', TAG: u'VBZ', POS: u'VERB'}]
    nlp.tokenizer.add_special_case(u'measures', special_case)

    # 'measure' is a non 3rd person singular present verb
    special_case = [{ORTH: u'measure', LEMMA: u'measure', TAG: u'VBP', POS: u'VERB'}]
    nlp.tokenizer.add_special_case(u'measure', special_case)

    # 'measured' is a verb, past participle
    special_case = [{ORTH: u'measured', LEMMA: u'measure', TAG: u'VBN', POS: u'VERB'}]
    nlp.tokenizer.add_special_case(u'measured', special_case)

    # 'measuring' is a verb form, either a gerund or present participle
    special_case = [{ORTH: u'measuring', LEMMA: u'measure', TAG: u'VBG', POS: u'VERB'}]
    nlp.tokenizer.add_special_case(u'measuring', special_case)    
    
###############################################################################
def load_ngram_file(filepath, ngram_dict):

    global ngram_min_chars
    
    path = os.path.join(FILE_DIR, "%s" % NGRAM_FILE)
    with open(path, 'rt') as fin:
        
        n = -1
        for line in fin:

            if line.startswith('#'):
                # read the ngram length
                match = regex_length.match(line)
                if match:
                    n = int(match.group('num'))
                else:
                    if TRACE:
                        print('Error reading {0}: length expected: {1}'.
                              format(NGRAM_FILE, line))
                    sys.exit(-1)
                ngram_dict[n] = []
                continue

            # append ngram of length n to its list
            ngram = line.rstrip()
            ngram_dict[n].append(ngram)

            char_count = len(ngram)
            if char_count < ngram_min_chars:
                ngram_min_chars = char_count

###############################################################################
def to_json(original_terms, original_sentence, measurements):
    """
    Convert the results to a JSON string.
    """

    result_dict = {}
    result_dict['sentence'] = original_sentence
    result_dict['measurementCount'] = len(measurements)
    result_dict['terms'] = original_terms

    # check for presence of query terms in the meas subjects
    terms_lc = [t.lower() for t in original_terms]

    # Determine if search terms match the subject. A match is declared if
    # a search term is a component of the subject string. For instance,
    # search term 'kidney' would match a subject of 'right kidney'.
    found_it = False
    for m in measurements:
        for t in terms_lc:
            # m.subject is a list of strings
            for subject_term in m.subject:
                if -1 != subject_term.find(t):
                    m.matchingTerm.append(t)
                    found_it = True

    result_dict['querySuccess'] = found_it

    dict_list = []
    for m in measurements:
        m_dict = {}

        for field in MEASUREMENT_FIELDS:
            m_dict[field] = getattr(m, field)
        
        dict_list.append(m_dict)
    
    # serialize everything to JSON
    result_dict['measurementList'] = dict_list
    return json.dumps(result_dict, indent=4)

###############################################################################
def print_token(token):
    """
    Print useful token data to the screen for debugging.
    """

    print('[{0:3}]: {1:30}\t{2:6}\t{3:8}\t{4:12}\t{5}'.format(token.i,
                                                              token.text,
                                                              token.tag_,
                                                              token.pos_,
                                                              token.dep_,
                                                              token.head))
    
###############################################################################
def print_tokens(doc):
    
    print('{0:7}{1:30}\t{2:6}\t{3:8}\t{4:12}\t{5}'.format('INDEX', 'TOKEN', 'TAG',
                                                          'POS', 'DEP', 'HEAD'))
    for token in doc:
        print_token(token)

###############################################################################
def erase(sentence, start, end):
    """
    Overwrite characters [start, end) with whitespace.
    """
    piece1 = sentence[:start]
    piece2 = ' '*(end-start)
    piece3 = sentence[end:]
    return piece1 + piece2 + piece3

###############################################################################
def collapse_ws(sentence):
    """
    Replace repeated whitespace chars with a sigle space.
    """

    return re.sub(r'\s+', ' ', sentence)

###############################################################################
def get_meas_count(sentence):
    """
    Count the number of measurements in the sentence or fragment.
    """

    count = 0
    for c in sentence:
        if 'M' == c:
            count += 1
    return count

###############################################################################
def replace_verbosity(sentence):
    """
    Replace verbose forms with simpler expressions, preserving sentence length.
    """

    for regex in verbosity_map:
        iterator = regex.finditer(sentence)
        for match in iterator:
            matching_text = match.group()
            num_chars = len(matching_text)

            # replacement word list associated with this regex
            word_list = verbosity_map[regex]

            # if multiple words, join with a single space
            if len(word_list) > 1:
                replacement = ' '.join(word_list)
            else:
                replacement = word_list[0]

            piece1 = sentence[:match.start()]
            piece2 = replacement
            piece3 = ' '*(num_chars - len(replacement))
            piece4 = sentence[match.end():]
            sentence = piece1 + piece2 + piece3 + piece4

    return sentence

###############################################################################
def replace_preserving_length(regex, sentence, str_new):
    """
    Replace all text matched by the regex without changing sentence length.
    """

    iterator = regex.finditer(sentence)
    for match in iterator:
        text = match.group()
        start = match.start()
        end   = match.end()
        
        piece1 = sentence[0:start]
        
        pad_char_count = (end - start - len(str_new))
        assert pad_char_count >= 0
        
        piece2 = str_new    
        piece3 = ' ' * pad_char_count
        piece4 = sentence[end:]
        sentence = piece1 + piece2 + piece3 + piece4

    return sentence

###############################################################################
def clean_sentence(sentence):
    """
    Attempt to clean up the sentence to make the subject finder's task easier.
    Some of these transformations do NOT preserve the sentence length.
    """

    # convert to lowercase
    sentence = sentence.lower()

    # erase image annotations
    iterator = regex_image.finditer(sentence)
    for match in iterator:
        sentence = erase(sentence, match.start(), match.end())

    # remove anything in [] or {}, such as anonymized dates
    sentence = replace_preserving_length(regex_brackets, sentence, STR_DATE)
        
    # erase parens from the sentence, but keep any text inbetween
    sentence = regex_parens.sub(' ', sentence)

    # erase "one month ago" and similar expressions
    iterator = regex_ago.finditer(sentence)
    for match in iterator:
        sentence = erase(sentence, match.start(), match.end())

    # erase blather
    for regex in blather_regexes:
        iterator = regex.finditer(sentence)
        for match in iterator:
            sentence = erase(sentence, match.start(), match.end())

    # replace verbosity with simpler expressions padded with spaces
    sentence = replace_verbosity(sentence)

    # replace liver segments in roman numerals with decimals
    iterator = regex_liver_seg.finditer(sentence)
    for match in iterator:
        text = match.group()
        segnum = match.group('segnum')
        start = match.start('segnum')
        end   = match.end('segnum')
        if segnum in liver_seg_map:
            decimal_num = liver_seg_map[segnum]
            piece1 = sentence[0:start]
            piece2 = decimal_num
            piece3 = ' '*(end - start - len(decimal_num))
            piece4 = sentence[end:]
            sentence = piece1 + piece2 + piece3 + piece4
            new_text = re.sub(segnum, decimal_num, text)
            replacements[new_text] = text
            if TRACE:
                print('\treplaced {0} with {1}'.format(segnum, decimal_num))
                print('\tnew match text: ->{0}<-'.format(new_text))
    
    # replace any problem adjectives with colors
    substitutions = []
    iterator = regex_problem_adjectives.finditer(sentence)
    for match in iterator:
        substitutions.append(match.group())

    for adj in substitutions:
        for color in COLORS:
            # if color not already in sentence, use as the replacement adj
            if -1 != sentence.find(color):
                sentence = re.sub(adj, color, sentence)
                replacements[color] = adj
                if TRACE: print("Replaced '{0}' with '{1}'".format(adj, color))
                break

    # replace roman numerals in narrative sections with numbers
    iterator = regex_roman_numeral.finditer(sentence)
    for match in iterator:
        text = match.group('numeral_text')
        start = match.start('numeral_text')
        end   = match.end('numeral_text')
        piece1 = sentence[0:start]
        digit = roman_numeral_map[text]
        piece2 = digit
        piece3 = ' '*(end - start - len(digit))
        piece4 = sentence[end:]
        sentence = piece1 + piece2 + piece3 + piece4

    # SpaCy sometimes gives dramatically different results depending on whether
    # semicolons are present in the sentence or not, so replace semicolons
    # with whitespace.
    sentence = re.sub(r';', r' ', sentence)
    
    return sentence

###############################################################################
def find_child_candidates(token):
    """
    Examine the syntactic children of the given node and find the child most 
    suitable to be a measurement subject.
    """

    candidates = []
    
    for child in token.children:
        if TRACE: print_token(child)
        # want nouns and superlative adjectives such as 'largest'
        if 'NOUN' != child.pos_ and 'JJS' != child.tag_:
            continue
        elif 'M' == child.text:
            continue
        else:
            candidates.append(child)

    num_candidates = len(candidates)
    return candidates

###############################################################################
def resolve_superlative_adj(m_token, adj_token):
    """
    Determine what a superlative adjective such as 'largest' refers to.
    Return the result as a list.
    """
    
    # follow a chain of prepositions up the tree to a terminating noun

    root = None
    token = m_token.head
    subject_list = None

    if TRACE: print('resolve_superlative_adj: starting...')
    while True:
        if 'NOUN' == token.pos_ and not token.dep_ in RESOLVE_DEPS:
            if TRACE: print('\tresolved to noun: "{0}"'.format(token.text))
            subject_list = [token]
            break
        elif 'ROOT' == token.dep_:
            if TRACE: print('\tresolved to root: "{0}"'.format(token.text))
            root = token
            break
        else:
            if TRACE: print('\tskipping "{0}", continuing...'.format(token.text))
            token = token.head

    if root is not None:
        if TRACE: print('\tresolving to child candidates of root {0}'.format(root.text))
        subject_list = find_child_candidates(root)

    if 0 == len(subject_list):
        subject_list = [adj_token]
        
    return subject_list

###############################################################################
def get_meas_subj(m_token_index, doc):
    """
    Use information in the dependency parse tree to find what's being measured.
    """

    root = None
    do_child_search = False

    # If the current M token is not its own head, start there. If it is its
    # own head, walk backwards through the token list and find the nearest
    # verb and start there.
    m_token = doc[m_token_index]
    if m_token.head != m_token:
        token = m_token.head
    else:
        found_verb = False
        for i in reversed(range(m_token_index)):
            token = doc[i]
            if 'VERB' == token.pos_:
                found_verb = True
                break
        if not found_verb:
            if TRACE: print('no verb found, returning empty subject...')
            return []
        
    prev_dep = m_token.dep_

    # any other candidate subjects
    extra_candidates = []

    if TRACE:
        print('starting token: {0}'.format(token.text))
    
    while True:
        if TRACE: print_token(token)
        if 'ROOT' == token.dep_:
            # subject of measurement is a child of this node
            if TRACE: print('\tat root token')
            root = token
            break
        elif 'VERB' == token.pos_:
            if 'nsubj' == token.dep_:
                if TRACE: print('\tverb with nsubj dep')
                break
            elif -1 != token.text.find('meas'):
                # measurement verb
                if token.head and 'NOUN' == token.head.pos_ and token.head.dep_ not in MEAS_VERB_DEPS:
                    if TRACE: print('\tbreak on meas verb')
                    token = token.head
                    break
                elif token.head and token.head.text == 'with':
                    if TRACE: print('exit at with prior to meas verb')
                    do_child_search = True
                    break
        elif 'NOUN' == token.pos_ and prev_dep in WITH_DEPS:
            if token.head and 'with' == token.head.text:
                # special case - break on 'with'
                if TRACE: print('extra candidate at check-with: {0}'.format(token)) #print('\tbreak at check-with')
                extra_candidates.append(token)
                prev_dep = token.dep_
                token = token.head
                continue
                #break
            elif 'dobj' == token.dep_:
                if TRACE: print('\tbreak on dobj at check-with')
                break;
        elif 'NOUN' == token.pos_ and token.dep_ in IGNORE_DEPS:
            # this noun is part of an ignorable dependency
            if TRACE: print('\tcontinue at NOUN and ignorable dep: "{0}"'.format(token.dep_))
            prev_dep = token.dep_
            token = token.head
            continue
        elif 'NOUN' == token.pos_ and 'acl' == prev_dep:
            # prevous deps modify this noun, candidate subject of meas
            if TRACE: print('\tbreak at NOUN and acl dep')
            break
        elif token.dep_ in IGNORE_DEPS:
            if TRACE: print('\tcontinue at ingorable dep: "{0}"'.format(token.dep_))
            prev_dep = token.dep_
            token = token.head
            continue
        elif 'NOUN' != token.pos_:
            if TRACE: print('\tcontinue at POS == "{0}", not NOUN'.format(token.pos_))
            prev_dep = token.dep_
            token = token.head
            continue
        else:
            if TRACE: print('\tdefault break')
            break

        if TRACE: print('\tcontinuing up the tree')
        prev_dep = token.dep_
        token = token.head

    candidates = []

    if root is not None:
        # subject is potentially either the root or a child of the root
        if TRACE:
            print('get_meas_subj: root is not None')
        if 'NOUN' == root.pos_:
            candidates.append(root)
        else:
            candidates = find_child_candidates(root)
    else:

        if token.text in GENERAL_TERMS:
            if TRACE:
                print('get_meas_subj: attempt resolution of general term "{0}"'.format(token.text))
            token = resolve_superlative_adj(m_token, token)[0]
            candidates.append(token)
        elif not do_child_search:
            if TRACE:
                print('get_meas_subj: appending token')
            candidates.append(token)
        elif do_child_search:
            candidates = find_child_candidates(token)

    if len(extra_candidates) > 0:
        candidates.extend(extra_candidates)
            
    return candidates

###############################################################################
def tokenize_and_find_subjects(sentence):
    """
    """

    # save a copy of the original sentence
    original_sentence = sentence
    
    # tokenize and produce a dependency parse of the sentence
    doc = nlp(sentence)

    if TRACE:
        print_tokens(doc)
        
    if ENABLE_DISPLACY:
        displacy.serve(doc, style='dep')
        
    meas_subjects = []

    first_try = True
    num_tokens = len(doc)

    # find the M tokens and resolve the subject of each
    
    i = 0
    while i < num_tokens:
        tok = doc[i]
        if 'M' == tok.text:
            meas_subject = get_meas_subj(i, doc)

            # if no subject found, remove punctuation and try again
            if 0 == len(meas_subject) and first_try:
                if TRACE: print('no subject found, retrying...')
                sentence2 = regex_punctuation.sub(' ', sentence)
                doc = nlp(sentence2)
                i = 0
                num_tokens = len(doc)
                first_try = False
                continue

            # if subject is a single superlative adjective, find what it refers to
            if 1 == len(meas_subject):
                if 'JJS' == meas_subject[0].tag_:
                    if TRACE:
                        print('resolving superlative adj: {0}'.format(meas_subject[0]))
                    meas_subject = resolve_superlative_adj(tok, meas_subject[0])
                    
            meas_subjects.append(meas_subject)
            
        i += 1
    
    return meas_subjects

###############################################################################
def get_ngrams(words, n):
    """
    Return a list of all possible ngrams (of size n) from the given wordlist.
    The ngrams are returned as strings containing words separated by a single
    space.
    """
    result = []

    for i in range(0, len(words) - n + 1):
        result.append(' '.join(words[i:i+n]))

    return result

###############################################################################
def compute_ngrams(sentence):
    """
    Search the sentence for ngrams from the ngram file. Replace any ngrams
    found with a single noun from the 'ngram_replacements' list.
    """

    original_sentence = sentence
    max_ngram_length = ngram_word_counts[-1]
    
    # skip over common words at the start of the sentence
    str_common_start = r'\b(an?|the|there\s(is|are|was|were)(\san?)?)\b'
    regex_common_start = re.compile(str_common_start)
    match = regex_common_start.match(sentence)
    if match:
        sentence = sentence[match.end():]

    start = -1
    matches = []
    for i in range(len(sentence)):
        c = sentence[i:i+1]

        # accept only lowercase letters, spaces and hyphens
        if 'M' != c and (c.isalpha() or ' ' == c or '-' == c):
            if -1 == start:
                start = i
            continue
        else:
            # skip if too short
            if i - start < ngram_min_chars:
                start = -1
                continue
            if -1 == start:
                continue
            
            chunk = sentence[start:i]
            words = chunk.split()
            max_n = min( len(words), max_ngram_length)
            
            # find ngrams and substitute, work largest to smallest
            for n in reversed(ngram_word_counts):
                if n <= max_n:
                    ngrams_n = get_ngrams(words, n)

                    search_start = 0
                    for ngram in ngrams_n:
                        if ngram in ngram_dict[n]:

                            # search for ngrams as isolated words
                            iterator = re.finditer(r'\b' + ngram + r'\b', chunk)
                            for match in iterator:
                                n_start = start + match.start()
                                n_end   = start + match.end()

                                # check esisting matches for overlap
                                has_overlap = False
                                for t_ngram, t_start, t_end in matches:
                                    if n_start >= t_start and n_end <= t_end:
                                        has_overlap = True
                                        break
                                if not has_overlap:
                                    matches.append( (ngram, n_start, n_end))
                                    if TRACE:
                                        print('\tngram match: {0}'.format(ngram))
            start = -1

    # replace matching words in sentence
    sentence = original_sentence
    
    pos = 0
    index = 0
    for old_wd, start, end in matches:
        new_wd = ngram_replacements[index]
        sentence = re.sub(r'\b' + old_wd + r'\b', new_wd, sentence)
        replacements[new_wd] = old_wd
        if TRACE: print('\tReplaced {0} with {1}'.format(old_wd, new_wd))
        index += 1

    return sentence
    
###############################################################################
def run(term_string, sentence, nosub=False, use_displacy=False):
    """
    Do the main work of this module.
    """

    global ENABLE_DISPLACY

    if use_displacy:
        ENABLE_DISPLACY = True

    # save a copy of the original sentence, needed for JSON output
    original_sentence = sentence

    # if any search terms, split on comma into individual words
    if term_string and len(term_string) > 0:
        terms = term_string.split(',') # produces a list
        terms = [term.strip() for term in terms]
    else:
        terms = []

    # save a copy of the original terms, needed for JSON output
    original_terms = terms.copy()

    # convert terms to lowercase
    terms = [term.lower() for term in terms]
    
    sentence = clean_sentence(sentence)
    
    # find all size measurements
    json_string = smf_run(sentence)
    json_data = json.loads(json_string)
    size_measurements = [SizeMeasurement(**m) for m in json_data]

    if TRACE:
        print('SizeMeasurements: ')
        for sm in size_measurements:
            print('\t{0}'.format(sm))

    # convert from immutable SizeMeasurement namedtuple to mutable Meas
    measurements = [Meas(sm) for sm in size_measurements]
            
    # if no measurements then no measurement subjects
    if 0 == len(measurements):
        return to_json(original_terms, original_sentence, [])

    # replace measurement text with <space>M<space+>, preserves sentence length
    for m in measurements:
        num_chars = len(m.text)
        piece1 = sentence[:m.start]
        piece2 = ' M' + ' '*(num_chars - 2)
        piece3 = sentence[m.end:]                        
        sentence = piece1 + piece2 + piece3
        
    # replace \s+ with a single space, could change sentence length
    sentence_ss = collapse_ws(sentence)

    # do ngram substitutions unless the 'nosub' flag is set
    if not nosub:
        sentence_ss = compute_ngrams(sentence_ss)
    
    # save a copy, used to find context later
    m_sentence = sentence_ss
    
    if TRACE:
        print('Sentence prior to analysis: ')
        print('\t{0}'.format(sentence_ss))

    meas_subjects = []        
    m_count = get_meas_count(sentence_ss)

    # try to find subject of each measurement
    ok = False
    if 3 == m_count:
         ok = process_3(m_sentence, sentence_ss, measurements)
    elif 2 == m_count:
         ok = process_2(m_sentence, sentence_ss, measurements)
    elif 1 == m_count:
        ok = process_1(m_sentence, sentence_ss, measurements)

    if not ok:
        if TRACE: print('\tno subject found, using default')
        set_default_subject(m_sentence, sentence_ss, measurements)

    # remove duplicates, convert to string, and undo replacements
    for m in measurements:
        subj_list = m.subject
        subj_list = [t.text for t in set(subj_list)]
        if len(replacements) > 0:
            for new_text, old_text in replacements.items():
                for i in range(len(subj_list)):
                    if subj_list[i] == new_text:
                        subj_list[i] = old_text
        m.subject = subj_list

    # convert to JSON result
    return to_json(original_terms, original_sentence, measurements)

###############################################################################
def set_default_subject(m_sentence, sentence_ss, measurements):
    """
    Find all nouns in the sentence as use them as the measurement subject.
    """

    # generate a dependency parse of the sentence
    doc = nlp(sentence_ss)

    noun_list = []
    for token in doc:
        if 'NOUN' == token.pos_:
            noun_list.append(token)

    # remove duplicates and keep lowercase words only (no 'M')
    # words could be abbreviations or contain hyphens
    noun_list = [n for n in set(noun_list) if re.match(r'[-.a-z]+', n.text)]

    for m in measurements:
        m.subject = noun_list.copy()

###############################################################################
def process_3(m_sentence, sentence, measurements):
    """
    Find subjects of three measurements.
    """

    if TRACE: print('called process_3')

    # check for three independent "measures M' clauses
    matcher_3 = regex_measures_m_3.search(sentence)
    if matcher_3:

        if TRACE: print('process_3: MEASURES_M_3 match')
        
        count = 0
        prev_subject = []
        iterator = regex_measures_m_1.finditer(sentence)
        for match in iterator:

            if TRACE:
                print('process_3: text for iteration {0}: {1}'.
                      format(count, match.group()))
            subjects = tokenize_and_find_subjects(match.group())
            if TRACE: print('subjects for iteration: {0}: {1}'.
                            format(count, subjects))

            m_index = m_index_from_context(m_sentence, match.group())
            assert m_index < len(measurements)
            
            #assert len(subjects) > 0
            if subjects[0] and len(subjects[0]) > 0:
                # found subject for this measurement
                for s in subjects[0]:
                    measurements[m_index].subject.append(s)
                prev_subject = subjects[0]
            elif count > 0:
                # no subjects found on this iteration, so use previous subject
                for s in prev_subject:
                    measurements[m_index].subject.append(s)
                    
            count += 1

        return True

    # try 'a M wds' and 'a wds M' forms...
    found_it, sentence = process_a_m_wds(m_sentence, sentence, measurements)
    m_count = get_meas_count(sentence)
    if found_it and 0 == m_count:
        return True

    found_it, sentence = process_a_wds_m(m_sentence, sentence, measurements)
    m_count = get_meas_count(sentence)
    if found_it and 0 == m_count:
        return True

    if 2 == m_count:
        return process_2(m_sentence, sentence, measurements)
    elif 1 == m_count:
        return process_1(m_sentence, sentence, measurements)
    else:
        return False
            
###############################################################################
def process_2(m_sentence, sentence, measurements):
    """
    Find subjects of two measurements.
    """

    if TRACE: print('called process_2')
    
    # check for a now-vs-then sentence form
    matcher_nvt1 = regex_now_vs_then_1.search(sentence)
    if matcher_nvt1:
        if TRACE: print('process_2: NVT1 match')
        text2 = sentence[matcher_nvt1.end():]
        matcher_nvt2 = regex_now_vs_then_2.search(text2)
        if matcher_nvt2:
            if TRACE: print('process_2: NVT2 match')

            text1 = sentence[0:matcher_nvt1.end()]
            subjects = tokenize_and_find_subjects(text1)

            m_index = m_index_from_context(m_sentence, text1)
            assert m_index < len(measurements) - 1

            if len(subjects) > 0 and len(subjects[0]) > 0:
                # found a subject for this measurement and the next
                for s in subjects[0]:
                    measurements[m_index + 0].subject.append(s)
                    measurements[m_index + 1].subject.append(s)

                # this is also the subject for the second measurement

                # set temporality of 2nd measurement to PREVIOUS
                measurements[m_index + 1].temporality = STR_PREVIOUS
                return True
            
    # check for two independent 'measures M' clauses
    matcher_2 = regex_measures_m_2.search(sentence)
    if matcher_2:

        if TRACE: print('process_2: MEASURES_M_2 match')
        
        count = 0
        prev_subject = []
        iterator = regex_measures_m_1.finditer(sentence)
        for match in iterator:

            if TRACE:
                print('process_2: text for iteration {0}: {1}'.
                      format(count, match.group()))
            subjects = tokenize_and_find_subjects(match.group())
            if TRACE: print('subjects for iteration: {0}: {1}'.
                            format(count, subjects))

            m_index = m_index_from_context(m_sentence, match.group())
            assert m_index < len(measurements)
            
            assert len(subjects) > 0
            if subjects[0] and len(subjects[0]) > 0:
                # found a subject for this measurement
                for s in subjects[0]:
                    measurements[m_index].subject.append(s)
                prev_subject = subjects[0]
            elif count > 0:
                # no subjects found on this iteration, so use previous subject
                for s in prev_subject:
                    measurements[m_index].subject.append(s)
                    
            count += 1

        return True

    # check for subjects before and after the measurements
    matcher_ba = regex_before_and_after.search(sentence)
    if matcher_ba:

        if TRACE: print('process_2: BA match')

        # find the measurement subject up to the first M
        m_pos = sentence.find('M')
        text1 = sentence[0:m_pos+1]
        subjects = tokenize_and_find_subjects(text1)
        m_index = m_index_from_context(m_sentence, text1)
        assert m_index < len(measurements)-1
        if subjects[0] and len(subjects[0]) > 0:
            for s in subjects[0]:
                measurements[m_index].subject.append(s)
        else:
            return False    

        # find the subject in the text after the second M
        m_pos = sentence.find('M', m_pos+1)
        text2 = sentence[m_pos:]
        subjects = tokenize_and_find_subjects(text2)
        if subjects[0] and len(subjects[0]) > 0:
            for s in subjects[0]:
                measurements[m_index+1].subject.append(s)
        else:
            return False    

        return True

    # check for 'M and M' forms
    matcher_m_and_m = regex_m_and_m_2.search(sentence)
    if matcher_m_and_m:
        if TRACE: print('process_2: M AND M 2 match')

        text1 = matcher_m_and_m.group('text1')
        subjects = tokenize_and_find_subjects(text1)
        if TRACE: print('SUBJECTS 1: {0}'.format(subjects))
        m_index = m_index_from_context(m_sentence, text1)
        assert m_index < len(measurements) - 1
        if subjects[0] and len(subjects[0]) > 0:
            for s in subjects[0]:
                measurements[m_index].subject.append(s)
        else:
            return False
                
        # this form has the same subject for both measurements
        for s in subjects[0]:
            measurements[m_index + 1].subject.append(s)

        return True
    
    matcher_m_and_m = regex_m_and_m_1.search(sentence)
    if matcher_m_and_m:
        if TRACE: print('process_2: M AND M 1 match')

        text1 = matcher_m_and_m.group('text1')
        subjects = tokenize_and_find_subjects(text1)
        if TRACE: print('\tSUBJECTS 1: {0}'.format(subjects))
        m_index = m_index_from_context(m_sentence, text1)
        assert m_index < len(measurements) - 1
        if subjects[0] and len(subjects[0]) > 0:
            for s in subjects[0]:
                measurements[m_index].subject.append(s)
            prev_subject = subjects[0]
        else:
            return False

        text2 = matcher_m_and_m.group('text2')
        subjects2 = tokenize_and_find_subjects(text2)
        if TRACE: print('\tSUBJECTS 2: {0}'.format(subjects2))

        if subjects2[0] and len(subjects2[0]) > 0:
            # found next subject
            for s in subjects2[0]:
                measurements[m_index + 1].subject.append(s)
        elif prev_subject:
            # no subjects found, so duplicate the previous subject
            if TRACE: print('\tusing previous subject')
            for s in prev_subject:
                measurements[m_index + 1].subject.append(s)
        else:
            # no subjects found
            return False

        return True

    # try 'a M wds' and 'a wds M' forms...
    found_it, sentence = process_a_m_wds(m_sentence, sentence, measurements)
    m_count = get_meas_count(sentence)
    if found_it and 0 == m_count:
        return True

    found_it, sentence = process_a_wds_m(m_sentence, sentence, measurements)
    m_count = get_meas_count(sentence)
    if found_it and 0 == m_count:
        return True

    if 2 == m_count:
        return False
    else:
        return process_1(m_sentence, sentence, measurements)
    
###############################################################################
def m_index_from_context(m_sentence, match_text):
    """
    Given a text string 'match_text', search 'm_sentence' for it and find
    which measurement the match_text is associated with.
    """

    start = m_sentence.find(match_text)
    if -1 == start:
        text = match_text
        while -1 == start:
            # remove one word at a time and repeat the search
            pos = text.find(' ')
            if -1 == pos:
                # big problem: match_text not contained in m_sentence
                msg = 'm_index_from_context: no match for text: '\
                      '{0}\tm_sentence: {1}'.format(match_text, m_sentence)
                raise RuntimeError(msg)
            else:
                text = text[pos+1:]
                start = m_sentence.find(text)

    assert -1 != start
    end = start + len(match_text)

    # the desired m-index is the index of the 'M' between [start, end)
    
    iterator = re.finditer(r'\bM\b', m_sentence)

    index = 0
    for match in iterator:
        if match.start() >= start and match.end() <= end:
            break
        else:
            index += 1
            
    return index

###############################################################################
def process_1(m_sentence, sentence, measurements):
    """
    Find the subject of a sentence (or sentence fragment) containing a 
    single measurement.
    """

    if TRACE: print('called process_1')
    
    match = regex_carina.search(sentence)
    if match:
        if TRACE: print('\tprocess_1: carina match')

        text = match.group()
        m_index = m_index_from_context(m_sentence, text)

        texts = [sentence, text]
        for t in texts:
            subjects = tokenize_and_find_subjects(t)
            if len(subjects) > 0 and len(subjects[0]) > 0:
                # found a subject
                for s in subjects[0]:
                    measurements[m_index].subject.append(s)
                return True

        # no subject found, so use endo tube group text
        if TRACE: print('\tprocess_1: using endodube group text')
        measurements[m_index].subject.append(match.group('endotube'))
        return True
    
    match = regex_measures_m_1.search(sentence)
    if match:        
        text = match.group()

        if TRACE:
            print('process_1 matching text: {0}'.format(text))

        m_index = m_index_from_context(m_sentence, text)

        # subjects is a list of lists
        subjects = tokenize_and_find_subjects(text)
        if len(subjects) > 0 and len(subjects[0]) > 0:
            # found a subject for this measurement
            for s in subjects[0]:
                measurements[m_index].subject.append(s)
            return True

    # try 'a M wds' and 'a wds M' forms...
    found_it, sentence = process_a_m_wds(m_sentence, sentence, measurements)
    if found_it:
        return True

    found_it, sentence = process_a_wds_m(m_sentence, sentence, measurements)
    if found_it:
        return True

    # try to tokenize whatever is left
    if TRACE:
        print('process_1 text: {0}'.format(sentence))

    m_index = m_index_from_context(m_sentence, sentence)
        
    subjects = tokenize_and_find_subjects(sentence)
    if len(subjects) > 0 and len(subjects[0]) > 0:
        for s in subjects[0]:
            measurements[m_index].subject.append(s)
        return True
    
    return False

###############################################################################
def process_a_m_wds(m_sentence, sentence, measurements):
    """
    Try to match regex_a_m_wds to the sentence or fragment and derive a
    measurement subject from it.
    """

    if TRACE: print('called process_a_m_wds')
    
    found_subject = False
    while True:

        match = regex_a_m_wds.search(sentence)
        if not match:
            break

        # either group 'wordsNG' or group 'wordsG' matches, but not both
        text1 = match.group('wordsNG')
        text2 = match.group('wordsG')
        assert (text1 is not None) ^ (text2 is not None)                                   

        if text1 is not None:
            matching_text = text1
        else:
            matching_text = text2

        msi = regex_size_or_image.search(matching_text)
        if msi:
            matching_text = erase(matching_text, msi.start(), msi.end())

        # restore the 'M' and search for subjects
        matching_text = 'M ' + matching_text
        matching_text = collapse_ws(matching_text)

        if TRACE:
            print('\tA M WDS matching_text: ->{0}<-'.format(matching_text))

        text = match.group()
        m_index = m_index_from_context(m_sentence, text)

        # subjects is a list of lists
        texts = [matching_text, text]
        for t in texts:
            subjects = tokenize_and_find_subjects(t)
            if len(subjects) > 0 and len(subjects[0]) > 0:
                # found a subject for this measurement
                for s in subjects[0]:
                    measurements[m_index].subject.append(s)
                found_subject = True
                if TRACE:
                    print('\tSUBJECT: {0}'.format(subjects[0]))

                # erase matching text from sentence
                sentence = erase(sentence, match.start(), match.end())
                sentence = collapse_ws(sentence)
                
                if found_subject:
                    break

            if TRACE:
                if not found_subject:
                    print('\tno subject found with first text, trying again...')

        if not found_subject:
            break
    
    return (found_subject, sentence)
            
###############################################################################
def process_a_wds_m(m_sentence, sentence, measurements):
    """
    Try to match regex_a_wds_m to the sentence or fragment and derive a
    measurement subject from it.
    """

    if TRACE: print('called process_a_wds_m')
    
    found_subject = False
    while True:
        match = regex_a_wds_m.search(sentence)
        if not match:
            break

        if TRACE:
            print('\tRegex match: {0}'.format(match.group()))
        
        # either the groups 'words1' and 'words2' match
        # or the groups 'words3' and 'words4' do
        text1 = match.group('words1')
        text2 = match.group('words2')
        if text1 is None and text2 is None:
            text1 = match.group('words3')
            text2 = match.group('words4')

        assert text1 is not None or text2 is not None

        msi = regex_size_or_image.search(text1)
        if msi:
            text1 = erase(text1, msi.start(), msi.end())
        msi = regex_size_or_image.search(text1)
        if msi:
            text2 = erase(text2, msi.start(), msi.end())

        # restore the M, which occurs between the two matching texts
        matching_text = text1 + ' M ' + text2
        matching_text = collapse_ws(matching_text)
        
        if TRACE:
            print('\tA WDS M matching_text: ->{0}<-'.format(matching_text))

        text = match.group()
        m_index = m_index_from_context(m_sentence, text)

        # subjects is a list of lists
        texts = [matching_text, text]
        for t in texts:
            subjects = tokenize_and_find_subjects(t)
            if len(subjects) > 0 and len(subjects[0]) > 0:
                # found a subject for this measurement
                for s in subjects[0]:
                    measurements[m_index].subject.append(s)
                found_subject = True
                if TRACE:
                    print('\tSUBJECT: {0}'.format(subjects[0]))
                
                # erase matching text from sentence
                sentence = erase(sentence, match.start(), match.end())
                sentence = collapse_ws(sentence)

                if found_subject:
                    break

            if TRACE:
                if not found_subject:
                    print('\tno subject found with first text, trying again...')
                
        if not found_subject:
            break

    return (found_subject, sentence)

###############################################################################
def self_test(TEST_DICT, terms, nosub):
    """
    Verify that the subject finder can correctly find the measurement subject
    for all test sentences.
    """

    # subj_list is a list of lists, the known results
    for sentence, subj_lists in TEST_DICT.items():

        # find the subject of each measurement
        json_string = run(terms, sentence, nosub)

        # parse the JSON result
        json_data = json.loads(json_string)

        # unpack to a SubjectFinderResult namedtuple
        result = SubjectFinderResult(**json_data)
        
        # get the array of measurements
        measurement_list = result.measurementList

        # unpack to a list of Measurement namedtuples
        measurements = [Measurement(**m) for m in measurement_list]

        if 0 == len(measurements):
            print('\n*** SELF TEST FAILURE: ***\n{0}'.format(sentence))
            print('\tNo measurement subjects were found.')
        else:
            num_entries = len(subj_lists)
            assert num_entries == len(measurements)
            
            # verify the subject of each
            for i in range(len(subj_lists)):
                m = measurements[i]
                subjects = subj_lists[i]
                for s in subjects:

                    # search for the truth string among all candidate strings
                    # (m.subject is a list of strings)
                    found_it = False
                    for candidate_str in m.subject:
                        if -1 != candidate_str.find(s):
                            found_it = True
                            break

                    if not found_it:
                        print('\n*** SELF TEST FAILURE: ***\n{0}'.format(sentence))
                        print('\t   Measurement: {0}'.format(m.text))
                        print('\tSubjects found: {0}'.format(m.subject))
                        print('\t         Truth: {0}'.format(subjects))
        

###############################################################################
def get_version():
    return 'subject_finder {0}.{1}'.format(VERSION_MAJOR, VERSION_MINOR)
        
###############################################################################
def show_help():
    print(get_version())
    print("""
    USAGE: python3 ./subject_finder.py -t <terms> -s <sentence> [-hvznxd]

    OPTIONS:

        -t, --terms     <quoted string> List of comma-separated search terms.
        -s, --sentence  <quoted string> Sentence to be processed.

    FLAGS:

        -h, --help                      Print this information and exit.
        -v, --version                   Print version information and exit.
        -z, --test                      Disable -s option and use test sentences.
        -n, --nosub                     Do not perform ngram substitution.
        -x, --selftest                  Run self-tests.
        -d, --displacy                  Show the dependency parse using SpaCy's
                                        'displacy' tool. Not valid if -r or -z
                                        option are used. See the visualization
                                        by opening a web browser at the URL
                                        localhost:5000.
    """)

###############################################################################
if __name__ == '__main__':

    # maps a sentence to a list of lists, one list for each measurement
    TEST_DICT = {

        # {subject} is/measure(s|ed|ing) M
        'The spleen is 7.5 cm.' :
        [['spleen']],
        'The spleen was 7.5 cm.' :
        [['spleen']],
        'The spleen measures 7.5 cm.' :
        [['spleen']],
        'The lymph nodes are all 2 cm.' :
        [['nodes']],
        'The spleen is unremarkable measuring 8.6 cm.' :
        [['spleen']],
        'The spleen measures 10 cm and appears normal.' :
        [['spleen']],
        'Local lymphadenopathy measures up to 10 x 7 mm.' :
        [['lymphadenopathy']],
        'These nonobstructing calculi measure up to 6 mm.' :
        [['calculi']],
        'The spleen measures 8.6 cm and is normal in appearance.' :
        [['spleen']],
        'Lymph nodes measure up to approximately 2 cm in levels II '         +\
        'through IV.' :
        [['nodes']],
        'The cyst in the lower pole of the kidney is 1.3 cm in size.' :
        [['cyst']],
        'There are small surrounding lymph nodes, the largest measuring '    +\
        '10 x 7 millimeters.' :
        [['nodes']],
        'The spleen is top normal in size, measuring 12.3 centimeters in '   +\
        'craniocaudal dimension.' :
        [['spleen']],
        'The duct tapers smoothly to the head of the pancreas, where it '    +\
        'measures approximately 5 mm.' :
        [['duct']],
        'Another indistinct cluster can be seen also in the peripheral '     +\
        'right middle lobe, measuring 12 x 11 millimeters (3: 39).' :
        [['cluster']],
        'Immediately inferior to the right lobe of the thyroid gland there ' +\
        'is a hypoechoic nodule measuring 0.7 cm in greatest dimension.' :
        [['nodule']],
        'There is no evidence of intrahepatic or extrahepatic biliary '      +\
        'dilatation with the common bile duct measuring 5.3 millimeters.' :
        [['duct']],
        'The largest right renal cyst arising from the lower pole measures ' +\
        '5.5 x 5.4 x 5.5 cm, and demonstrates benign features with a thin '  +\
        'wall and anechoic center.' :
        [['cyst']],
        'Right hilar node (5:28) is enlarged, measuring 1.4 cm, unchanged '  +\
        'from prior, and was previously shown to be non-FDG avid.' :
        [['node']],
        'Again seen is a loculated low density at the pancreatic head, now ' +\
        'measuring 6 cm x 4.5 cm (5:24) essentially unchanged compared to '  +\
        'the prior exam.' :
        [['density']],
        'There is a small to moderate pericardial effusion, predominantly '  +\
        'adjacent to the right ventricle and best seen on subcostal views, ' +\
        'measuring up to 1.4 centimeters in greatest dimension.' :
        [['effusion']],
        'There is diffuse severe dilatation of the stomach and small bowel ' +\
        'loops, with the small bowel maximally measuring 6.3 cm in '         +\
        'dimension.' :
        [['bowel']],
        'Scale imaging of the artery in this region shows a diffuse '        +\
        'symmetric wall thickening with the arterial wall measuring up to '  +\
        '7 mm in diameter over several centimeters length.' :
        [['wall']],
        'Soft tissue structures demonstrate mediastinal lymphadenopathy '    +\
        'with numerous lymph nodes throughout the mediastinum, with the '    +\
        'largest node in the right paratracheal region measuring about 1.7 ' +\
        'cm in greatest short axis dimension.' :
        [['node']],        
        'Extensive, pronounced cervical lymphadenopathy throughout levels '  +\
        'II through IV, with lymph nodes measuring up to 2 cm.' :
        [['nodes']],        
        'Multiple lymph nodes are present at different mediastinal '         +\
        'stations with largest measuring 13 mm in the right upper '          +\
        'paratracheal region.' :
        [['nodes']],
        'Ectatic abdominal aorta, with multiple regions of enlargement, '    +\
        'with a focal dilatation measuring 4.3 cm just below the renal '     +\
        'arteries.' :
        [['dilatation']],
        'Multiple simple renal cysts are noted within the bilateral kidneys,'+\
        ' left more so than right, with the largest cyst identified within ' +\
        'the inferior pole of the left kidney measuring 2.4 cm.' :
        [['cyst']],
        'The liver is normal in architecture and echogenicity, and is '      +\
        'seen to contain numerous small cysts ranging in size from a few '   +\
        'millimeters to approximately 1.2 cm in diameter.' :
        [['cysts']],        
        'Nondistended gallbladder with small amount of intraluminal sludge, '+\
        'and marked gallbladder wall edema measuring 7 mm.' :
        [['edema']],
        'Additionally, there is a small, loculated pericardial fluid '       +\
        'collection consistent with cellular debris abutting the '           +\
        'inferolateral wall of the left ventricle, measuring up to 0.9 '     +\
        'centimeters in size (clips 10, 11).' :
        [['collection']],
        'In the left upper lobe, there is a large irregular mass abutting '  +\
        'the anterior mediastinal and the anterior pleural space (3:29), '   +\
        'measuring with maximum cross-sectional area of 8.1 x 6.6 cm '       +\
        '(AP x TRV), found to likely represent non-small carcinoma per '     +\
        'outside hospital ([**Hospital6 **]) biopsy report.' :
        [['mass']],

        # ranging in size
        'Distended gallbladder with multiple stones ranging in size from '   +\
        'a few millimeters to 1 cm in diameter.' :
        [['stones']],
        'The liver is normal in architecture and echogenicity, and is '      +\
        'seen to contain numerous small cysts ranging in size from a few '   +\
        'millimeters to approximately 1.2 cm in diameter.' :
        [['cysts']],
        'Just cephalad to the solid mass is an area of multiloculated '      +\
        'cystic change in the pancreatic head, with multiple cysts ranging ' +\
        'in size from a few millimeters up to 1.2 cm.' :
        [['cysts']],

        # two measurements

        # now vs. then
        'A necrotic periportal lymph node  measures 2.0 cm compared to '     +\
        '1.7 cm previously.' :
        [['node'], ['node']],
        'There is a fusiform infrarenal abdominal aortic aneurysm measuring '+\
        '4.4 x 5.2 cm, which previously measured 4.3 x 5.3 cm, and has '     +\
        'therefore not significantly changed.' :
        [['aneurysm'], ['aneurysm']],
        'A left deltoid mass measures 4.2 x 4.6 cm (5:22, previously '       +\
        'measuring 3.4 x 4.4 cm).' :
        [['mass'], ['mass']],
        'A left adrenal nodule measures 1.2 x 1.4 cm as compared to 1.2 x '  +\
        '1.4 cm previously (2:55).' :
        [['nodule'], ['nodule']],
        'A right hilar node measures 12 mm in short axis (7:24), previously '+\
        '11 mm.' :
        [['node'], ['node']],
        'A segment III lesion currently measures 1.3 x 1.8 cm and '          +\
        'previously measured 1.2 x 1.6 cm (2:45).' :
        [['lesion'], ['lesion']],
        'Lesion one within the hilar region measures 35 mm x 24 mm, '        +\
        'previously measuring 31 mm x 23 mm.' :
        [['lesion'], ['lesion']],
        'The main pulmonary trunk measures 3.4 cm in diameter, previously '  +\
        'measured 3.8 cm.' :
        [['trunk'], ['trunk']],
        'Target lesion 1 which was a supraclavicular lymph node measures '   +\
        '5.5 x 3.3 cm today (5:17, measuring 5 x  2.7 cm on the prior).' :
        [['lesion'], ['lesion']],
        'The dominant lesion in the right parietal lobe near the vertex '    +\
        'now measures 2.5 x 2.1 cm (image 1000:28), compared to the 2.4 x '  +\
        '1.7 cm on [**2869-12-17**], appears unchanged.' :
        [['lesion'], ['lesion']],
        'Within the abdomen lesion five, which likely comprises a left '     +\
        'adrenal lesion now measures 36 mm x 28 mm previously measuring '    +\
        '38 mm x 29 mm.' :
        [['lesion'], ['lesion']],
        'The markedly enlarged spleen now measures 16 cm craniocaudally, '   +\
        'compared to approximately 13 cm in the prior PET-CT one month ago, '+\
        'concerning for lymphoma  recurrence.' :
        [['spleen'], ['spleen']],
        'The second largest lymph node in the left inferior axilla now '     +\
        'measures 1.9 x 2.0 x 2.5 cm (4:27), which is increased from the '   +\
        'prior measurement of 2.2 x 1.9 x 1.8 cm.' :
        [['node'], ['node']],
        'Interval decrease is seen in the fluid collections in the '         +\
        'perirenal space specifically in the right perirenal space the '     +\
        'fluid collection measures 2.2 cm x 2.4 cm now versus 2.6 x 2.5 cm ' +\
        'then.' :
        [['collection'], ['collection']],        
        'For example a left upper lung nodule measures 8.1 mm and '          +\
        'previously had measured 7.4 mm (3:36).' :
        [['nodule'], ['nodule']],
        'Again seen is aneurysmal dilatation of the left iliac artery, '     +\
        'currently measuring 3.3 x 3.3 cm, which previously measured 3.2 x ' +\
        '3.2 cm.' :
        [['dilatation'], ['dilatation']],
        'Again seen is an expansile soft tissue lytic lesion in the '        +\
        'anterior left second rib which now measures 7.9 x 2.6 cm, '         +\
        'previously measuring 6.7 x 2.7 cm (3:21).' :
        [['lesion'], ['lesion']],
        'A cavitary nodule is again seen within the right upper lobe '       +\
        'currently measuring 1.5 x 1 cm, from the previous measurement of '  +\
        '1.6 x 2.2 cm on [**2680-6-1**] study (4:21).' :
        [['nodule'], ['nodule']],

        # before and after
        'The left kidney measures 8.5 cm and contains an 8 mm x 8 mm '       +\
        'anechoic rounded focus along the lateral edge, which is most '      +\
        'likely a simple renal cyst.' :
        [['kidney'], ['focus']],
        'The left kidney measures 11.5 cm and contains a 2.8 x 2.2 cm '      +\
        'cyst in the upper pole which may contain a small septation, '       +\
        'however, no increased vascularity.' :
        [['kidney'], ['cyst']],
        'The right kidney measures 9.2 cm, and contains a 5 mm, hyperechoic '+\
        'calculus with posterior shadowing within the interpolar region.' :
        [['kidney'], ['calculus']],
        'The right kidney measures 12.1 cm in length, and in its lower '     +\
        'pole, there is a 3.0 x 2.1 x 2.7 cm simple cyst.' :
        [['kidney'], ['cyst']],
        'The right kidney measures 11.9 cm and demonstrates a 7 mm '         +\
        'hyperechoic focus without acoustic shadowing that may represent a ' +\
        'small nonobstructing renal calculus.' :
        [['kidney'], ['focus']],
        'The left kidney measures 12.6 cm, with that measurement including ' +\
        'a 4.5 cm exophytic cyst at the upper pole of the kidney, not '      +\
        'significantly changed compared to prior CT.' :
        [['kidney'], ['cyst']],

        # two 'measures M'
        'The right kidney measures 11.6 centimeters and the left kidney '    +\
        'measures 12.8 centimeters.' :
        [['kidney'], ['kidney']],
        'The right kidney measures 10 cm and left kidney measures 9.4 '      +\
        'cm, with no hydronephrosis, masses, or stones.' :
        [['kidney'], ['kidney']],
        'The right kidney measures 11.7 cm and the left kidney measures '    +\
        'approximately 12 cm.' :
        [['kidney'], ['kidney']],
        'The cyst on the right measures approximately 3.0 millimeters and '  +\
        'the cyst on the left measures approximately 3.6 millimeters.' :
        [['cyst'], ['cyst']],        
        'Two additional smaller  lesions are newly identified; one within '  +\
        'segment VIII measures 0.5 x 1.1 cm (2:45) and the second in '       +\
        'segment VI measures 11 x 4 mm  (601b:31).' :
        [['lesions'], ['lesions']],
        'The right kidney measures 11.5 cm and contains multiple '           +\
        'thin-walled anechoic rounded structures consistent with simple '    +\
        'renal cysts, the largest of which is within the mid pole and '      +\
        'measures 4.5 x 4.8 x 3.9 cm.' :
        [['kidney'], ['structures']],

        # M and M forms
        'The lower trachea measures 14 x 8 mm on expiratory imaging, and '   +\
        '16 x 17 mm on inspiratory imaging.' :
        [['trachea'], ['trachea']],
        'The largest porta hepatis lymph node measures 1.6 cm in short axis '+\
        'and 2.6 cm in long axis.' :
        [['node'], ['node']],
        'A large bulla at the left lung base measuring 4.6 x 4.2 cm and '    +\
        '1 cm bleb in the right lung base are unchanged.' :
        [['bulla'], ['bleb']],
        'Right kidney measures 12.1 cm, and the left kidney 13.1 cm in its ' +\
        'long axis.' :
        [['kidney'], ['kidney']],
        'The spleen is enlarged measuring 12.5 cm and has a simple 1.2 x '   +\
        '2.9 x 2.9 cm cyst.' :
        [['spleen'], ['cyst']],
        # 'The other bronchi demonstrate no evidence of malacia except to '    +\
        # 'note moderate malacia of the bronchus intermedius, which measures ' +\
        # '10 cm on inspiratory imaging and 4 mm in diameter on expiratory '   +\
        # 'imaging.' :
        # [['bronchus intermedius'], ['bronchus intermedius']],
        'At the level of the left main stem bronchus, the airway measures '  +\
        '171 square millimeters at end inspiration and reduces to 67 square '+\
        'millimeters (61% reduction in cross sectional area).' :
        [['airway'], ['airway']],
        'The right kidney measures 9.7 cm and again seen is an unchanged '   +\
        'approximately 1 cm hyperechoic focus in the upper pole consistent ' +\
        'with an angiomyolipoma.' :
        [['kidney'], ['focus']],
        'Two large enhancing masses are again noted arising from the lower ' +\
        'pole of the left kidney, which measure 5.0 cm x 4.1 cm (4:87) '     +\
        'and 7.3 cm x 5.7 cm (4:84).' :
        [['masses'], ['masses']],

        # three measurements
        'There is a small lesion measuring 1.3 cm, an enlarged lymph node '  +\
        'measuring 1.5 cm, and an echogenic focus in the lower pole '        +\
        'measuring 2.1 cm.' :
        [['lesion'], ['node'], ['focus']],
        'Additional lesions include a 6 mm ring-enhancing mass within the '  +\
        'left lentiform nucleus, a 10 mm peripherally based mass within the '+\
        'anterior left frontal lobe as well as a more confluent plaque-like '+\
        'mass with a broad base along the tentorial surface measuring '      +\
        'approximately 2 cm in greatest dimension.' :
        [['mass'], ['mass'], ['mass']],
        'Most representative are a 1.9 cm nodal mass in the right low '      +\
        'paratracheal station (2:16), a 1.8 cm node in the right low '       +\
        'paratracheal station (2:18), and a 1.6 cm infracarinal lymph node ' +\
        '(2:26).' :
        [['mass'], ['node'], ['node']],

        # carina
        'ET tube tip is 2.2 cm above the carina.' :
        [['tip']],
        'Endotracheal tube is in place, roughly 4 cm above the carina.' :
        [['tube']],
        'Endotracheal tube is in standard position about 5 cm above the '    +\
        'carina.' :
        [['tube']],
        'ET tube is in the standard position, the tip is 6.1 cm above the '  +\
        'carina.' :
        [['tip']],
        'ET tube is low terminating in the lower thoracic trachea '          +\
        'approximately 1 mm above the carina.' :
        [['tube']],
        'A single supine frontal view of the chest shows an endotracheal '   +\
        'tube terminating 3 cm from the carina.' :
        [['tube']],
        'Tip of endotracheal tube is above the level of the clavicles, '     +\
        'terminating about 7 cm above the carina.' :
        [['tip']],
        'ET tube tip is high, 8.1 cm above the carina at the level of the '  +\
        'clavicles and should be advanced couple of centimeters for '        +\
        'standard positioning.' :
        [['tip']],

        # distance
        'The distance from the top of the graft to the aortic bifurcation '  +\
        'measures 117 mm.' :
        [['distance']],

        # need more distance examples - TBD
    }

    optparser = optparse.OptionParser(add_help_option=False)
    optparser.add_option('-t', '--terms',    action='store',      dest='terms')
    optparser.add_option('-s', '--sentence', action='store',      dest='sentence')
    optparser.add_option('-v', '--version',  action='store_true', dest='get_version')
    optparser.add_option('-h', '--help',     action='store_true', dest='show_help', default=False)
    optparser.add_option('-n', '--nosub',    action='store_true', dest='nosub',     default=False)
    optparser.add_option('-x', '--selftest', action='store_true', dest='selftest', default=False)
    optparser.add_option('-z', '--test',     action='store_true', dest='use_test_sentences', default=False)
    optparser.add_option('-d', '--displacy', action='store_true', dest='use_displacy', default=False)
    
    if 1 == len(sys.argv):
        show_help()
        sys.exit(0)

    opts, other = optparser.parse_args(sys.argv)

    if opts.show_help:
        show_help()
        sys.exit(0)

    if opts.get_version:
        print(get_version())
        sys.exit(0)

    terms     = opts.terms
    nosub     = opts.nosub
    sentence  = opts.sentence
    selftest = opts.selftest
    use_displacy = opts.use_displacy
    use_test_sentences = opts.use_test_sentences

    if not sentence and not (selftest or use_test_sentences):
        print('A sentence must be specified on the command line.')
        sys.exit(-1)

    if not terms and not selftest:
        print('One or more search terms must be provided on the command line.')
        sys.exit(-1)

    # displacy option valid only for single-sentence use
    if selftest or use_test_sentences:
        use_displacy = False

    sentences = []
    if selftest or use_test_sentences:
        sentences = [s for s in TEST_DICT]
    else:
        sentences.append(sentence)


    # call prior to processing any sentences
    init()
    
        
    if selftest:
        self_test(TEST_DICT, terms, nosub)

    else:
        for sentence in sentences:
            if use_test_sentences:
                print(sentence)
            json_result = run(terms, sentence, nosub, use_displacy)
            print(json_result)
