{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cooking with ClarityNLP - Session #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this series is to introduce you to writing basic queries using NLPQL.  Today we will also be covering an introduction to data ingestion, document selection, lexical variants, and more custom algorithms.  For other details on installing and using ClarityNLP, please see our [documentation](https://claritynlp.readthedocs.io/en/latest/index.html).  We welcome questions via Slack or on [GitHub](https://github.com/ClarityNLP/ClarityNLP/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run NLP jobs using ClarityNLP, data must first be ingested into the system.  You can ingest data from various sources (eg. flat files, relational databases, APIs, etc) and of various types (eg. txt, doc, pdf, etc). Today we will cover one of the most common ingestion patterns-- bringing in data from a CSV.  ClarityNLP has a user interface to support CSV ingestion.  In a typical instance, this will be located at `localhost:6543/csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Postman.png](assets/Ingest_UI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of ingesting data from CSV involves the following steps:\n",
    "1. Select your CSV file to load column headers\n",
    "2. Assign the required fields for ClarityNLP to columns in your file\n",
    "3. Add any additional fields you would like to include from your source file\n",
    "4. Start the Import process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of the ingestion screen filled out for the MIMIC-III notes file (NOTEEVENTS.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Postman.png](assets/MIMIC_Ingest_UI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Run NLPQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run NLPQL, you must submit it to a ClarityNLP server either via API or via the ClarityNLP user interface.  If you are running a local instance, the API endpoint is typically `localhost:5000/nlpql`.  NLPQL should be POSTed as text/plain.  An example from [Postman](www.postman.com) is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Postman.png](assets/Postman.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are unfamiliar with using tools such as Postman, you can submit NLPQL via the ClarityNLP user interface running in a web browser. For local instances, this will be at [localhost:8200/runner](localhost:8200/runner). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLPQL_Runner.png](assets/NLPQL_Runner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to run NLPQL directly from this notebook, then please use the following code.  You will need to edit the `url` variable to \"localhost:5000/\" or your ClarityNLP server IP address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code below is only required for running ClarityNLP in Jupyter notebooks. It is not required if running NLPQL via API or the ClarityNLP GUI.\n",
    "\n",
    "import pandas as pd\n",
    "import claritynlp_notebook_helpers as claritynlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note: Throughout these tutorials, we will prepend all examples with `limit 100;`.  This limits the server to analyzing a maxium of 100 documents, reducing runtime and compute load when testing new queries. Once a query is producing the expected output, removing this line will allow the full dataset to be run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case #1:  Prostate Cancer\n",
    "For this first use case, we are going to look at a few different approaches to analyzing prostate cancer.  First we will start with just the basic approach we covered last time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Find mentions of \"Prostate Cancer\" in the patient chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a basic example, simply looking for a positive assertion of \"prostate cancer\" or \"prostate ca\" in the record.\n",
    "\n",
    "```java\n",
    "limit 100;\n",
    "\n",
    "//phenotype name\n",
    "phenotype \"Prostate Cancer\" version \"1\";\n",
    "\n",
    "//include Clarity main NLP libraries\n",
    "include ClarityCore version \"1.0\" called Clarity;\n",
    "\n",
    "termset ProstateTerms:\n",
    "  [\"prostate cancer\",\"prostate ca\"];\n",
    "\n",
    "define ProstateCA:\n",
    "  Clarity.ProviderAssertion({\n",
    "    termset:[ProstateCA]\n",
    "    });\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this NLPQL, copy/paste the above and submit via API or the ClarityNLP interface.  Or if you would like to run the NLPQL directoly within this notebook, run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample NLPQL\n",
    "nlpql ='''\n",
    "limit 100;\n",
    "\n",
    "//phenotype name\n",
    "phenotype \"Prostate Cancer\" version \"1\";\n",
    "\n",
    "//include Clarity main NLP libraries\n",
    "include ClarityCore version \"1.0\" called Clarity;\n",
    "\n",
    "termset ProstateTerms:\n",
    "  [\"prostate cancer\",\"prostate ca\"];\n",
    "\n",
    "define ProstateCA:\n",
    "  Clarity.ProviderAssertion({\n",
    "    termset:[ProstateTerms]\n",
    "    });\n",
    "'''\n",
    "run_result, main_csv, intermediate_csv, luigi = claritynlp.run_nlpql(nlpql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_csv_df = pd.read_csv(intermediate_csv)\n",
    "inter_csv_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Document Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we don't want to look for mentions of a concept in just any document, but rather only want to look within certain types of documents.  With ClarityNLP, we can extensively control for the types on documents in which we perform any  algorithm.  A group of documents is referred to in NLPQL as a `documentset`. There are four modifiers that can be used in creating document sets:\n",
    "\n",
    "- report_type\n",
    "- report_tag\n",
    "- filter_query\n",
    "- query\n",
    "\n",
    "***Report Type***\n",
    "\n",
    "When documents are ingested into ClarityNLP, you have the option to assign a report type.  For clinical documents, this is typically something like Discharge Summary, Head CT WWO Contrast, Colonoscopy Report, etc.  Clarity has a convenient function `createReportTypeList` for building a document set based on report type.  Here is an example:\n",
    "\n",
    "```\n",
    "documentset ChestXRayDocuments:\n",
    "   Clarity.createReportTypeList([\"CXR PA/LAT\",\"CXR 2V\",\"AP/LAT CHEST\"]);\n",
    "```\n",
    "\n",
    "***Report Tag***\n",
    "\n",
    "In our research looking at multiple health systems, we found thousands of different report type names.  Such diversity makes it challenging to create phenotypes that can be applied across diverse settings.  To address this, ClarityNLP embeds a Report Type tagging system that facilitates linking report types to the LOINC / RadLex document ontology.  This enables creation of more standardized NLPQL code.  We will discuss report tagging in more detail in a future *Cooking with ClarityNLP* session.\n",
    "\n",
    "```\n",
    "documentset ChestXRayDocuments:\n",
    "   Clarity.createReportTagList([\"XR\",\"Chest\"]);\n",
    "```\n",
    "![Report_Tagger.png](assets/Report_Type_Mapper4.png)\n",
    "\n",
    "***Filter Query***\n",
    "\n",
    "Filter queries are a powerful function that will create a apply a filtering function to the documents you have already selected.  These can include dynamic fields added at ingestion time or standard ClarityNLP document fields. See [Solr query documentation](https://lucene.apache.org/solr/guide/7_4/query-syntax-and-parsing.html) for details.\n",
    "\n",
    "```\n",
    "documentset CXRDocuments:\n",
    "    Clarity.createDocumentSet({\n",
    "        \"report_types\":[],\n",
    "        \"report_tags\": [],\n",
    "        \"filter_query\": \"subject:23224\"});\n",
    "```\n",
    "\n",
    "***Query***\n",
    "\n",
    "Sometimes a highly customized document selection is required that cannot be managed with ClarityNLP's built in functions.  For these situations, you can create an entirely custom document query using Solr, including wildcards, fuzzy searches, proximity searches, range searchers, boosting, etc.  See [Solr query language](https://lucene.apache.org/solr/guide/7_4/query-syntax-and-parsing.html) for full details.\n",
    "\n",
    "```\n",
    "documentset CXRDocuments:\n",
    "    Clarity.createDocumentSet({\n",
    "        \"report_types\":[],\n",
    "        \"report_tags\": [],\n",
    "        \"query\":\"*astatin\"});\n",
    "```\n",
    "\n",
    "![Statin_Result.png](assets/Statin_Result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Finding Prostate mentions in Discharge Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a basic example, simply looking for a positive assertion of \"prostate cancer\" or \"prostate ca\" in the record.\n",
    "\n",
    "```java\n",
    "limit 100;\n",
    "\n",
    "//phenotype name\n",
    "phenotype \"Prostate Cancer\" version \"1\";\n",
    "\n",
    "//include Clarity main NLP libraries\n",
    "include ClarityCore version \"1.0\" called Clarity;\n",
    "\n",
    "documentset DischargeSummaries:\n",
    "    Clarity.createReportTagList(\"Discharge summary\");\n",
    "\n",
    "termset ProstateTerms:\n",
    "  [\"prostate cancer\",\"prostate ca\"];\n",
    "\n",
    "define ProstateCA:\n",
    "  Clarity.ProviderAssertion({\n",
    "    documentset: [DischargeSummaries],\n",
    "    termset:[ProstateTerms]\n",
    "    });\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample NLPQL\n",
    "nlpql ='''\n",
    "limit 100;\n",
    "\n",
    "//phenotype name\n",
    "phenotype \"Prostate Cancer in Discharge Summary\" version \"1\";\n",
    "\n",
    "//include Clarity main NLP libraries\n",
    "include ClarityCore version \"1.0\" called Clarity;\n",
    "\n",
    "documentset DischargeSummaries:\n",
    "    Clarity.createReportTagList(\"Discharge summary\");\n",
    "\n",
    "termset ProstateTerms:\n",
    "  [\"prostate cancer\",\"prostate ca\"];\n",
    "\n",
    "define ProstateCA:\n",
    "  Clarity.ProviderAssertion({\n",
    "    documentset: [DischargeSummaries],\n",
    "    termset:[ProstateTerms]\n",
    "    });\n",
    "'''\n",
    "run_result, main_csv, intermediate_csv, luigi = claritynlp.run_nlpql(nlpql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_csv_df = pd.read_csv(intermediate_csv)\n",
    "inter_csv_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Term Sets with Lexical Variants\n",
    "Building term sets can be a time consuming process. ClarityNLP has a number of cool built-in features for creating synonyms, plurals, lexical variants and so forth.  Check out the full list of [Termset Expansion](https://clarity-nlp.readthedocs.io/en/latest/user_guide/nlpql/macros.html?highlight=lexical) functions.  Here are a few handy ones:\n",
    "\n",
    "- Clarity.Synonyms\n",
    "- Clarity.Plurals\n",
    "- Clarity.VerbInflections\n",
    "\n",
    "- OHDSI.Synonyms\n",
    "\n",
    "Here is an example of a termset to be expanded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```java\n",
    "  phenotype \"Test Expansion Using English Phrases\";\n",
    "\n",
    "  // # Structured Data Model #\n",
    "  datamodel OMOP version \"5.3\";\n",
    "\n",
    "  // # Referenced libraries #\n",
    "  // The ClarityCore library provides common functions for simplifying NLP pipeline creation\n",
    "  include ClarityCore version \"1.0\" called Clarity;\n",
    "  include OHDSIHelpers version \"1.0\" called OHDSI;\n",
    "\n",
    "  // ## Code Systems ##\n",
    "  codesystem OMOP: \"http://omop.org\"; // OMOP vocabulary https://github.com/OHDSI/Vocabulary-v5.0;\n",
    "\n",
    "  termset SynonymTesting: [\n",
    "\n",
    "  // WordNet synonyms for 'prostate'\n",
    "  // Clarity.Synonyms(\"prostate\"),\n",
    "  Clarity.Synonyms(\"prostate\"),\n",
    "\n",
    "\n",
    "  // WordNet synonyms for 'neoplasm'\n",
    "  // Clarity.Synonyms(\"neoplasm\"),\n",
    "  Clarity.Synonyms(\"neoplasm\"),\n",
    "\n",
    "  // Pluralize Synonyms\n",
    "  // Clarity.Plurals(Clarity.Synonyms(\"neoplasm\")),\n",
    "  Clarity.Plurals(Clarity.Synonyms(\"neoplasm\")),\n",
    "\n",
    "  // OHDSI synonyms for 'neoplasm'\n",
    "  // OHDSI.Synonyms(\"neoplasm\"),\n",
    "  OHDSI.Synonyms(\"neoplasm\"),\n",
    "\n",
    "  // OHDSI synonyms for 'myocardial infarction'\n",
    "  // OHDSI.Synonyms(\"myocardial infarction\"),\n",
    "  OHDSI.Synonyms(\"myocardial infarction\"),\n",
    "\n",
    "  //Wordnet synonyms for myocardial infarction\n",
    " //  Clarity.Synonyms(\"myocardial infarction\"),\n",
    "  Clarity.Synonyms(\"myocardial infarction\")\n",
    "  ];\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now post-expansion:\n",
    "\n",
    "```java\n",
    "termset SynonymTesting: [\n",
    "\n",
    "  // WordNet synonyms for 'prostate'\n",
    "  // Clarity.Synonyms(\"prostate\"),\n",
    "  \"prostate\",\"prostate gland\",\"prostatic\",\n",
    "\n",
    "\n",
    "  // WordNet synonyms for 'neoplasm'\n",
    "  // Clarity.Synonyms(\"neoplasm\"),\n",
    "  \"neoplasm\",\"tumor\",\"tumour\",\n",
    "\n",
    "  // Pluralize Synonyms\n",
    "  // Clarity.Plurals(Clarity.Synonyms(\"neoplasm\")),\n",
    "  \"neoplasm\",\"neoplasms\",\"tumor\",\"tumors\",\"tumour\",\"tumours\",\n",
    "\n",
    "  // OHDSI synonyms for 'neoplasm'\n",
    "  // OHDSI.Synonyms(\"neoplasm\"),\n",
    "  \"neoplasm\",\"neoplasm (morphologic abnormality)\",\"tumor\",\"tumour\",\n",
    "\n",
    "  // OHDSI synonyms for 'myocardial infarction'\n",
    "  // OHDSI.Synonyms(\"myocardial infarction\"),\n",
    "  \"cardiac infarction\",\"heart attack\",\"infarction of heart\",\"mi - myocardial infarction\",\"myocardial infarct\",\"myocardial infarction\",\"myocardial infarction (disorder)\",\n",
    "\n",
    "  //Wordnet synonyms for myocardial infarction\n",
    " //  Clarity.Synonyms(\"myocardial infarction\")\n",
    "  \"myocardial infarct\",\"myocardial infarction\"\n",
    "  ];\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Prostate Biopsies\n",
    "Let's do a slight addition where we look for mention of prostate and biopsy.  We can use the new synonym expansion capability we learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample NLPQL\n",
    "nlpql ='''\n",
    "limit 100;\n",
    "\n",
    "//phenotype name\n",
    "phenotype \"Prostate Cancer and Biopsy\" version \"1\";\n",
    "\n",
    "//include Clarity main NLP libraries\n",
    "include ClarityCore version \"1.0\" called Clarity;\n",
    "\n",
    "documentset PathologyDocuments:\n",
    "    Clarity.createDocumentSet({\n",
    "        \"report_types\":[\"Pathology\"]\n",
    "    });\n",
    "\n",
    "termset ProstateTerms:\n",
    "  [\"prostate\"];\n",
    "\n",
    "termset BiopsyTerms:\n",
    "  [\n",
    "  Clarity.VerbInflections(\"biopsy\")\n",
    "  ];\n",
    "  \n",
    "define ProstateCA:\n",
    "  Clarity.ProviderAssertion({\n",
    "    documentset:[PathologyDocuments],\n",
    "    termset:[ProstateTerms]\n",
    "    });\n",
    "\n",
    "define Biopsy:\n",
    "  Clarity.ProviderAssertion({\n",
    "    documentset:[PathologyDocuments],\n",
    "    termset:[BiopsyTerms]\n",
    "    });\n",
    "\n",
    "context Document;\n",
    "\n",
    "define final ProstateCAandBiopsy:\n",
    "    where ProstateCA AND Biopsy;\n",
    "'''\n",
    "expanded_nlpql = claritynlp.run_term_expansion(nlpql)\n",
    "print(expanded_nlpql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NLPQL after term expansion\n",
    "run_result, main_csv, intermediate_csv, luigi = claritynlp.run_nlpql(expanded_nlpql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_csv_df = pd.read_csv(main_csv)\n",
    "final_csv_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This search is not terribly useful because it is really just finding documents where both words show up.  To perform a more precise search for term pairs by distance, you can use the convenient `TermProximity` task.  This lets you set the TermSets, word distance, and ordering constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlpql ='''\n",
    "limit 100;\n",
    "\n",
    "phenotype \"Prostate and Biopsy Proximity\" version \"1\";\n",
    "include ClarityCore version \"1.0\" called Clarity;\n",
    "\n",
    "documentset Docs:\n",
    "    Clarity.createDocumentSet({\n",
    "        \"report_types\":[\"Pathology\"]\n",
    "    });\n",
    "\n",
    "define final TermProximityFunction:\n",
    "    Clarity.TermProximityTask({\n",
    "        documentset: [Docs],\n",
    "        \"termset1\": \"prostate\",\n",
    "        \"termset2\": \"biopsy,biopsied,bx\",\n",
    "        \"word_distance\": 5,\n",
    "        \"any_order\": \"True\"\n",
    "    });\n",
    "'''\n",
    "run_result, main_csv, intermediate_csv, luigi = claritynlp.run_nlpql(nlpql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_csv_df = pd.read_csv(main_csv)\n",
    "final_csv_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prostate Volume**\n",
    "\n",
    "Another function which is useful in this scenario is our size extraction function.\n",
    "```java\n",
    "//phenotype name\n",
    "phenotype \"Prostate Volume v3\" version \"1\";\n",
    "\n",
    "//include Clarity main NLP libraries\n",
    "include ClarityCore version \"1.0\" called Clarity;\n",
    "include OHDSIHelpers version \"1.0\" called OHDSI;\n",
    "\n",
    "documentset Docs:\n",
    "    Clarity.createDocumentSet({\n",
    "        \"report_types\":[\"Pathology\"]\n",
    "    });\n",
    "    \n",
    "termset Prostate:\n",
    "  [\"prostate\"];\n",
    "\n",
    "define final PSADimensions:\n",
    "  Clarity.MeasurementFinder({\n",
    "    cohort:PSAPatients,\n",
    "    termset:[Prostate]\n",
    "    });\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlpql ='''\n",
    "limit 100;\n",
    "\n",
    "//phenotype name\n",
    "phenotype \"Prostate Volume v3\" version \"1\";\n",
    "\n",
    "//include Clarity main NLP libraries\n",
    "include ClarityCore version \"1.0\" called Clarity;\n",
    "include OHDSIHelpers version \"1.0\" called OHDSI;\n",
    "\n",
    "documentset Docs:\n",
    "    Clarity.createDocumentSet({\n",
    "        \"report_types\":[\"Pathology\"]\n",
    "    });\n",
    "    \n",
    "termset Prostate:\n",
    "  [\"prostate\"];\n",
    "\n",
    "define final PSADimensions:\n",
    "  Clarity.MeasurementFinder({\n",
    "    cohort:PSAPatients,\n",
    "    termset:[Prostate]\n",
    "    });\n",
    "'''\n",
    "run_result, main_csv, intermediate_csv, luigi = claritynlp.run_nlpql(nlpql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_csv_df = pd.read_csv(main_csv)\n",
    "final_csv_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Cooking Special: Finding Gleason Scores\n",
    "Clues to the severity of a prostate tumor can be obtained from a needle biopsy and examination of the tissue under a microscope. Pathologists have developed a numerical scoring system for classifying the microscopic tissue morphology. This score, called the Gleason score, is an important factor in determining the stage of the cancer and the patient's overall prognosis. In this example we will develop a custom task for extracting Gleason scores.\n",
    "\n",
    "#### 1.4.1 Gleason Score Regular Expressions\n",
    "\n",
    "To develop a custom Gleason score extractor we first need to investigate how these scores are actually reported in medical records. By searching for the term 'Gleason' in a corpus of prostate-related health data you will find considerable variation:\n",
    "\n",
    "* Gleason score 7\n",
    "* Gleasons score 7 (3+4)\n",
    "* Gleason's score 3 + 4 = 7/10\n",
    "* Gleason pattern of 3+4\n",
    "* Gleasons 5\n",
    "* Gleason five\n",
    "* Gleason's 3 + 3\n",
    "* Gleason grade is 4+\n",
    "* etc.\n",
    "\n",
    "In the first example, the value 7 is the overall score, which consists of the sum of two other components. Sometimes these components are listed in the form `i+j`, as in the second example, sometimes not. Sometimes only the components are listed and the overall score is omitted, as in the fourth example. Whether listed or not, the value of each component ranges from 1 to 5 inclusive, which means that the total Gleason score has a minimum value of 2 and a maximum value of 10.\n",
    "\n",
    "These forms and others that we observed all fit this generic pattern: \n",
    "\n",
    "1. Gleason text string: `Gleason`, `Gleason's`, `Gleasons`, ...\n",
    "2. (optional) designator text string: `score`, `pattern`, `grade`, possibly followed by \"is\" or \"of\"\n",
    "3. (optional) score, either numeric or text\n",
    "4. (optional) two-part component values, possibly parenthesized\n",
    "\n",
    "A regular expression for recognizing these forms is:\n",
    "\n",
    "```python\n",
    "str_gleason  = r'Gleason(\\'?s)?\\s*'\n",
    "str_desig    = r'(score|sum|grade|pattern)(\\s+(is|of))?'\n",
    "\n",
    "# accept a score in digits under these circumstances:\n",
    "#     digit not followed by a '+', e.g. 'Gleason score 6'\n",
    "#     digit followed by '+' but no digit after, e.g. 'Gleason score 3+'\n",
    "# constructs such as Gleason score 3+3 captured in two-part expression below\n",
    "str_score = r'(?P<score>(\\d+(?!\\+)(?!\\s\\+)|\\d+(?=\\+(?!\\d))|'             +\\\n",
    "            r'two|three|four|five|six|seven|eight|nine|ten))'\n",
    "\n",
    "# parens are optional, space surrounding the '+' varies\n",
    "str_two_part = r'((\\(\\s*)?(?P<first_num>\\d+)\\s*\\+\\s*(?P<second_num>\\d+)' +\\\n",
    "               r'(\\s*\\))?)?'\n",
    "\n",
    "# combine all strings\n",
    "str_total = str_gleason + r'(' + str_desig + r'\\s*)?'                    +\\\n",
    "             r'(' + str_score + r'\\s*)?' + str_two_part\n",
    "             \n",
    "# final Gleason regex\n",
    "regex_gleason = re.compile(str_total, re.IGNORECASE)\n",
    "```\n",
    "\n",
    "Named capture groups are used in the regex to extract the score and each component, if present.\n",
    "\n",
    "#### 1.4.2 Extraction of the Gleason Score from Sentences\n",
    "\n",
    "This regex, along with some additional logic, can be used to recognize and extract Gleason scores from sentences. An outline of the process is:\n",
    "\n",
    "* Attempt a regex match on the current sentence.\n",
    "* If match, extract all named capture groups that exist.\n",
    "* Convert captured values from string to int.\n",
    "* Check captured values and make sure they fall within expected ranges.\n",
    "* If valid, save captured text and values.\n",
    "\n",
    "Our implementation of this logic is in the next code block:\n",
    "\n",
    "```python\n",
    "# convert text scores to integers\n",
    "SCORE_TEXT_TO_INT = {\n",
    "    'two':2,\n",
    "    'three':3,\n",
    "    'four':4,\n",
    "    'five':5,\n",
    "    'six':6,\n",
    "    'seven':7,\n",
    "    'eight':8,\n",
    "    'nine':9,\n",
    "    'ten':10\n",
    "}\n",
    "\n",
    "# namedtuple for result\n",
    "GLEASON_SCORE_RESULT_FIELDS = ['sentence_index', 'start', 'end',\n",
    "                               'score', 'first_num', 'second_num']\n",
    "GleasonScoreResult = namedtuple('GleasonScoreResult', GLEASON_SCORE_RESULT_FIELDS)\n",
    "\n",
    "def find_gleason_score(sentence_list):\n",
    "    \"\"\"\n",
    "    Scan a list of sentences and run Gleason score-finding regexes on each.\n",
    "    Returns a list of GleasonScoreResult namedtuples.\n",
    "    \"\"\"\n",
    "\n",
    "    result_list = []\n",
    "\n",
    "    for i in range(len(sentence_list)):\n",
    "        s = sentence_list[i]\n",
    "        \n",
    "        # attempt regex match\n",
    "        iterator = regex_gleason.finditer(s)\n",
    "        for match in iterator:\n",
    "            start = match.start()\n",
    "            end   = match.end()\n",
    "\n",
    "            # extract first component if it exists\n",
    "            try:\n",
    "                first_num = int(match.group('first_num'))\n",
    "            except:\n",
    "                first_num = None\n",
    "\n",
    "            # extract second component if it exists\n",
    "            try:\n",
    "                second_num = int(match.group('second_num'))\n",
    "            except:\n",
    "                second_num = None\n",
    "\n",
    "            # extract score if it exists\n",
    "            try:\n",
    "                match_text = match.group('score')\n",
    "                if match_text.isdigit():\n",
    "                    score = int(match_text)\n",
    "                else:\n",
    "                    # convert text score to int\n",
    "                    match_text = match_text.strip()\n",
    "                    if match_text in SCORE_TEXT_TO_INT:\n",
    "                        score = SCORE_TEXT_TO_INT[match_text]\n",
    "                    else:\n",
    "                        score = None\n",
    "            except:\n",
    "                # no single score was given\n",
    "                if first_num is not None and second_num is not None:\n",
    "                    score = first_num + second_num\n",
    "                else:\n",
    "                    score = None\n",
    "\n",
    "            # Now apply these rules to determine if score is valid:\n",
    "            #\n",
    "            #     1 <= first_num <= 5\n",
    "            #     1 <= second_num <= 5\n",
    "            #     2 <= score <= 10\n",
    "            #\n",
    "            # anything outside of these limits is invalid\n",
    "\n",
    "            if first_num is not None and (first_num > 5 and first_num <= 10):\n",
    "                # assume score reported for first_num\n",
    "                score = first_num\n",
    "                first_num = None\n",
    "                second_num = None\n",
    "            elif score is not None and (score < 2 or score > 10):\n",
    "                # invalid\n",
    "                score = None\n",
    "                continue\n",
    "                    \n",
    "            result = GleasonScoreResult(i, start, end, score, first_num, second_num)\n",
    "            result_list.append(result)\n",
    "\n",
    "return result_list\n",
    "```\n",
    "\n",
    "#### 1.4.3 Gleason Score Custom Task for ClarityNLP\n",
    "\n",
    "The code presented above can be combined into a custom task for extracting Gleason scores. Using the custom task framework presented in [Cooking with ClarityNLP Session 1](https://github.com/ClarityNLP/ClarityNLP/blob/master/nlp/notebooks/cooking/Cooking%20with%20ClarityNLP%20-%20082818.ipynb), we have the following code outline:\n",
    "```python\n",
    "\n",
    "def find_gleason_score(sentence_list):\n",
    "    # see code above\n",
    "\n",
    "class GleasonScoreTask(BaseTask):\n",
    "    \"\"\"\n",
    "    A custom task for finding the Gleason score, which is relevant to \n",
    "    prostate cancer diagnosis and staging.\n",
    "    \"\"\"\n",
    "    \n",
    "    # use this name in NLPQL\n",
    "    task_name = \"GleasonScoreTask\"\n",
    "\n",
    "    def run_custom_task(self, temp_file, mongo_client: MongoClient):\n",
    "\n",
    "        # for each document in the NLPQL-specified doc set\n",
    "        for doc in self.docs:\n",
    "\n",
    "            # all sentences in this document\n",
    "            sentence_list = self.get_document_sentences(doc)\n",
    "\n",
    "            # all Gleason score results in this document\n",
    "            result_list = find_gleason_score(sentence_list)\n",
    "                \n",
    "            if len(result_list) > 0:\n",
    "                for result in result_list:\n",
    "                    obj = {\n",
    "                        'sentence':sentence_list[result.sentence_index],\n",
    "                        'start':result.start,\n",
    "                        'end':result.end,\n",
    "                        'value':result.score,\n",
    "                        'value_first':result.first_num,\n",
    "                        'value_second':result.second_num\n",
    "                    }\n",
    "            \n",
    "                self.write_result_data(temp_file, mongo_client, doc, obj)\n",
    "```\n",
    "\n",
    "Each ClarityNLP custom task must be implemented as a derived class of ClarityNLP's `BaseTask` class.  Our custom Gleason score task is called `GleasonScoreTask`, and it is a child of `BaseTask`, as required.\n",
    "\n",
    "The `task_name` field is the name by which this custom task will be invoked from NLPQL. This name is `GleasonScoreTask`.\n",
    "\n",
    "Each custom task must implement the `run_custom_task` function. We do so by iterating over all documents, extracting the document's sentences, and calling our `find_gleason_score` function on each sentence to recognize and extract the Gleason score and its components.\n",
    "\n",
    "If any Gleason scores are found in the document's sentences they are returned as a list of `GleasonScoreResult` namedtuples. We iterate over the list of these tuples and build a python dict that contains the output desired in the phenotype results. The result fields that we write out are:\n",
    "\n",
    "* `sentence`: the sentence containing the Gleason score\n",
    "* `start`: the first character of the text matched by the regex\n",
    "* `end`: one past the last character matched by the regex\n",
    "* `value`: the Gleason score value\n",
    "* `value_first`: the first component of the Gleason score, if any\n",
    "* `value_second`: the second component of the Gleason score, if any\n",
    "\n",
    "In the next cell we present a sample NLPQL program to invoke the custom task and extract Gleason scores. This code uses the `createDocumentSet` function to limit the input documents to those with a `report_type` field equal to `Pathology`. Gleason scores are determined by examining tissue under a microscope, so pathology reports are the expected source of these scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample NLPQL to find Gleason scores from pathology reports\n",
    "nlpql ='''\n",
    "limit 100;\n",
    "\n",
    "phenotype \"Gleason Score Finder\" version \"1\";\n",
    "include ClarityCore version \"1.0\" called Clarity;\n",
    "\n",
    "documentset Docs:\n",
    "    Clarity.createDocumentSet({\n",
    "        \"report_types\":[\"Pathology\"]\n",
    "    });\n",
    "\n",
    "define final GleasonFinderFunction:\n",
    "    Clarity.GleasonScoreTask({\n",
    "        documentset: [Docs]\n",
    "    });\n",
    "'''\n",
    "run_result, main_csv, intermediate_csv, luigi = claritynlp.run_nlpql(nlpql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_csv_df = pd.read_csv(main_csv)\n",
    "final_csv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display selected result columns from a run at GA Tech\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "gleason_df = pd.read_csv('assets/phenotype_gleason.csv')\n",
    "\n",
    "# limit display cols to extracted score and components\n",
    "df = gleason_df[['sentence', 'value', 'value_first', 'value_second']]\n",
    "\n",
    "# make the display wider to see complete sentence text\n",
    "pd.set_option('max_colwidth', 800)\n",
    "\n",
    "# print results\n",
    "df\n",
    "\n",
    "# The results of this run appear in the next cell. The NaN entries mean\n",
    "# \"not a number\", indicating that either the score, the components, \n",
    "# or both, were not found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview: Working with Coded Data in ClarityNLP\n",
    "One of the  biggest challenges in phenotyping is the integration of structured and unstructured data.  We will dedicate a future Cooking session to this topic, but let's wrap up today with a preview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Finding Patients with Prostate Cancer Diagnosis and Abnormal Gleason Scores\n",
    "Using the OHDSI Atlas stack, we have defined an OHDSI Cohort looking for patients with PSA > 4 using the following SNOMED codes:\n",
    "\n",
    "- 200962\t93974005\tPrimary malignant neoplasm of prostate\tCondition\tStandard\t\t\t\n",
    "- 4129902\t126906006\tNeoplasm of prostate\tCondition\tStandard\t\t\t\n",
    "- 4141960\t427492003\tHormone refractory prostate cancer\tCondition\tStandard\n",
    "\n",
    "![Atlas_PSA.png](assets/Atlas_PCa.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to look at these patient specifically in ClarityNLP, we can reference the cohort while combining any other features.  For example:\n",
    "\n",
    "```java\n",
    "limit 100;\n",
    "\n",
    "phenotype \"Gleason Score and PSA\" version \"1\";\n",
    "include ClarityCore version \"1.0\" called Clarity;\n",
    "\n",
    "cohort ProstateCaPatients:OHDSI.getCohort(336);\n",
    "\n",
    "documentset Docs:\n",
    "    Clarity.createDocumentSet({\n",
    "        \"report_types\":[\"Pathology\"]\n",
    "    });\n",
    "\n",
    "define GleasonScore:\n",
    "    Clarity.GleasonScoreTask({\n",
    "        cohort:ProstateCaPatients,\n",
    "        documentset: [Docs]\n",
    "    });\n",
    "\n",
    "define final ElevatedGleason:\n",
    "    where GleasonScore.value > 5;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample NLPQL to find Gleason scores from pathology reports\n",
    "nlpql ='''\n",
    "phenotype \"Gleason Score and Prosate Ca\" version \"1\";\n",
    "include ClarityCore version \"1.0\" called Clarity;\n",
    "\n",
    "cohort ProstateCaPatients:OHDSI.getCohort(336);\n",
    "\n",
    "documentset Docs:\n",
    "    Clarity.createDocumentSet({\n",
    "        \"report_types\":[\"Pathology\"]\n",
    "    });\n",
    "\n",
    "define GleasonScore:\n",
    "    Clarity.GleasonScoreTask({\n",
    "        cohort:ProstateCaPatients,\n",
    "        documentset: [Docs]\n",
    "    });\n",
    "\n",
    "define final ElevatedGleason:\n",
    "    where GleasonScore.value > 5;\n",
    "\n",
    "'''\n",
    "run_result, main_csv, intermediate_csv, luigi = claritynlp.run_nlpql(nlpql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_csv_df = pd.read_csv(main_csv)\n",
    "final_csv_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
