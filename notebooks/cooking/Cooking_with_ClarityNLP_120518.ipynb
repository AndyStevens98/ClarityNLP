{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cooking with ClarityNLP - Session #7 - NLPQL Under the Hood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will take a behind-the-scenes look at how ClarityNLP evaluates NLPQL expressions. We will walk through the construction of an NLPQL file and give a high-level description of how its results are generated. We will also provide an overview of our new NLPQL editor tool that makes the task of creating NLPQL files much easier. For background on installing and using ClarityNLP, please see our [documentation](https://claritynlp.readthedocs.io/en/latest/index.html).  We welcome questions via Slack or on [GitHub](https://github.com/ClarityNLP/ClarityNLP/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Measurements from Radiology Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start things off, suppose that we're developing a promising new immunotherapy drug. This drug has proven to be effective on tumors of various sizes, but we have noted particular efficacy for tumors in the 1 cm to 2 cm size range. We want to recruit patients for a new clinical trial designed to test the drug on tumors of this size.  We have access to a corpus of radiology reports, and we would like to search these reports for patients with appropriately-sized tumors. How can we use ClarityNLP to find more patients?\n",
    "\n",
    "As you've learned in previous cooking sessions, we need to create an NLPQL file that defines a documentset for the radiology reports and a termset with tumor-related terms. We will need to run the measurement finder to extract measurements, and then filter the measurements with mathematical expressions that constrain the allowable tumor sizes.\n",
    "\n",
    "When developing a new NLPQL file, it is best to limit the number of documents processed until the NLPQL is fully debugged and working. So let's start by limiting our initial document set to 50 documents. It shouldn't take too long to processes 50 documents, and if we make a mistake, we can quickly recover. \n",
    "\n",
    "A limit on the number of documents is specified by a ``limit`` statement on the first line of the NLPQL file. So open a text editor, create a new file called ``lesion.nlpql``, and enter the following line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>limit 50;</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to insert some boilerplate that identifies the phenotype and version, provides a description, and imports the ClarityNLP libraries. All of your NLPQL files will have text similar to this at the start:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>phenotype \"Lesions1to2Cm\" version \"1\";\n",
    "description \"Find lesions of sizes ranging from 1 to 2 cm.\";\n",
    "include ClarityCore version \"1.0\" called Clarity;</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only want to search radiology reports, we can create a documentset specifically for this purpose. Note that the ``report_types`` field actually takes an array argument (identified by the square brackets). We will use a single-element array containing the term ``Radiology``, the label used by the MIMIC-III data set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "documentset Docs:\n",
    "    Clarity.createDocumentSet({\n",
    "        \"report_types\":[\"Radiology\"]\n",
    "    });\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create a list of the tumor-related terms we want ClarityNLP to search for. We ponder this for a while and eventually arrive at a termset with these words:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "termset LesionTerms: [\n",
    "    \"lesion\", \"growth\", \"mass\", \"malignancy\", \"tumor\",\n",
    "    \"neoplasm\", \"nodule\", \"cyst\", \"focus of enhancement\",\n",
    "    \"echodensity\", \"hypoechoic focus\", \"echogenic focus\"\n",
    "];\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to find and extract measurements, we must insert a command to activate ClarityNLP's measurement finder. The simplest command to do this is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "define LesionMeasurement:\n",
    "    Clarity.MeasurementFinder({\n",
    "        documentset: [Docs],\n",
    "        termset: [LesionTerms]\n",
    "    });\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command runs the measurement finder on each sentence of our source documents. It returns any measurements that occur in the same sentence as a term in our termset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to find **patients** with tumors of the specified dimensions, so we specify a ``Patient`` context:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "context Patient;\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to write the commands for constraining the lesion measurements to our desired size of 1-2 cm. Here we will insert three commands to do so, and we will explain the differences in results for each below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "define xBetween10and20mm:\n",
    "    where LesionMeasurement.dimension_X >= 10 AND LesionMeasurement.dimension_X <= 20;\n",
    "\n",
    "define xyBetween10and20mm:\n",
    "    where LesionMeasurement.dimension_X >= 10 AND LesionMeasurement.dimension_X <= 20 AND\n",
    "          LesionMeasurement.dimension_Y >= 10 AND LesionMeasurement.dimension_Y <= 20;\n",
    "\n",
    "define xyzBetween10and20mm:\n",
    "    where LesionMeasurement.dimension_X >= 10 AND LesionMeasurement.dimension_X <= 20 AND\n",
    "          LesionMeasurement.dimension_Y >= 10 AND LesionMeasurement.dimension_Y <= 20 AND\n",
    "          LesionMeasurement.dimension_Z >= 10 AND LesionMeasurement.dimension_Z <= 20;\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ClarityNLP normalizes all dimensional measurements to units of **millimeters**, so our desired range of 1-2 cm becomes 10-20 mm. These three statements enforce constratints on the X, XY, and XYZ measurement components respectively.\n",
    "\n",
    "And with that we're done. Here is the final text of our ``lesion.nlpql``:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "limit 50;\n",
    "phenotype \"LesionDemo\" version \"1\";\n",
    "description \"Find lesions of various sizes.\";\n",
    "include ClarityCore version \"1.0\" called Clarity;\n",
    "\n",
    "// radiology documents only in the documentset\n",
    "documentset Docs:\n",
    "    Clarity.createDocumentSet({\n",
    "        \"report_types\":[\"Radiology\"]\n",
    "    });\n",
    "\n",
    "// lesion terms\n",
    "termset LesionTerms: [\n",
    "    \"lesion\", \"growth\", \"mass\", \"malignancy\", \"tumor\",\n",
    "    \"neoplasm\", \"nodule\", \"cyst\", \"focus of enhancement\",\n",
    "    \"echodensity\", \"hyperechogenic focus\"\n",
    "];\n",
    "\n",
    "// extract lesion measurements\n",
    "define LesionMeasurement:\n",
    "    Clarity.MeasurementFinder({\n",
    "        documentset: [Docs],\n",
    "        termset: [LesionTerms]\n",
    "    });\n",
    "\n",
    "// we want to find patients, so use 'Patient' context\n",
    "context Patient;\n",
    "\n",
    "// constraints on X, XY, and XYZ\n",
    "\n",
    "define xBetween10and20mm:\n",
    "    where LesionMeasurement.dimension_X >= 10 AND LesionMeasurement.dimension_X <= 20;\n",
    "\n",
    "define xyBetween10and20mm:\n",
    "    where LesionMeasurement.dimension_X >= 10 AND LesionMeasurement.dimension_X <= 20 AND\n",
    "          LesionMeasurement.dimension_Y >= 10 AND LesionMeasurement.dimension_Y <= 20;\n",
    "\n",
    "define xyzBetween10and20mm:\n",
    "    where LesionMeasurement.dimension_X >= 10 AND LesionMeasurement.dimension_X <= 20 AND\n",
    "          LesionMeasurement.dimension_Y >= 10 AND LesionMeasurement.dimension_Y <= 20 AND\n",
    "          LesionMeasurement.dimension_Z >= 10 AND LesionMeasurement.dimension_Z <= 20;\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the NLPQL for Syntax Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before trying to process documents with our new NLPQL file, it is a good idea to first check it for syntax errors. We can do this by submitting it to the ``nlpql_tester`` API endpoint, a useful tool for the NLPQL developer.\n",
    "\n",
    "In prevous cooking sessions we showed you how to use the [Postman](www.postman.com) GUI tool to submit NLPQL files to the ClarityNLP webserver. Today we will show you how to use a command-line tool called [cURL](https://curl.haxx.se/) to do the same thing.\n",
    "\n",
    "The nlpql_tester API for a local ClarityNLP instance is typically found at ``localhost:5000/nlpql_tester``. The NLPQL file should be sent via HTTP POST using a content type of ``text/plain``.\n",
    "\n",
    "To submit the file, install ``curl`` on your system, then open a terminal window, change directories to the location of ``lesion.nlpql``, and run the next command. If you are following along in the notebook, there is a copy of ``lesion.nlpql`` in ``notebooks/cooking/assets/``.\n",
    "\n",
    "<pre>\n",
    "curl -i -X POST http://localhost:5000/nlpql_tester -H \"Content-Type: text/plain\" --data-binary \"@lesion.nlpql\"\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the various options mean:\n",
    "```\n",
    "-i: include the HTTP header in the output\n",
    "-X: request type (must be ``POST``)\n",
    "-H: add the subsequent ``Content-Type: text/plain`` to the header of the HTTP request\n",
    "--data-binary: POST the data exactly as specified, no additional processing\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cURL command submits the file to the ``nlpql_tester`` API endpoint via HTTP POST. If the syntax is OK the system responds with a JSON result. Otherwise the system responds with an error message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the NLPQL tester directly from this notebook by first running the code in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code below is only required for running ClarityNLP in Jupyter notebooks.\n",
    "# It is not required if running NLPQL via API or the ClarityNLP GUI.\n",
    "import pandas as pd\n",
    "import claritynlp_notebook_helpers as claritynlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the next cell to test the NLPQL file:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**BROKEN**\n",
    "lesion_nlpql_text = claritynlp.load_file('assets/lesion.nlpql')\n",
    "json_result = claritynlp.run_nlpql_tester(lesion_nlpql_text)\n",
    "print(json_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the NLPQL File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having verified that the NLPQL file has the proper syntax, we submit the job to the ClarityNLP server with a similar cURL command:\n",
    "<pre>\n",
    "curl -i -X POST http://localhost:5000/nlpql -H \"Content-Type: text/plain\" --data-binary \"@lesion.nlpql\"\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can run from the next notebook cell:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**BROKEN**\n",
    "lesion_nlpql_text = claritynlp.load_file('assets/lesion.nlpql')\n",
    "run_result, main_csv, intermediate_csv, luigi = claritynlp.run_nlpql(lesion_nlpql_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job may take several minutes to run. After it runs to completion, browse to the location of the CSV file containing the intermediate results, and open it in in a spreadsheet application such as Microsoft Excel. We have saved the results of a run to ``notebooks/cooking/assets/lesion_intermediate.csv``, some of which is displayed in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dimension_X</th>\n",
       "      <th>dimension_Y</th>\n",
       "      <th>dimension_Z</th>\n",
       "      <th>nlpql_feature</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>40463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>40463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>40463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110</td>\n",
       "      <td>101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>40463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>37766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>37766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>37766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>39</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>26259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>32971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>32971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>32971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>32971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>23384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>19296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>4383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>35074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>35074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>35074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>35074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>35074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>35074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>35074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>[14.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>27718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>[14.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>27718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>[15.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>27718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>[15.0]</td>\n",
       "      <td>[15.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>24429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>[15.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>2423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>[16.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>35307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>[11.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>35307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>[18.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>37536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>[10.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>37536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>[10.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>37536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>[18.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>37536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>[17.0]</td>\n",
       "      <td>[26.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>16730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>[14.0]</td>\n",
       "      <td>[25.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>16730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>[13.0]</td>\n",
       "      <td>[12.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>16730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>[11.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>16730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>[11.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>16730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>[19.0]</td>\n",
       "      <td>[15.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>37362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>[12.0]</td>\n",
       "      <td>[11.0]</td>\n",
       "      <td>[7.0]</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>38408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>[20.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>27726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>[20.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>27726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>[16.0]</td>\n",
       "      <td>[18.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>19296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>[11.0]</td>\n",
       "      <td>[10.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>43310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>[18.0]</td>\n",
       "      <td>[17.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>38528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>[11.0]</td>\n",
       "      <td>[20.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>29739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>[20.0]</td>\n",
       "      <td>[10.0]</td>\n",
       "      <td>[10.0]</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>12017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>[15.0]</td>\n",
       "      <td>[15.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>24429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>[13.0]</td>\n",
       "      <td>[12.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>16730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>[19.0]</td>\n",
       "      <td>[15.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>37362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>[12.0]</td>\n",
       "      <td>[11.0]</td>\n",
       "      <td>[7.0]</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>38408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>[20.0]</td>\n",
       "      <td>[10.0]</td>\n",
       "      <td>[10.0]</td>\n",
       "      <td>xyzBetween10and20mm</td>\n",
       "      <td>12017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    dimension_X dimension_Y dimension_Z        nlpql_feature  subject\n",
       "0            28          16         NaN    LesionMeasurement    40463\n",
       "1             6         NaN         NaN    LesionMeasurement    40463\n",
       "2            17           8         NaN    LesionMeasurement    40463\n",
       "3           110         101         NaN    LesionMeasurement    40463\n",
       "4             7         NaN         NaN    LesionMeasurement    37766\n",
       "5             6         NaN         NaN    LesionMeasurement    37766\n",
       "6             7         NaN         NaN    LesionMeasurement    37766\n",
       "7            39          20         NaN    LesionMeasurement    26259\n",
       "8             8         NaN         NaN    LesionMeasurement    43634\n",
       "9             5         NaN         NaN    LesionMeasurement    43634\n",
       "10            6         NaN         NaN    LesionMeasurement    43634\n",
       "11           12         NaN         NaN    LesionMeasurement    43634\n",
       "12            9         NaN         NaN    LesionMeasurement    43634\n",
       "13           11         NaN         NaN    LesionMeasurement    43634\n",
       "14            8         NaN         NaN    LesionMeasurement    43634\n",
       "15           34         NaN         NaN    LesionMeasurement    43634\n",
       "16            6         NaN         NaN    LesionMeasurement    32971\n",
       "17           25          23         NaN    LesionMeasurement    32971\n",
       "18           15         NaN         NaN    LesionMeasurement    32971\n",
       "19           17         NaN         NaN    LesionMeasurement    32971\n",
       "20            2         NaN         NaN    LesionMeasurement    23384\n",
       "21           16          18         NaN    LesionMeasurement    19296\n",
       "22            4         NaN         NaN    LesionMeasurement     4383\n",
       "23            5         NaN         NaN    LesionMeasurement    35074\n",
       "24            4         NaN         NaN    LesionMeasurement    35074\n",
       "25            3         NaN         NaN    LesionMeasurement    35074\n",
       "26           30          40         NaN    LesionMeasurement    35074\n",
       "27            4           3           2    LesionMeasurement    35074\n",
       "28           20         NaN         NaN    LesionMeasurement    35074\n",
       "29            2           2           2    LesionMeasurement    35074\n",
       "..          ...         ...         ...                  ...      ...\n",
       "164      [14.0]         NaN         NaN    xBetween10and20mm    27718\n",
       "165      [14.0]         NaN         NaN    xBetween10and20mm    27718\n",
       "166      [15.0]         NaN         NaN    xBetween10and20mm    27718\n",
       "167      [15.0]      [15.0]         NaN    xBetween10and20mm    24429\n",
       "168      [15.0]         NaN         NaN    xBetween10and20mm     2423\n",
       "169      [16.0]         NaN         NaN    xBetween10and20mm    35307\n",
       "170      [11.0]         NaN         NaN    xBetween10and20mm    35307\n",
       "171      [18.0]         NaN         NaN    xBetween10and20mm    37536\n",
       "172      [10.0]         NaN         NaN    xBetween10and20mm    37536\n",
       "173      [10.0]         NaN         NaN    xBetween10and20mm    37536\n",
       "174      [18.0]         NaN         NaN    xBetween10and20mm    37536\n",
       "175      [17.0]      [26.0]         NaN    xBetween10and20mm    16730\n",
       "176      [14.0]      [25.0]         NaN    xBetween10and20mm    16730\n",
       "177      [13.0]      [12.0]         NaN    xBetween10and20mm    16730\n",
       "178      [11.0]         NaN         NaN    xBetween10and20mm    16730\n",
       "179      [11.0]         NaN         NaN    xBetween10and20mm    16730\n",
       "180      [19.0]      [15.0]         NaN    xBetween10and20mm    37362\n",
       "181      [12.0]      [11.0]       [7.0]    xBetween10and20mm    38408\n",
       "182      [20.0]         NaN         NaN    xBetween10and20mm    27726\n",
       "183      [20.0]         NaN         NaN    xBetween10and20mm    27726\n",
       "184      [16.0]      [18.0]         NaN   xyBetween10and20mm    19296\n",
       "185      [11.0]      [10.0]         NaN   xyBetween10and20mm    43310\n",
       "186      [18.0]      [17.0]         NaN   xyBetween10and20mm    38528\n",
       "187      [11.0]      [20.0]         NaN   xyBetween10and20mm    29739\n",
       "188      [20.0]      [10.0]      [10.0]   xyBetween10and20mm    12017\n",
       "189      [15.0]      [15.0]         NaN   xyBetween10and20mm    24429\n",
       "190      [13.0]      [12.0]         NaN   xyBetween10and20mm    16730\n",
       "191      [19.0]      [15.0]         NaN   xyBetween10and20mm    37362\n",
       "192      [12.0]      [11.0]       [7.0]   xyBetween10and20mm    38408\n",
       "193      [20.0]      [10.0]      [10.0]  xyzBetween10and20mm    12017\n",
       "\n",
       "[194 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesion_csv = pd.read_csv('assets/lesion_intermediate.csv', \n",
    "                         usecols=['dimension_X', 'dimension_Y', 'dimension_Z', \n",
    "                                  'nlpql_feature', 'subject'])\n",
    "lesion_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spreadsheet Rows are MongoDB Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our run generated a CSV file containing a header row and 194 rows of data. This CSV file is a dump of the results for our particular job. These results are stored in a MongoDB collection called ``phenotype_results`` in a database called ``nlp``. It is important to understand that **each row** of data above is a separate document in the MongoDB database. For instance, here is the underlying ``MeasurementFinder`` database document for row 2 above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"_id\": \"5bfd9c9a31ab5b2e981dca14\",\n",
      "    \"sentence\": \"there is a 1.7 x 0.8 cm fdg-avid soft tissue nodule in the subcutaneous tissues of the right breast.\",\n",
      "    \"text\": \"1.7 x 0.8 cm\",\n",
      "    \"start\": 11,\n",
      "    \"value\": 17,\n",
      "    \"end\": 23,\n",
      "    \"term\": \"avid soft tissue nodule\",\n",
      "    \"dimension_X\": 17,\n",
      "    \"dimension_Y\": 8,\n",
      "    \"dimension_Z\": null,\n",
      "    \"units\": \"MILLIMETERS\",\n",
      "    \"location\": [\n",
      "        \"subcutaneous tissues of the right breast\"\n",
      "    ],\n",
      "    \"condition\": \"EQUAL\",\n",
      "    \"value1\": null,\n",
      "    \"value2\": \"\",\n",
      "    \"temporality\": \"CURRENT\",\n",
      "    \"min_value\": 8,\n",
      "    \"max_value\": 17,\n",
      "    \"pipeline_type\": \"MeasurementFinder\",\n",
      "    \"pipeline_id\": 12573,\n",
      "    \"job_id\": 11131,\n",
      "    \"batch\": 50,\n",
      "    \"owner\": \"claritynlp\",\n",
      "    \"nlpql_feature\": \"LesionMeasurement\",\n",
      "    \"inserted_date\": \"2018-11-27T14:35:54.749Z\",\n",
      "    \"concept_code\": -1,\n",
      "    \"phenotype_final\": false,\n",
      "    \"report_id\": \"1048492\",\n",
      "    \"subject\": \"40463\",\n",
      "    \"report_date\": \"2119-02-16T00:00:00Z\",\n",
      "    \"report_type\": \"Radiology\",\n",
      "    \"source\": \"MIMIC\",\n",
      "    \"solr_id\": \"1048492\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "obj = { \"_id\" : \"5bfd9c9a31ab5b2e981dca14\", \"sentence\" : \"there is a 1.7 x 0.8 cm fdg-avid soft tissue nodule in the subcutaneous tissues of the right breast.\", \"text\" : \"1.7 x 0.8 cm\", \"start\" : 11, \"value\" : 17, \"end\" : 23, \"term\" : \"avid soft tissue nodule\", \"dimension_X\" : 17, \"dimension_Y\" : 8, \"dimension_Z\" : None, \"units\" : \"MILLIMETERS\", \"location\" : [ \"subcutaneous tissues of the right breast\" ], \"condition\" : \"EQUAL\", \"value1\" : None, \"value2\" : \"\", \"temporality\" : \"CURRENT\", \"min_value\" : 8, \"max_value\" : 17, \"pipeline_type\" : \"MeasurementFinder\", \"pipeline_id\" : 12573, \"job_id\" : 11131, \"batch\" : 50, \"owner\" : \"claritynlp\", \"nlpql_feature\" : \"LesionMeasurement\", \"inserted_date\" : \"2018-11-27T14:35:54.749Z\", \"concept_code\" : -1, \"phenotype_final\" : False, \"report_id\" : \"1048492\", \"subject\" : \"40463\", \"report_date\" : \"2119-02-16T00:00:00Z\", \"report_type\" : \"Radiology\", \"source\" : \"MIMIC\", \"solr_id\" : \"1048492\" }\n",
    "print(json.dumps(obj, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to view the results in a spreadsheet to see how the field names become column names in the intermediate CSV file. Thus the CSV file provides a 'flattened' view of the database results for a particular job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output rows above, you can see that the results are broadly grouped by the value of the ``nlpql_feature`` field. There are four such groups with values ``LesionMeasurement``, ``xBetween10and20mm``, ``xyBetween10and20mm``, and ``xyzBetween10and20mm``. Take a look at the NLPQL file above and observe that these are the name strings in each ``define`` statement.\n",
    "\n",
    "A value of ``NaN`` (not a number) is the equivalent of a null result, meaning that no data was found for that measurement dimension.\n",
    "\n",
    "Rows 0-144 contain the extracted measurements, all of which have their ``nlpql_feature`` field equal to ``LesionMeasurement``. These rows comprise the output of the measurement extractor. They are the **input** data for the mathematical expressions that constrain the lesion measurements. The underlying documents for these ``LesionMeasurement`` results in the MongoDB database are called *task result documents*.\n",
    "\n",
    "Rows 145-183 have their ``nlpql_feature`` field equal to ``xBetween10and20mm``. Unlike the ``LesionMeasurement`` rows, which are directly generated by the MeasurementFinder task, these rows are generated by evaluation of a mathematical epxression. This expression places a constraint on the X dimension of each measurement. Only those measurements that satisfy the constraint fill these rows of the intermediate result file.\n",
    "\n",
    "Rows 184-192 have their ``nlpql_feature`` field equal to ``xyBetween10and20mm``. These rows are generated by evaluation of a mathematical expression that constrains both the X and Y measurement dimensions. Only those measurements that satisfy the constraints fill these rows of the intermediate result file.\n",
    "\n",
    "Row 193 has its ``nlpql_feature`` field equal to ``xyzBetween10and20mm``.  This row is the only measurement that survives the constraint on all three dimensions.\n",
    "\n",
    "Note that the ``xBetween10and20mm`` results contain 2D and 3D measurements, some of which have Y or Z dimensions that exceed 20 mm (such as rows 175 and 176). The constraint for these rows is only on the X dimension. The Y and Z dimensions can have any value whatsoever, even NaN (which means they don't exist).\n",
    "\n",
    "We see a single 3D measurement in the ``xy`` result section, in row 188. This measurement happens to have its Z dimension satisfying the constraints on the X and Y dimensions, but there is no constraint imposed on the measurement by the code itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLPQL Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the NLPQL example above, we expressed constraints on the measurement dimensions with NLPQL expressions. In this section we describe the different types of expression and provide an overview of how ClarityNLP evaluates them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematical Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An NLPQL mathematical expression is found in a ``define`` statement such as:\n",
    "<pre>\n",
    "define hasFever:\n",
    "    where Temperature.value >= 100.4;\n",
    "    \n",
    "define xBetween10and20mm:\n",
    "    where LesionMeasurement.dimension_X >= 10 AND LesionMeasurement.dimension_X <= 20;\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``where`` portion of the statement is the mathematical expression. These expressions feature mathematical operations on variables of the form ``nlpql_feature.variable_name`` such as ``Temperature.value``, ``LesionMeasurement.dimension_X``, etc. They can also include numeric literals such as ``100.4``.\n",
    "\n",
    "NLPQL mathematical expressions produce a numerical result from data contained in a **single** task result document. Since each task result document comprises a row in the intermediate results CSV file (see above), the evaluation of mathematical expressions is also called a **single-row operation**. The numerical result from the expression evaluation is written to a new MongoDB result document, as demonstrated in the lesion example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logical Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An NLPQL logical expression is also found in a ``define`` statement and involves the logical operators AND, OR, and NOT, such as:\n",
    "<pre>\n",
    "define hasSmallOrLargeLesion:\n",
    "    where xLessThan5mm OR xGreaterThan30mm;\n",
    "\n",
    "define hasSepsis:\n",
    "    where hasFever and hasSepsisSymptoms;\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``where`` portion of the statement is the logical expression. Logical expressions operate on high-level NLPQL features such as ``hasFever`` and ``hasSepsisSymptoms``, **not** on individual variables such as ``Temperature.value``. The presence of an ``nlpql_feature.variable_name`` token indicates that the expression is actually single-row, not multi-row.\n",
    "\n",
    "NLPQL logical expressions use data from one or more task result documents and compute a new set of results. The results are written back to MongoDB as a set of new result documents. The evaluation of a logical expression is also called a **multi-row operation**, since it typically consumes and generates multiple rows in the intermediate results CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Single-Row Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what does ClarityNLP have to do to evaluate a mathematical expression?\n",
    "\n",
    "First, the NLPQL front end parses the NLPQL file and generates a string of whitespace-separated tokens for each expression. The token string is passed to the evaluator which determines if it is single-row, multi-row, or something else that cannot be evaluated. If single-row, the the nlpql_feature and field list are extracted.\n",
    "\n",
    "Consider these examples, both of which are single-row mathematical expressions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "where Temperature.value >= 100.4\n",
    "where LesionMeasurement.dimension_X < 5 AND LesionMeasurement.dimension_Y < 5\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first expression has an ``nlpql_feature`` of ``Temperature`` and a field list containing the single entry ``value``. The second expression has an ``nlpql_feature`` of ``LesionMeasurement`` and a field list consisting of the entries ``dimension_X`` and ``dimension_Y``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial MongoDB Aggregation Pipeline Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next task for the evaluator is to convert the expression into a sequence of [MongoDB aggregation](https://docs.mongodb.com/manual/aggregation/) pipeline stages. The aggregation pipeline provides filtering, document transformation operations, and mathematical operations that ClarityNLP uses to evaluate expressions.\n",
    "\n",
    "The conversion process involves the generation of an initial [``$match``](https://docs.mongodb.com/manual/reference/operator/aggregation/match/#pipe._S_match) query to filter out everything but the data for the current job. The match query also checks for the existence of all entries in the field list and that they have non-null values. **A simple existence check is not sufficient**, since a null field actually exists but has a value that cannot be used for computation. Hence checks for existence and a non-null value are both necessary.\n",
    "\n",
    "For the two examples above, the ``$match`` query generates a pipeline filter stage that looks like this, assuming a job_id of 11116:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "// first example\n",
    "{\n",
    "    $match : {\n",
    "        \"job_id\" : 11116,\n",
    "        \"nlpql_feature\" : {$exists:true, $ne:null},\n",
    "        \"value\"         : {$exists:true, $ne:null}\n",
    "    }\n",
    "}\n",
    "\n",
    "// second example\n",
    "{\n",
    "    $match : {\n",
    "        \"job_id\" : 11116,\n",
    "        \"nlpql_feature\" : {$exists:true, $ne:null},\n",
    "        \"dimension_X\"   : {$exists:true, $ne:null},\n",
    "        \"dimension_Y\"   : {$exists:true, $ne:null}\n",
    "    }\n",
    "}\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the presence of this initial filter is the reason why the ``xBetween10and20mm`` results ignore the Y and Z dimensions, and why the ``xyBetween10and20mm`` results ignore the Z dimension. There are no filters on those variables in their respective ``define`` statements!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsequent Pipeline Stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating the initial filter stage, ClarityNLP transforms the mathematical expression from infix to postfix. The reason for this is to remove parentheses and to resolve operator precedence and associativity issues. NLPQL uses the same [operator precedence](https://docs.python.org/3/reference/expressions.html#operator-precedence) and associativity as the Python programming language.\n",
    "\n",
    "After conversion to postfix the expressions become:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "'nlpql_feature', 'Temperature', '==', 'value', '100.4', '>=', 'and'\n",
    "'nlpql_feature', 'LesionMeasurement', '==', 'dimension_X', '5', '<', 'dimension_Y', '5', '<', 'and', 'and'\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A postfix expression can be evaluated with a stack-based evaluator. The general idea is to push the postfix tokens onto a stack until an operator is encountered, at which point its operands are popped, the operator expression evaluated, and the result pushed back onto the stack.\n",
    "\n",
    "ClarityNLP uses this method, but the evaluation process does not compute a mathematical result. Instead, it performs string processing to generate MongoDB aggregation commands for evaluating the mathematical expression. MongoDB aggregation uses a consistent syntax that makes this evaluation process possible.\n",
    "\n",
    "After the postfix evaluation stage the expressions become:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "// (nlpql_feature == Temperature) and (value >= 100.4)\n",
    "{\n",
    "   $match : {\n",
    "       \"job_id\" : 11116,\n",
    "       \"nlpql_feature\" : {$exists:true, $ne:null},\n",
    "       \"value\"         : {$exists:true, $ne:null}\n",
    "   }\n",
    "},\n",
    "{\n",
    "    \"$project\" : {\n",
    "        \"value\" : {\n",
    "            \"$and\" : [\n",
    "                {\"$eq\"  : [\"$nlpql_feature\", \"Temperature\"]},\n",
    "                {\"$gte\" : [\"$value\", 100.4]}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// (nlpql_feature == LesionMeasurement) and (dimension_X < 5 and dimension_Y < 5)\n",
    "{\n",
    "    \"$match\" : {\n",
    "        \"job_id\" : 11116,\n",
    "        \"nlpql_feature\" : {$exists:true, $ne:null},\n",
    "        \"dimension_X\"   : {$exists:true, $ne:null},\n",
    "        \"dimension_Y\"   : {$exists:true, $ne:null}\n",
    "    }\n",
    "},\n",
    "{\n",
    "    \"$project\" : {\n",
    "        \"value\" : {\n",
    "            \"$and\" : [\n",
    "                {\n",
    "                    \"$eq\" : [\"$nlpql_feature\", \"LesionMeasurement\"]\n",
    "                },\n",
    "                {\n",
    "                    \"$and\" : [\n",
    "                        {\"$lt\" : [\"$dimension_X\", 5]},\n",
    "                        {\"$lt\" : [\"$dimension_Y\", 5]}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the aggregation pipelines for both expressions are complete. Each pipeline is sent to MongoDB where it runs and generates the results seen in the spreadsheet output above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Multi-Row Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case #1:  Sentiment Analysis\n",
    "For this  Cooking session, we are going to integrate a few external APIs that perform [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) and enable their use within the ClarityNLP ecosystem.  By the end of the session, you should have a good handle on how to incorporate any REST API into your [NLPQL](https://clarity-nlp.readthedocs.io/en/latest/user_guide/intro/overview.html#example-nlpql-phenotype-walkthrough) phenotypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Identify external APIs for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we want to leverage some of the brilliant minds in text analytics to help us perform Sentiment Analysis using ClarityNLP.  You may or may not be surprised to learn that there are >100 APIs out there for performing sentiment analysis.\n",
    "\n",
    "![NLPQL_Runner.png](assets/Sentiment_APIs.png)\n",
    "\n",
    "Our first stop will be [Microsoft Azure Text Analytics](https://westus.dev.cognitive.microsoft.com/docs/services/TextAnalytics.V2.0/operations/56f30ceeeda5650db055a3c9/console).  The Azure Sentiment API lets you pass in a simple sentence or group of sentences and get back an overall sentiment score from 0 to 1.  0 being very negative and 1 very positive.\n",
    "\n",
    "Here is an example from Postman:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLPQL_Runner.png](assets/Azure_Sentiment_Query.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentiment score for the above sentence is very low (i.e., negative).  Let's try something a little more upbeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLPQL_Runner.png](assets/Azure_Happy_Query.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have a much more positive score (99+).  It's pretty fun to play around with just different sentences (\"I am super mad at you\" scores a 0.14 whereas \"I am not super mad at you\" score a 0.03).  Cool stuff, but our goal today is to look at how we might integrate such an API into ClarityNLP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Transforming APIs into Custom Tasks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Start with a Template*\n",
    "\n",
    "The first thing we'll do is start with a [Custom API Task Base Template](https://github.com/ClarityNLP/ClarityNLP/blob/ceb40586257078ef4f3f7ea91739141d47e83748/nlp/custom_tasks/SampleAPITask.py). This sample task calls an API to assign a random Chuck Norris joke to every document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from tasks.task_utilities import BaseTask\n",
    "from pymongo import MongoClient\n",
    "import requests\n",
    "\n",
    "\n",
    "class SampleAPITask(BaseTask):\n",
    "    task_name = \"ChuckNorrisJokeTask\"\n",
    "\n",
    "    # NLPQL\n",
    "\n",
    "    # define sampleTask:\n",
    "    # Clarity.ChuckNorrisJokeTask({\n",
    "    #   documentset: [ProviderNotes]\n",
    "    # });\n",
    "\n",
    "    def run_custom_task(self, temp_file, mongo_client: MongoClient):\n",
    "        for doc in self.docs:\n",
    "\n",
    "            response = requests.post('http://api.icndb.com/jokes/random')\n",
    "            if response.status_code == 200:\n",
    "                json_response = response.json()\n",
    "                if json_response['type'] == 'success':\n",
    "                    val = json_response['value']\n",
    "                    obj = {\n",
    "                        'joke': val['joke']\n",
    "                    }\n",
    "\n",
    "                    # writing results\n",
    "                    self.write_result_data(temp_file, mongo_client, doc, obj)\n",
    "\n",
    "            else:\n",
    "                # writing to log (optional)\n",
    "                self.write_log_data(\"OOPS\", \"No jokes this time!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there is a lot of stuff to look at in there, but the only part you really have to pay attention to is the middle part below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "     \n",
    "        for doc in self.docs:\n",
    "\n",
    "            response = requests.post('http://api.icndb.com/jokes/random')\n",
    "            if response.status_code == 200:\n",
    "                json_response = response.json()\n",
    "                if json_response['type'] == 'success':\n",
    "                    val = json_response['value']\n",
    "                    obj = {\n",
    "                        'joke': val['joke']\n",
    "                    }\n",
    "\n",
    "                    # writing results\n",
    "                    self.write_result_data(temp_file, mongo_client, doc, obj)\n",
    "\n",
    "            else:\n",
    "                # writing to log (optional)\n",
    "                self.write_log_data(\"OOPS\", \"No jokes this time!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this means is that for each document in the selected documentset, make an API POST request. (The parameter `documentset: [ProviderNotes]` from our NLPQL becomes `self.docs` in the Custom Task code.)  The documentset could be nursing notes containing the word \"central line\" or  documents tagged \"Echocardiogram\" or any documentset you can imagine as we discusssed in a [prior Cooking class](https://github.com/ClarityNLP/ClarityNLP/blob/master/notebooks/cooking/Cooking_with_ClarityNLP_091218.ipynb).  They will always be referred to as `self.docs` in a Custom Task.\n",
    "\n",
    "For every one of these documents, this Task is going to ring up the `http://api.icndb.com/jokes/random` joke API and pick a good joke.  It will then add the joke to an object called `obj` and store it back in our results database.  Now, let's see if we can modify this for our Azure Sentiment API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Change the API Call*\n",
    "\n",
    "For our sentiment analysis, we need to change up the POST headers and body to match the Azure API specifications.  So our we'll change a couple things:\n",
    "\n",
    "```python\n",
    "headers = {'Content-Type': 'application/json', 'Ocp-Apim-Subscription-Key': 'XXXXXX'}\n",
    "payload = {\"documents\": [{\"language\": \"en\", \"id\": \"1\", \"text\": doc}]}\n",
    "response = requests.post('https://eastus.api.cognitive.microsoft.com/text/analytics/sentiment', headers=headers, json=payload)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've done is added some of the headers required (like our secret API key) and made the body of the request (the \"payload\") match the configuration shown in the Postman image above. Then instead of calling the ChuckNorris API, we change our call to Microsoft's URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Change the API Result Handling*\n",
    "\n",
    "Each API returns results in its own way, so you've got to follow the API documentation so see what you can expect back.  As we saw earlier, this Sentiment API responds with this kind of result:\n",
    "\n",
    "```json\n",
    "{\n",
    "\t\"documents\": [{\n",
    "\t\t\"score\": 0.14780092239379883,\n",
    "\t\t\"id\": \"1\"\n",
    "\t}],\n",
    "\t\"errors\": []\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we'll build our object a little differently than we did for Chuck Norris.  It'll need to look something like this.\n",
    "\n",
    "```python\n",
    "json_response = response.json()\n",
    "val = json_response['documents'][0]\n",
    "obj = {\n",
    "    'sentiment_score': val['score']\n",
    "    }\n",
    "```\n",
    "\n",
    "If we were passing in multiple documents at a time (which we are not), we would need to loop through the response one document at a time.  But in this case, we can just take the first (and only) response, hence the [0]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*API Keys*\n",
    "\n",
    "Chuck Norris was a free API.  Azure is also free for limited usage, but you need an API key.  In this version, we are going to rely on the user to supply us the API key by passing a parameter in their NLPQL.  Here is example NLPQL we might see:\n",
    "\n",
    "```\n",
    "define PatientFeelings:\n",
    "    Clarity.AzureSentiment({\n",
    "        documentset: [ProviderNotes],\n",
    "        \"api_key\": \"{your_api_key}\"\n",
    "    });\n",
    "```\n",
    "\n",
    "In order to \"catch\" this api_key and use it in our Custom Task, we've got a library that get custom_arguments from the NLPQL.  It looks like this:\n",
    "\n",
    "```\n",
    "self.pipeline_config.custom_arguments['{parameter_name']\n",
    "```\n",
    "\n",
    "So in this case, `self.pipeline_config.custom_arguments['api_key']` would retrieve the API key submitted by the user in the NLPQL.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So putting the whole thing together, we've got our final code:\n",
    "\n",
    "```python\n",
    "    for doc in self.docs:\n",
    "        headers = {'Content-Type': 'application/json', 'Ocp-Apim-Subscription-Key': self.pipeline_config.custom_arguments['api_key']}\n",
    "        payload = {\"documents\": [{\"language\": \"en\", \"id\": \"1\", \"text\": doc}]}\n",
    "        response = requests.post('https://eastus.api.cognitive.microsoft.com/text/analytics/v2.0/sentiment', headers=headers, json=payload)\n",
    "        json_response = response.json()\n",
    "        val = json_response['documents'][0]\n",
    "        obj = {\n",
    "            'sentiment_score': val['score'],\n",
    "        }\n",
    "\n",
    "        # writing results\n",
    "        self.write_result_data(temp_file, mongo_client, doc, obj)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the final code, with the wrapping back in place and a little bit of error handling thrown in, take a look at [AzureSentimentTask.py](https://github.com/ClarityNLP/ClarityNLP/blob/master/nlp/custom_tasks/AzureSentimentTask.py) in the repo. We made one additional tweak to be sure we are only sending sentences containing birds.  \n",
    "\n",
    "```java\n",
    "for doc in self.docs:\n",
    "  sentence_list = self.get_document_sentences(doc)\n",
    "    for sentence in sentence_list:\n",
    "      if any(word.lower() in sentence.lower() for word in self.pipeline_config.terms):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Using the Sentiment API Task in a Query "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our API can now be called using the NLPQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Successfully Submitted\n",
      "{\n",
      "    \"intermediate_results_csv\": \"http://18.220.133.76:5000/job_results/643/phenotype_intermediate\",\n",
      "    \"job_id\": \"643\",\n",
      "    \"luigi_task_monitoring\": \"http://18.220.133.76:8082/static/visualiser/index.html#search__search=job=643\",\n",
      "    \"main_results_csv\": \"http://18.220.133.76:5000/job_results/643/phenotype\",\n",
      "    \"phenotype_config\": \"http://18.220.133.76:5000/phenotype_id/643\",\n",
      "    \"phenotype_id\": \"643\",\n",
      "    \"pipeline_configs\": [\n",
      "        \"http://18.220.133.76:5000/pipeline_id/862\"\n",
      "    ],\n",
      "    \"pipeline_ids\": [\n",
      "        862\n",
      "    ],\n",
      "    \"results_viewer\": \"?job=643\",\n",
      "    \"status_endpoint\": \"http://18.220.133.76:5000/status/643\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "nlpql ='''\n",
    "limit 1;\n",
    "\n",
    "//phenotype name\n",
    "phenotype \"How we feel about birds\" version \"1\";\n",
    "\n",
    "//include Clarity main NLP libraries\n",
    "include ClarityCore version \"1.0\" called Clarity;\n",
    "\n",
    "termset Birds:\n",
    "  [\"football\"];\n",
    "\n",
    "define BirdFeelings:\n",
    "  Clarity.AzureSentiment({\n",
    "    termset:[Birds],\n",
    "    \"api_key\":\"'''+azure_key+'''\"\n",
    "    });\n",
    "'''\n",
    "run_result, main_csv, intermediate_csv, luigi = claritynlp.run_nlpql(nlpql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLPQL Editor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know-- NLPQL isn't too hard to read or copy/tweak, but it is pretty tough to generate *de novo*.  So we've created an editor that helps you build your NLPQL without worrying about a missed semi-colon here or bracket there.  Let's [check it out](https://nlpql-editor.herokuapp.com/demo.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLPQL_Runner.png](assets/NLPQL_editor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for joining this week's Cooking with ClarityNLP!  Please send any requests or ideas for future Cooking shows to charity.hilton@gtri.gatech.edu.\n",
    "\n",
    "Have a great week!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
