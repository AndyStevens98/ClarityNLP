{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cooking with ClarityNLP - Session #7 - NLPQL Under the Hood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will take a behind-the-scenes look at how ClarityNLP evaluates NLPQL expressions. We will walk through the construction of an NLPQL file and give a high-level description of how its results are generated. We will also provide an overview of our new NLPQL editor tool that makes the task of creating NLPQL files much easier. For background on installing and using ClarityNLP, please see our [documentation](https://claritynlp.readthedocs.io/en/latest/index.html).  We welcome questions via Slack or on [GitHub](https://github.com/ClarityNLP/ClarityNLP/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Measurements from Radiology Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start things off, suppose that we're developing a promising new immunotherapy drug. This drug has proven effective on tumors of various sizes, but we have noted particular efficacy for tumors in the 1 cm to 2 cm size range. We want to recruit patients for a new clinical trial designed to test the drug on tumors of this size.  We have access to a corpus of radiology reports, and we would like to search these reports for patients with appropriately-sized tumors. How can we use ClarityNLP to find more patients?\n",
    "\n",
    "As you've learned in previous cooking sessions, we need to create an NLPQL file with the relevant commands.\n",
    "\n",
    "When developing new NLPQL it is best to limit the number of documents processed, until the NLPQL is fully debugged and working. So let's start by limiting our initial document set to 50 documents. It shouldn't take too long to processes 50 documents, and if we make a mistake, we can quickly recover. \n",
    "\n",
    "A limit on the number of documents processed is specified by a ``limit`` statement on the first line of the NLPQL file. So open a text editor, create a new file called ``lesion.nlpql``, and enter the following line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>limit 50;</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to insert some boilerplate that identifies the phenotype and version, provides a description, and imports the ClarityNLP libraries. All of your NLPQL files will have something like this at the start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>phenotype \"Lesions1to2Cm\" version \"1\";\n",
    "description \"Find lesions of sizes ranging from 1 to 2 cm.\";\n",
    "include ClarityCore version \"1.0\" called Clarity;</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to search only radiology reports, we can create a documentset specifically for this purpose. Note that the ``report_types`` field is an array with the single entry ``Radiology``. We will process documents from the MIMIC-III dataset, which identifies radiology reports with this label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "documentset Docs:\n",
    "    Clarity.createDocumentSet({\n",
    "        \"report_types\":[\"Radiology\"]\n",
    "    });\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create a list of the terms we want ClarityNLP to search for. We ponder this for a while and eventually construct a termset that uses language common to radiology:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "termset LesionTerms: [\n",
    "    \"lesion\", \"growth\", \"mass\", \"malignancy\", \"tumor\",\n",
    "    \"neoplasm\", \"nodule\", \"cyst\", \"focus of enhancement\",\n",
    "    \"echodensity\", \"hypoechoic focus\", \"echogenic focus\"\n",
    "];\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to find and extract measurements, we must insert a command to activate ClarityNLP's measurement finder. The simplest command to do this is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "define LesionMeasurement:\n",
    "    Clarity.MeasurementFinder({\n",
    "        documentset: [Docs],\n",
    "        termset: [LesionTerms]\n",
    "    });\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how we tell the measurement finder to examine only the documents in our custom document set. The termset specification tells the measurement finder to return a measurement only if it appears in the same sentence as one of our custom lesion terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to find **patients** with tumors of the specified dimensions, so we specify a ``Patient`` context:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "context Patient;\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to write the commands for constraining the lesion measurements to our desired size of 1-2 cm. Here we will insert three commands to do so, and will explain the differences in results for each below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "define xBetween10and20mm:\n",
    "    where LesionMeasurement.dimension_X >= 10 AND LesionMeasurement.dimension_X <= 20;\n",
    "\n",
    "define xyBetween10and20mm:\n",
    "    where LesionMeasurement.dimension_X >= 10 AND LesionMeasurement.dimension_X <= 20 AND\n",
    "          LesionMeasurement.dimension_Y >= 10 AND LesionMeasurement.dimension_Y <= 20;\n",
    "\n",
    "define xyzBetween10and20mm:\n",
    "    where LesionMeasurement.dimension_X >= 10 AND LesionMeasurement.dimension_X <= 20 AND\n",
    "          LesionMeasurement.dimension_Y >= 10 AND LesionMeasurement.dimension_Y <= 20 AND\n",
    "          LesionMeasurement.dimension_Z >= 10 AND LesionMeasurement.dimension_Z <= 20;\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ClarityNLP normalizes all dimensional measurements to units of **millimeters**, so our desired range of 1-2 cm becomes 10-20 mm. These three statements enforce constratints on the X, XY, and XYZ measurement components respectively.\n",
    "\n",
    "And with that we're done. Here is the text of the final ``lesion.nlpql``:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "limit 50;\n",
    "phenotype \"LesionDemo\" version \"1\";\n",
    "description \"Find lesions of various sizes.\";\n",
    "include ClarityCore version \"1.0\" called Clarity;\n",
    "\n",
    "// radiology documents only in the documentset\n",
    "documentset Docs:\n",
    "    Clarity.createDocumentSet({\n",
    "        \"report_types\":[\"Radiology\"]\n",
    "    });\n",
    "\n",
    "// lesion terms\n",
    "termset LesionTerms: [\n",
    "    \"lesion\", \"growth\", \"mass\", \"malignancy\", \"tumor\",\n",
    "    \"neoplasm\", \"nodule\", \"cyst\", \"focus of enhancement\",\n",
    "    \"echodensity\", \"hyperechogenic focus\"\n",
    "];\n",
    "\n",
    "// extract lesion measurements\n",
    "define LesionMeasurement:\n",
    "    Clarity.MeasurementFinder({\n",
    "        documentset: [Docs],\n",
    "        termset: [LesionTerms]\n",
    "    });\n",
    "\n",
    "// we want to find patients, so use 'Patient' context\n",
    "context Patient;\n",
    "\n",
    "define xBetween10and20mm:\n",
    "    where LesionMeasurement.dimension_X >= 10 AND LesionMeasurement.dimension_X <= 20;\n",
    "\n",
    "define xyBetween10and20mm:\n",
    "    where LesionMeasurement.dimension_X >= 10 AND LesionMeasurement.dimension_X <= 20 AND\n",
    "          LesionMeasurement.dimension_Y >= 10 AND LesionMeasurement.dimension_Y <= 20;\n",
    "\n",
    "define xyzBetween10and20mm:\n",
    "    where LesionMeasurement.dimension_X >= 10 AND LesionMeasurement.dimension_X <= 20 AND\n",
    "          LesionMeasurement.dimension_Y >= 10 AND LesionMeasurement.dimension_Y <= 20 AND\n",
    "          LesionMeasurement.dimension_Z >= 10 AND LesionMeasurement.dimension_Z <= 20;\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the NLPQL Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before trying to process documents with our new NLPQL file, it is a good idea to first check it for syntax errors. We can do this by submitting it to the ``nlpql_tester`` API endpoint, a useful tool for the NLPQL developer.\n",
    "\n",
    "In prevous cooking sessions we showed you how to use the [Postman](www.postman.com) GUI tool to submit NLPQL files to the ClarityNLP webserver. Today we will show you how to use a command-line tool called [cURL](https://curl.haxx.se/) to do the same thing.\n",
    "\n",
    "The nlpql_tester API for a local ClarityNLP instance is typically found at ``localhost:5000/nlpql_tester``. The NLPQL file should be sent via HTTP POST using a content type of ``text/plain``.\n",
    "\n",
    "To submit the file, install ``curl`` on your system, then open a terminal window, change directories to the location of ``lesion.nlpql``, and run this command:\n",
    "\n",
    "<pre>\n",
    "curl -i -X POST http://localhost:5000/nlpql_tester -H \"Content-Type: text/plain\" --data-binary \"@lesion.nlpql\"\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The various options have the following meanings:\n",
    "```\n",
    "-i: include the HTTP header in the output\n",
    "-X: request type (must be ``POST``)\n",
    "-H: add the subsequent ``Content-Type: text/plain`` to the header of the HTTP request\n",
    "--data-binary: POST the data exactly as specified, no additional processing\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the NLPQL tester directly from this notebook by first running the code in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code below is only required for running ClarityNLP in Jupyter notebooks.\n",
    "# It is not required if running NLPQL via API or the ClarityNLP GUI.\n",
    "import pandas as pd\n",
    "import claritynlp_notebook_helpers as claritynlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the next cell to test the NLPQL file:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**BROKEN**\n",
    "lesion_nlpql_text = claritynlp.load_file('assets/lesion.nlpql')\n",
    "json_result = claritynlp.run_nlpql_tester(lesion_nlpql_text)\n",
    "print(json_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system should respond with a JSON result with no mention of error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the NLPQL File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having verified that the NLPQL file has the proper syntax, you can submit the job to the ClarityNLP server via cURL with a similar command:\n",
    "<pre>\n",
    "curl -i -X POST http://localhost:5000/nlpql -H \"Content-Type: text/plain\" --data-binary \"@lesion.nlpql\"\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can run from the next notebook cell:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**BROKEN**\n",
    "lesion_nlpql_text = claritynlp.load_file('assets/lesion.nlpql')\n",
    "run_result, main_csv, intermediate_csv, luigi = claritynlp.run_nlpql(lesion_nlpql_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job may take several minutes to run. After it runs to completion, browse to the location of the CSV file containing the intermediate results, and open in in a spreadsheet application such as Microsoft Excel. We have saved the results of a run to ``assets/lesion_intermediate.csv``, some of which is displayed in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dimension_X</th>\n",
       "      <th>dimension_Y</th>\n",
       "      <th>dimension_Z</th>\n",
       "      <th>nlpql_feature</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>40463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>40463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>40463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110</td>\n",
       "      <td>101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>40463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>37766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>37766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>37766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>39</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>26259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>43634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>32971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>32971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>32971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>32971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>23384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>19296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>4383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>35074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>35074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>35074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>35074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>35074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>35074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>LesionMeasurement</td>\n",
       "      <td>35074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>[14.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>27718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>[14.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>27718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>[15.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>27718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>[15.0]</td>\n",
       "      <td>[15.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>24429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>[15.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>2423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>[16.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>35307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>[11.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>35307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>[18.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>37536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>[10.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>37536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>[10.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>37536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>[18.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>37536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>[17.0]</td>\n",
       "      <td>[26.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>16730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>[14.0]</td>\n",
       "      <td>[25.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>16730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>[13.0]</td>\n",
       "      <td>[12.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>16730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>[11.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>16730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>[11.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>16730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>[19.0]</td>\n",
       "      <td>[15.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>37362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>[12.0]</td>\n",
       "      <td>[11.0]</td>\n",
       "      <td>[7.0]</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>38408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>[20.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>27726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>[20.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xBetween10and20mm</td>\n",
       "      <td>27726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>[16.0]</td>\n",
       "      <td>[18.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>19296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>[11.0]</td>\n",
       "      <td>[10.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>43310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>[18.0]</td>\n",
       "      <td>[17.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>38528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>[11.0]</td>\n",
       "      <td>[20.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>29739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>[20.0]</td>\n",
       "      <td>[10.0]</td>\n",
       "      <td>[10.0]</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>12017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>[15.0]</td>\n",
       "      <td>[15.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>24429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>[13.0]</td>\n",
       "      <td>[12.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>16730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>[19.0]</td>\n",
       "      <td>[15.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>37362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>[12.0]</td>\n",
       "      <td>[11.0]</td>\n",
       "      <td>[7.0]</td>\n",
       "      <td>xyBetween10and20mm</td>\n",
       "      <td>38408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>[20.0]</td>\n",
       "      <td>[10.0]</td>\n",
       "      <td>[10.0]</td>\n",
       "      <td>xyzBetween10and20mm</td>\n",
       "      <td>12017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    dimension_X dimension_Y dimension_Z        nlpql_feature  subject\n",
       "0            28          16         NaN    LesionMeasurement    40463\n",
       "1             6         NaN         NaN    LesionMeasurement    40463\n",
       "2            17           8         NaN    LesionMeasurement    40463\n",
       "3           110         101         NaN    LesionMeasurement    40463\n",
       "4             7         NaN         NaN    LesionMeasurement    37766\n",
       "5             6         NaN         NaN    LesionMeasurement    37766\n",
       "6             7         NaN         NaN    LesionMeasurement    37766\n",
       "7            39          20         NaN    LesionMeasurement    26259\n",
       "8             8         NaN         NaN    LesionMeasurement    43634\n",
       "9             5         NaN         NaN    LesionMeasurement    43634\n",
       "10            6         NaN         NaN    LesionMeasurement    43634\n",
       "11           12         NaN         NaN    LesionMeasurement    43634\n",
       "12            9         NaN         NaN    LesionMeasurement    43634\n",
       "13           11         NaN         NaN    LesionMeasurement    43634\n",
       "14            8         NaN         NaN    LesionMeasurement    43634\n",
       "15           34         NaN         NaN    LesionMeasurement    43634\n",
       "16            6         NaN         NaN    LesionMeasurement    32971\n",
       "17           25          23         NaN    LesionMeasurement    32971\n",
       "18           15         NaN         NaN    LesionMeasurement    32971\n",
       "19           17         NaN         NaN    LesionMeasurement    32971\n",
       "20            2         NaN         NaN    LesionMeasurement    23384\n",
       "21           16          18         NaN    LesionMeasurement    19296\n",
       "22            4         NaN         NaN    LesionMeasurement     4383\n",
       "23            5         NaN         NaN    LesionMeasurement    35074\n",
       "24            4         NaN         NaN    LesionMeasurement    35074\n",
       "25            3         NaN         NaN    LesionMeasurement    35074\n",
       "26           30          40         NaN    LesionMeasurement    35074\n",
       "27            4           3           2    LesionMeasurement    35074\n",
       "28           20         NaN         NaN    LesionMeasurement    35074\n",
       "29            2           2           2    LesionMeasurement    35074\n",
       "..          ...         ...         ...                  ...      ...\n",
       "164      [14.0]         NaN         NaN    xBetween10and20mm    27718\n",
       "165      [14.0]         NaN         NaN    xBetween10and20mm    27718\n",
       "166      [15.0]         NaN         NaN    xBetween10and20mm    27718\n",
       "167      [15.0]      [15.0]         NaN    xBetween10and20mm    24429\n",
       "168      [15.0]         NaN         NaN    xBetween10and20mm     2423\n",
       "169      [16.0]         NaN         NaN    xBetween10and20mm    35307\n",
       "170      [11.0]         NaN         NaN    xBetween10and20mm    35307\n",
       "171      [18.0]         NaN         NaN    xBetween10and20mm    37536\n",
       "172      [10.0]         NaN         NaN    xBetween10and20mm    37536\n",
       "173      [10.0]         NaN         NaN    xBetween10and20mm    37536\n",
       "174      [18.0]         NaN         NaN    xBetween10and20mm    37536\n",
       "175      [17.0]      [26.0]         NaN    xBetween10and20mm    16730\n",
       "176      [14.0]      [25.0]         NaN    xBetween10and20mm    16730\n",
       "177      [13.0]      [12.0]         NaN    xBetween10and20mm    16730\n",
       "178      [11.0]         NaN         NaN    xBetween10and20mm    16730\n",
       "179      [11.0]         NaN         NaN    xBetween10and20mm    16730\n",
       "180      [19.0]      [15.0]         NaN    xBetween10and20mm    37362\n",
       "181      [12.0]      [11.0]       [7.0]    xBetween10and20mm    38408\n",
       "182      [20.0]         NaN         NaN    xBetween10and20mm    27726\n",
       "183      [20.0]         NaN         NaN    xBetween10and20mm    27726\n",
       "184      [16.0]      [18.0]         NaN   xyBetween10and20mm    19296\n",
       "185      [11.0]      [10.0]         NaN   xyBetween10and20mm    43310\n",
       "186      [18.0]      [17.0]         NaN   xyBetween10and20mm    38528\n",
       "187      [11.0]      [20.0]         NaN   xyBetween10and20mm    29739\n",
       "188      [20.0]      [10.0]      [10.0]   xyBetween10and20mm    12017\n",
       "189      [15.0]      [15.0]         NaN   xyBetween10and20mm    24429\n",
       "190      [13.0]      [12.0]         NaN   xyBetween10and20mm    16730\n",
       "191      [19.0]      [15.0]         NaN   xyBetween10and20mm    37362\n",
       "192      [12.0]      [11.0]       [7.0]   xyBetween10and20mm    38408\n",
       "193      [20.0]      [10.0]      [10.0]  xyzBetween10and20mm    12017\n",
       "\n",
       "[194 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesion_csv = pd.read_csv('assets/lesion_intermediate.csv', \n",
    "                         usecols=['dimension_X', 'dimension_Y', 'dimension_Z', \n",
    "                                  'nlpql_feature', 'subject'])\n",
    "lesion_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our run generated a CSV file containing a header row and 194 rows of data. This CSV file is a dump of the results stored in a MongoDB collection called ``phenotype_results``, which resides in a database called ``nlp``. It is important to understand that **each row** of data above is a separate document in the MongoDB database.\n",
    "\n",
    "You can see that the results are broadly grouped by the value of the ``nlpql_feature`` field. There are four such groups with values ``LesionMeasurement``, ``xBetween10and20mm``, ``xyBetween10and20mm``, and ``xyzBetween10and20mm``. Take a look at the NLPQL file and see why this is so.\n",
    "\n",
    "A value of ``NaN`` (not a number) is the equivalent of a null result, meaning that no data was found for that measurement dimension.\n",
    "\n",
    "Rows 0-144 contain the extracted measurements, which have their ``nlpql_feature`` field equal to ``LesionMeasurement``. These rows comprise the output of the measurement extractor and form the **input** data for the mathematical expressions that set constraints on the desired lesion measurements. The underlying documents for these rows in the MongoDB database are called *task result documents*.\n",
    "\n",
    "Rows 145-183 have their ``nlpql_feature`` field equal to ``xBetween10and20mm``. These result from processing the raw measurements and subjecting them to the stated constraint on the X dimension.\n",
    "\n",
    "Rows 184-192 have their ``nlpql_feature`` field equal to ``xyBetween10and20mm``. These rows result from processing the raw measurements and subjecting them to the constraints on X and Y.\n",
    "\n",
    "Row 193 has its ``nlpql_feature`` field equal to ``xyzBetween10and20mm``.  This row is the only measurement that survives the constraint on all three dimensions.\n",
    "\n",
    "Note that the ``xBetween10and20mm`` results contain 2D and 3D measurements, some of which have Y or Z dimensions that exceed 20 mm (such as rows 175 and 176). These rows only impose constraints on the X dimension, so the Y and Z dimensions can have any value whatsoever, even NaN, which means they don't exist.\n",
    "\n",
    "We see a single 3D measurement in the ``xy`` result section, in row 188. This measurement happens to have its Z dimension satisfying the constraints on the X and Y dimensions, but there is no constraint imposed on the measurement by the code itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLPQL Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the NLPQL example above, we expressed constraints on the measurement dimensions via NLPQL expressions. In this section we describe the different types of expression and provide an overview of how ClarityNLP evaluates them.\n",
    "\n",
    "The ClarityNLP expression evaluator is built...mongo aggregation...more efficient\n",
    "\n",
    "math expressions\n",
    "logic expressions\n",
    "\n",
    "eval of math expressions\n",
    "eval of logic expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case #1:  Sentiment Analysis\n",
    "For this  Cooking session, we are going to integrate a few external APIs that perform [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) and enable their use within the ClarityNLP ecosystem.  By the end of the session, you should have a good handle on how to incorporate any REST API into your [NLPQL](https://clarity-nlp.readthedocs.io/en/latest/user_guide/intro/overview.html#example-nlpql-phenotype-walkthrough) phenotypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Identify external APIs for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we want to leverage some of the brilliant minds in text analytics to help us perform Sentiment Analysis using ClarityNLP.  You may or may not be surprised to learn that there are >100 APIs out there for performing sentiment analysis.\n",
    "\n",
    "![NLPQL_Runner.png](assets/Sentiment_APIs.png)\n",
    "\n",
    "Our first stop will be [Microsoft Azure Text Analytics](https://westus.dev.cognitive.microsoft.com/docs/services/TextAnalytics.V2.0/operations/56f30ceeeda5650db055a3c9/console).  The Azure Sentiment API lets you pass in a simple sentence or group of sentences and get back an overall sentiment score from 0 to 1.  0 being very negative and 1 very positive.\n",
    "\n",
    "Here is an example from Postman:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLPQL_Runner.png](assets/Azure_Sentiment_Query.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentiment score for the above sentence is very low (i.e., negative).  Let's try something a little more upbeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLPQL_Runner.png](assets/Azure_Happy_Query.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have a much more positive score (99+).  It's pretty fun to play around with just different sentences (\"I am super mad at you\" scores a 0.14 whereas \"I am not super mad at you\" score a 0.03).  Cool stuff, but our goal today is to look at how we might integrate such an API into ClarityNLP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Transforming APIs into Custom Tasks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Start with a Template*\n",
    "\n",
    "The first thing we'll do is start with a [Custom API Task Base Template](https://github.com/ClarityNLP/ClarityNLP/blob/ceb40586257078ef4f3f7ea91739141d47e83748/nlp/custom_tasks/SampleAPITask.py). This sample task calls an API to assign a random Chuck Norris joke to every document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from tasks.task_utilities import BaseTask\n",
    "from pymongo import MongoClient\n",
    "import requests\n",
    "\n",
    "\n",
    "class SampleAPITask(BaseTask):\n",
    "    task_name = \"ChuckNorrisJokeTask\"\n",
    "\n",
    "    # NLPQL\n",
    "\n",
    "    # define sampleTask:\n",
    "    # Clarity.ChuckNorrisJokeTask({\n",
    "    #   documentset: [ProviderNotes]\n",
    "    # });\n",
    "\n",
    "    def run_custom_task(self, temp_file, mongo_client: MongoClient):\n",
    "        for doc in self.docs:\n",
    "\n",
    "            response = requests.post('http://api.icndb.com/jokes/random')\n",
    "            if response.status_code == 200:\n",
    "                json_response = response.json()\n",
    "                if json_response['type'] == 'success':\n",
    "                    val = json_response['value']\n",
    "                    obj = {\n",
    "                        'joke': val['joke']\n",
    "                    }\n",
    "\n",
    "                    # writing results\n",
    "                    self.write_result_data(temp_file, mongo_client, doc, obj)\n",
    "\n",
    "            else:\n",
    "                # writing to log (optional)\n",
    "                self.write_log_data(\"OOPS\", \"No jokes this time!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there is a lot of stuff to look at in there, but the only part you really have to pay attention to is the middle part below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "     \n",
    "        for doc in self.docs:\n",
    "\n",
    "            response = requests.post('http://api.icndb.com/jokes/random')\n",
    "            if response.status_code == 200:\n",
    "                json_response = response.json()\n",
    "                if json_response['type'] == 'success':\n",
    "                    val = json_response['value']\n",
    "                    obj = {\n",
    "                        'joke': val['joke']\n",
    "                    }\n",
    "\n",
    "                    # writing results\n",
    "                    self.write_result_data(temp_file, mongo_client, doc, obj)\n",
    "\n",
    "            else:\n",
    "                # writing to log (optional)\n",
    "                self.write_log_data(\"OOPS\", \"No jokes this time!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this means is that for each document in the selected documentset, make an API POST request. (The parameter `documentset: [ProviderNotes]` from our NLPQL becomes `self.docs` in the Custom Task code.)  The documentset could be nursing notes containing the word \"central line\" or  documents tagged \"Echocardiogram\" or any documentset you can imagine as we discusssed in a [prior Cooking class](https://github.com/ClarityNLP/ClarityNLP/blob/master/notebooks/cooking/Cooking_with_ClarityNLP_091218.ipynb).  They will always be referred to as `self.docs` in a Custom Task.\n",
    "\n",
    "For every one of these documents, this Task is going to ring up the `http://api.icndb.com/jokes/random` joke API and pick a good joke.  It will then add the joke to an object called `obj` and store it back in our results database.  Now, let's see if we can modify this for our Azure Sentiment API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Change the API Call*\n",
    "\n",
    "For our sentiment analysis, we need to change up the POST headers and body to match the Azure API specifications.  So our we'll change a couple things:\n",
    "\n",
    "```python\n",
    "headers = {'Content-Type': 'application/json', 'Ocp-Apim-Subscription-Key': 'XXXXXX'}\n",
    "payload = {\"documents\": [{\"language\": \"en\", \"id\": \"1\", \"text\": doc}]}\n",
    "response = requests.post('https://eastus.api.cognitive.microsoft.com/text/analytics/sentiment', headers=headers, json=payload)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've done is added some of the headers required (like our secret API key) and made the body of the request (the \"payload\") match the configuration shown in the Postman image above. Then instead of calling the ChuckNorris API, we change our call to Microsoft's URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Change the API Result Handling*\n",
    "\n",
    "Each API returns results in its own way, so you've got to follow the API documentation so see what you can expect back.  As we saw earlier, this Sentiment API responds with this kind of result:\n",
    "\n",
    "```json\n",
    "{\n",
    "\t\"documents\": [{\n",
    "\t\t\"score\": 0.14780092239379883,\n",
    "\t\t\"id\": \"1\"\n",
    "\t}],\n",
    "\t\"errors\": []\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we'll build our object a little differently than we did for Chuck Norris.  It'll need to look something like this.\n",
    "\n",
    "```python\n",
    "json_response = response.json()\n",
    "val = json_response['documents'][0]\n",
    "obj = {\n",
    "    'sentiment_score': val['score']\n",
    "    }\n",
    "```\n",
    "\n",
    "If we were passing in multiple documents at a time (which we are not), we would need to loop through the response one document at a time.  But in this case, we can just take the first (and only) response, hence the [0]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*API Keys*\n",
    "\n",
    "Chuck Norris was a free API.  Azure is also free for limited usage, but you need an API key.  In this version, we are going to rely on the user to supply us the API key by passing a parameter in their NLPQL.  Here is example NLPQL we might see:\n",
    "\n",
    "```\n",
    "define PatientFeelings:\n",
    "    Clarity.AzureSentiment({\n",
    "        documentset: [ProviderNotes],\n",
    "        \"api_key\": \"{your_api_key}\"\n",
    "    });\n",
    "```\n",
    "\n",
    "In order to \"catch\" this api_key and use it in our Custom Task, we've got a library that get custom_arguments from the NLPQL.  It looks like this:\n",
    "\n",
    "```\n",
    "self.pipeline_config.custom_arguments['{parameter_name']\n",
    "```\n",
    "\n",
    "So in this case, `self.pipeline_config.custom_arguments['api_key']` would retrieve the API key submitted by the user in the NLPQL.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So putting the whole thing together, we've got our final code:\n",
    "\n",
    "```python\n",
    "    for doc in self.docs:\n",
    "        headers = {'Content-Type': 'application/json', 'Ocp-Apim-Subscription-Key': self.pipeline_config.custom_arguments['api_key']}\n",
    "        payload = {\"documents\": [{\"language\": \"en\", \"id\": \"1\", \"text\": doc}]}\n",
    "        response = requests.post('https://eastus.api.cognitive.microsoft.com/text/analytics/v2.0/sentiment', headers=headers, json=payload)\n",
    "        json_response = response.json()\n",
    "        val = json_response['documents'][0]\n",
    "        obj = {\n",
    "            'sentiment_score': val['score'],\n",
    "        }\n",
    "\n",
    "        # writing results\n",
    "        self.write_result_data(temp_file, mongo_client, doc, obj)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the final code, with the wrapping back in place and a little bit of error handling thrown in, take a look at [AzureSentimentTask.py](https://github.com/ClarityNLP/ClarityNLP/blob/master/nlp/custom_tasks/AzureSentimentTask.py) in the repo. We made one additional tweak to be sure we are only sending sentences containing birds.  \n",
    "\n",
    "```java\n",
    "for doc in self.docs:\n",
    "  sentence_list = self.get_document_sentences(doc)\n",
    "    for sentence in sentence_list:\n",
    "      if any(word.lower() in sentence.lower() for word in self.pipeline_config.terms):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Using the Sentiment API Task in a Query "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our API can now be called using the NLPQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Successfully Submitted\n",
      "{\n",
      "    \"intermediate_results_csv\": \"http://18.220.133.76:5000/job_results/643/phenotype_intermediate\",\n",
      "    \"job_id\": \"643\",\n",
      "    \"luigi_task_monitoring\": \"http://18.220.133.76:8082/static/visualiser/index.html#search__search=job=643\",\n",
      "    \"main_results_csv\": \"http://18.220.133.76:5000/job_results/643/phenotype\",\n",
      "    \"phenotype_config\": \"http://18.220.133.76:5000/phenotype_id/643\",\n",
      "    \"phenotype_id\": \"643\",\n",
      "    \"pipeline_configs\": [\n",
      "        \"http://18.220.133.76:5000/pipeline_id/862\"\n",
      "    ],\n",
      "    \"pipeline_ids\": [\n",
      "        862\n",
      "    ],\n",
      "    \"results_viewer\": \"?job=643\",\n",
      "    \"status_endpoint\": \"http://18.220.133.76:5000/status/643\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "nlpql ='''\n",
    "limit 1;\n",
    "\n",
    "//phenotype name\n",
    "phenotype \"How we feel about birds\" version \"1\";\n",
    "\n",
    "//include Clarity main NLP libraries\n",
    "include ClarityCore version \"1.0\" called Clarity;\n",
    "\n",
    "termset Birds:\n",
    "  [\"football\"];\n",
    "\n",
    "define BirdFeelings:\n",
    "  Clarity.AzureSentiment({\n",
    "    termset:[Birds],\n",
    "    \"api_key\":\"'''+azure_key+'''\"\n",
    "    });\n",
    "'''\n",
    "run_result, main_csv, intermediate_csv, luigi = claritynlp.run_nlpql(nlpql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLPQL Editor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know-- NLPQL isn't too hard to read or copy/tweak, but it is pretty tough to generate *de novo*.  So we've created an editor that helps you build your NLPQL without worrying about a missed semi-colon here or bracket there.  Let's [check it out](https://nlpql-editor.herokuapp.com/demo.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLPQL_Runner.png](assets/NLPQL_editor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for joining this week's Cooking with ClarityNLP!  Please send any requests or ideas for future Cooking shows to charity.hilton@gtri.gatech.edu.\n",
    "\n",
    "Have a great week!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
